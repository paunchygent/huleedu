# AI-stödd uppsatsbedömning för ökad likvärdighet i engelskundervisningen på gymnasiet

## Inledning

Som gymnasielärare i engelska och svenska på Hulebäcksgymnasiet bedömer jag varje läsår ett stort antal elevtexter: i ordinarie kursmoment, i återkommande skrivprov och – i vissa sammanhang – inom ramen för de nationella proven i Engelska 5 och 6 (Engelska 1 och 2 fr.o.m. HT 2025). I kurserna Engelska 5 och 6 förväntas eleverna kunna formulera sig skriftligt med anpassning till syfte, mottagare och situation (Skolverket, 2011). I praktiken innebär detta att lärare regelbundet måste bedöma elevuppsatser i kursuppgifter, prov och nationella prov. Dessa bedömningar har stor inverkan på elevernas betyg, vilket ställer höga krav på likvärdighet, rättssäkerhet och transparens.

Projektets utgångspunkt är ett praktiknära men också mätteoretiskt problem: fri textproduktion är en flerdimensionell prestation som måste bedömas genom att flera kvalitetsaspekter aggregeras till ett slutomdöme. Denna aggregation förutsätter beslut om hur styrkor och brister ska vägas mot varandra – med andra ord en implicit eller explicit logik för viktning och kompensation. Bedömningsmatriser kan tydliggöra vilka dimensioner som ska beaktas, men de anger sällan en fullständig och explicit algoritm för hur dimensionerna ska kombineras och viktas. Sadler (2009) beskriver denna situation som en principiell obestämbarhet (indeterminacy) i kriteriebaserad bedömning, med syftet att lämna utrymme för en professionell holistisk viktning av de fastslagna matriskriterierna. Ur ett validitetsperspektiv är detta centralt, eftersom resultatet från ett enskilt uppsatsprov ska extrapoleras till ett omdöme om elevens underliggande skrivförmåga. Det centrala är därmed inte enbart textens yta, utan vilken tolkning av textprestationen som kan anses legitim i relation till konstruktet skrivförmåga och provets funktion som underlag för likvärdig betygssättning och urval (Messick, 1989; Weigle, 2002). I enlighet med Bachman och Palmers (1996) ramverk måste denna tolkning underbyggas av ett nyttighetsargument (utility argument), där reliabilitet och validitet samverkar med aspekter som autenticitet och konsekvenser (impact). Detta innebär ett krav på att bedömningen ska vara såväl kommunikativt relevant som jämförbar över olika bedömare och elevgrupper.

Tidigare forskning visar att själva bedömningsmetoden kan påverka både bedömarbeteende och bedömningskvalitet. Studier som jämför holistisk och analytisk bedömning indikerar att de två angreppssätten kan skilja sig åt avseende stränghet, stabilitet och hur väl de fångar olika kvalitetsaspekter (Harsch & Martin, 2013; Klein et al., 1998). Barkaoui (2011) visar – med en many-facet Rasch-analys – att holistisk respektive analytisk poängsättning kan ge olika mönster i bedömares stränghet, själv-konsistens samt interbedömaröverensstämmelse, och att dessa effekter samspelar med bedömarnas erfarenhet. Eckes (2008) preciserar att bedömare kan uppvisa relativt stabila ”rater types”, dvs. systematiska kriteriespecifika biasmönster där bedömaren är strängare i vissa dimensioner (t.ex. språk och form) och mer generös i andra (t.ex. innehåll och uppgiftsanpassning). Detta innebär att två kompetenta bedömare kan landa i olika slutomdömen även när de i stort sett är samstämmiga gällande textens kvalitetsaspekter. Sådan forskning synliggör varför skrivbedömning är kognitivt krävande och varför likvärdighet inte kan reduceras till själva matrisens utformning, eftersom problemet ligger i hur kriterier tolkas, viktas och aggregeras i praktiken.

I svensk skolkontext blir dessa frågor extra betydelsefulla eftersom nationella prov är tänkta att stödja likvärdig bedömning, samtidigt som proven ofta rättas av lärare på den egna skolan. Empiriska analyser visar att bedömaröverensstämmelsen i nationella prov varierar mellan ämnen och uppgiftstyper, och att produktiva delprov tenderar att vara mer bedömarberoende än objektiva uppgifter (Skolverket, 2009). Dessutom pekar forskning på skillnader mellan lärares rättning och extern omrättning av prov, vilket är en central policyfråga (Skolverket, 2020; Gustafsson & Erickson, 2013). Detta ramar in projektets huvudmotiv: om bedömningspraktiken ska vara både effektiv och rättssäker behövs verktyg och procedurer som minskar oönskad variation utan att reducera skrivförmåga till lättmätta proxyvariabler.

En lokal empirisk utgångspunkt för projektet är en sambedömningsövning inom mitt ämneslag, där 14 kollegor bedömde 12 elevuppsatser från ett nationellt prov. Utfallet visade en spridning på upp till fem betygssteg (enligt provets 8-gradiga skala) för enskilda texter och en samlad samstämmighet på Krippendorffs alpha ca 0,56 (se bilaga). Detta kan inte förklaras enbart av bristande kompetens hos några bedömare; snarare illustrerar det hur bedömarvariation uppstår när flerdimensionella analytiska matriser ska omsättas i betyg under tidspress och mot individuellt och lokalt etablerade normer. Här finns en dubbel urvalsproblematik: (a) varje enskild skrivuppgift och text är ett begränsat stickprov av elevens förmåga, och (b) varje lärares ”interna skala” kalibreras mot den elevpopulation läraren oftast undervisar. En lärare som främst har yrkesprogram och en lärare som främst har högskoleförberedande program exponeras för olika fördelningar av textkvalitet, vilket tenderar att forma olika föreställningar om vad som är en svag, typisk respektive mycket stark elevtext. Det som ofta uppfattas som absoluta kvalitetsmått är alltså delvis relativa mot den elevgrupp som utgör lärarens erfarenhetsbas, och dessa normer reproduceras och förhandlas i praktikgemenskapen engelsklärare (Wenger, 1998) genom doxa, pragmatik och behovet av tydliga hållpunkter.

Mot denna bakgrund prövar projektet en alternativ bedömningslogik: komparativ bedömning (Comparative Judgment, CJ), där kvalitet fastställs genom upprepade parvisa jämförelser i stället för direkt betygsättning mot en skala. I projektets prototyp används en stor språkmodell som domare (LLM-as-a-judge) i dessa parjämförelser; en Bradley–Terry-modell skattar sedan varje texts latenta kvalitet utifrån jämförelserna, och ankartexter (benchmarks) med kända betygsnivåer används för att projicera den relativa rangordningen på en betygsskala. Pollitt (2012) beskriver hur adaptiv komparativ bedömning kan ge hög tillförlitlighet genom att reducera kravet på att varje bedömare ska ”hålla hela skalan i huvudet” vid varje beslut. I mitt första test rangordnade modellen 12 exempeluppsatser (benchmarkbedömningar) i samma ordning som en extern expertgrupp (NAFS), stabilt över två separata tillfällen (se bilaga). Resultatet är tillräckligt lovande för att motivera vidare utveckling och tester– men det aktualiserar också ett vetenskapligt huvudproblem: hur kan en LLM-baserad CJ-modell valideras rigoröst i en kontext där ground truth (lärarnas bedömningar) ofta är brusiga och delvis kontextbundna?

Projektets övergripande syfte är därför att utveckla och vetenskapligt utvärdera ett AI-baserat bedömningsinstrument för elevuppsatser i Engelska 5 och 6 som kan stödja likvärdig och effektiv bedömning samt ge formativ återkoppling utan att ersätta lärarens professionella omdöme. Arbetet organiseras i tre delstudier som tillsammans behandlar (1) konstruktion och teoretisk grund, (2) validering och prövning i samarbete med lärare (med fokus på validitet, reliabilitet och etik), samt (3) elevperspektiv på AI-genererad återkoppling och möjliga effekter på revidering och lärande. Projektet följer Vetenskapsrådets (2024) riktlinjer för god forskningssed avseende transparens, samtycke och personuppgiftshantering. Nedan formuleras de forskningsfrågor som studierna adresserar.

## Bakgrund

### Skrivförmåga som konstrukt och validitetsargument

Att bedöma skrivande innebär att operationalisera ett komplext konstrukt där språkliga resurser, textuell organisation och pragmatisk/retorisk anpassning samverkar. I en kommunikativ testteoretisk tradition betonas att skrivbedömning inte kan reduceras till enbart språklig korrekthet eller ytdrag, utan måste relateras till uppgiftens kommunikativa villkor och domän (Bachman & Palmer, 1996; Weigle, 2002). Detta påverkar hur ”kvalitet” definieras: en text kan vara språkligt sofistikerad men kommunikativt otillräcklig om den missar uppgiften eller målgruppen, medan en text med språkliga brister kan vara kommunikativt framgångsrik genom tydlig struktur, relevans och uppfyllelse av syftet.

I Messicks (1989) validitetsmodell blir detta centralt: ett bedömningsinstrument måste styrkas med evidens för att resultatens tolkning är välgrundad (konstruktrepresentation) och att irrelevanta variationskällor (t.ex. bedömarpreferenser eller textlängd som proxy) inte styr utfallet i oproportionerlig grad. Samtidigt måste konsekvenser och värderingsaspekter beaktas: i ett betygssystem är frågan inte bara om instrumentet ”korresponderar” med lärarbetyg, utan om det bidrar till rättssäkra och pedagogiskt rimliga beslut.

### Viktning, kompensation och bedömarprofiler

Bedömning av elevtexter är oftast kriteriebaserad och kräver någon form av slutgiltig sammanvägning av olika delkriterier. Här uppstår frågan om kompensatorisk logik: kan styrkor inom ett område väga upp brister i ett annat, eller fungerar vissa kriterier som trösklar som inte går att kompensera för? Sadler (2009) påpekar att matriser ofta innebär en bedömningsmässig obestämbarhet; även med detaljerade betygsmatriser kvarstår alltså ett tolkningsutrymme åt det professionella omdömet. Empiriskt syns detta som skillnader i hur bedömare väger olika aspekter. Barkaoui (2011) finner att både bedömningsmetod (holistisk vs. analytisk) och bedömarnas erfarenhet påverkar poängsättningen, och Eckes (2008) visade att bedömare kan grupperas utifrån stabila profiler med olika fokus. Således kan inte likvärdighet uppnås enbart genom gemensamma matriser; det handlar om hur bedömarna kalibreras och hur deras inre viktning kan stabiliseras över tid och kontexter.

### Urvalsproblematik, svensk provkontext och likvärdighet

Urvalsproblematiken blir tydlig i diskussionen om likvärdiga betyg och nationella prov. Svenska studier och utredningar visar att bedömaröverensstämmelsen i nationella prov varierar, och att skillnader mellan lärares och externa bedömare återkommer i utvärderingar (Skolverket, 2009; Skolverket, 2020; Gustafsson & Erickson, 2013). Samtidigt är de nationella proven del av en komplex infrastrukturell helhet där proven ska fungera både som stöd och kontroll (Erickson, Borger & Olsson, 2022). Provsystemet försöker motverka relativism i bedömning genom bland annat exempeltexter, detaljerade anvisningar och förankringsuppgifter (anchoring). Erickson et al. (2022) betonar exempelvis principen att göra ”det viktigaste” bedömbart snarare än ”det lätt mätbara”, samt att skilja mellan fel som stör respektive förstör kommunikation. Den distinktionen är ett tydligt försök att styra hur brister ska värderas och viktas i bedömningen. Trots dessa åtgärder kvarstår utmaningar: svenska omrättningsstudier och reliabilitetsmått pekar på behovet av robust statistik (t.ex. kappa, ICC, generaliserbarhet, MFRM) för att förstå vad som är stabilt respektive instabilt i skrivbedömning.

### Comparative Judgement och AI-stödd skrivbedömning: från ytdrag till moderna modeller

Automated Essay Scoring (AES) – att med hjälp av språkteknologi och statistik automatiskt sätta poäng på en text – har under lång tid kritiserats för att förlita sig på enkla kvantitativa mått som textlängd, ordvariation och felfrekvens. Till exempel fann Östling, Smolentzov, Tyrefors Hinnerich och Höglin (2013) att en datorbaserad uppsatsbedömning för gymnasiet kunde prestera rimligt i jämförelse med lärarbetyg (givet att endast 45,8 % av uppsatserna fick samma betyg av två lärare), men att just textlängd och ordvariation hörde till de starkaste betygspredikatorerna – vilket belyser risken att ”textytan” utgör en attraktiv genväg för bedömare. Eftersom modellerna ofta tränas för att efterlikna rådande bedömningspraxis snarare än att utgå från ett tydligt teoretiskt konstrukt finns det en risk att de ärver både bedömarbias och proxy-mönster. Samtidigt har fältet utvecklats snabbt med neurala nätverk och transformerbaserade språkmodeller som kan representera semantik och diskurs djupare. Studier visar att neurala AES-modeller som kombinerar vektor-embeddings med handgjorda (hand-crafted) textmått kan förbättra förutsägbarheten, och stora språkmodeller kan i vissa fall nå nära mänsklig bedömarprecision (Tate et al., 2024). Samtidigt kvarstår utmaningar kring transparens och rättvisa. Professionella riktlinjer för skrivbedömning avråder från att använda helautomatiserad poängsättning för high-stakes-test, just på grund av bristande förklarbarhet och risken för skevheter (Conference on College Composition and Communication, 2022).

I detta landskap blir comparative judgement (CJ) metodologiskt intressant. CJ innebär att bedömare (mänskliga eller AI) jämför texter parvis i stället för att ge absoluta poäng, och en statistisk modell (t.ex. Bradley–Terry) uppskattar varje texts position på en latent kvalitetsskala (Pollitt, 2012; Bradley & Terry, 1952). CJ har visat hög reliabilitet i flera tillämpningar och kan minska kognitiv belastning eftersom bedömaren endast måste göra relativa jämförelser (Bramley, 2015). Det finns en etablerad provteoretisk tradition att använda rangordningsmetoder för länkning av skalor, vilket är relevant när CJ-resultat ska översättas till betyg via ankare (Bramley, 2005; Bramley & Gill, 2010; Benton, 2021). Inom skrivbedömning pekar ny forskning mot att CJ kan användas för att skapa gemensamma normer (Wu, 2025). Samtidigt kvarstår valideringsfrågan: varken mänskliga eller maskinella domare är immuna mot konstruktirrelevanta signaler, varför det är nödvändigt att undersöka vilka egenskaper som faktiskt driver deras bedömningar (Chambers & Cunningham, 2022).

## Metod och studieupplägg

Projektet består av tre delstudier enligt nedan.

### Delstudie 1: Konstruktion och teoretisk grund

Delstudie 1 utvecklar själva instrumentet och preciserar dess teoretiska förankring. Bedömningsverktyget utformas som en tvådelad modellarkitektur som kombinerar summativ rangordning med formativ transparens. För det första används en CJ-motor där en stor språkmodell genomför ett stort antal parvisa textjämförelser (LLM-as-a-judge). Utfallet modelleras med Bradley–Terry-inferens för att skatta latent textkvalitet, och en uppsättning ankaruppsatser med kända betygsnivåer används för att länka den relativa skalan till betygsnivåer (Bradley & Terry, 1952; Pollitt, 2012; Benton, 2021). För det andra utvecklas ett förklaringslager i form av en feature-baserad modell som parallellt predicerar textkvalitet utifrån språkliga och diskursiva drag. Här används till exempel gradientboostade träd (XGBoost) med SHAP-analys för att tydligt redovisa vilka textdrag som bidrar till modellens prediktion (Chen & Guestrin, 2016; Lundberg & Lee, 2017). Som jämförelse och baslinje kommer även en transformerbaserad modell att användas för att generera kontinuerliga textrepresentationer (t.ex. DeBERTa; He, Gao & Chen, 2021), i linje med modern AES-forskning (Uto et al., 2020; Faseeh et al., 2024). Syftet är inte att ersätta CJ-rankningen med dessa modeller, utan att skapa en oberoende sanity check och ett transparent beslutsunderlag för lärare: verktyget ska kunna visa varför en viss text får ett visst utfall.

### Delstudie 2: Validering och implementeringsprövning i praktik

**Syfte:** Delstudie 2 prövar instrumentet i autentisk skolpraktik och utvärderar dess mättekniska kvalitet samt användbarhet bland lärare. Fokus är (a) i vilken utsträckning CJ-modellen ger stabila och rättvisande skattningar av textkvalitet, (b) hur väl instrumentets utfall korresponderar med och förklarar lärarbedömningar trots bedömarvariation, och (c) hur lärare upplever instrumentets nytta, transparens och etiska lämplighet.

**Design:** Delstudien genomförs som en mixed methods-studie i samarbete med 3–6 gymnasieskolor. Empirin består av ett korpus om ca 300–500 elevuppsatser från Engelska 5 och 6 med variation i uppgiftstyper och elevgrupper (både högskole- och yrkesförberedande program). Ett mindre urval ankaruppsatser (t.ex. 30 uppsatser) fastställs genom en CJ-panel och används för att förankra CJ-skalan vid betygsnivåer.

**Genomförande:**

1. Baseline-bedömning: Deltagande lärare bedömer initialt ett urval elevtexter enligt ordinarie rutin. Detta ger en utgångspunkt för att jämföra lärarbetyg med instrumentets skattningar.
2. Instrumentstödd bedömning: Samma texter bedöms sedan med stöd av AI-instrumentet. Varje lärare får för varje text: (i) en CJ-baserad kvalitetsskattning med förslag på betyg (utifrån ankarförankringen), samt (ii) en transparent redovisning av vilka textmått som i huvudsak har legat till grund utfallet. Läraren tar sedan ställning: acceptera eller justera AI:ns förslag. Alla interaktioner och beslut loggas.
3. Kalibreringsworkshops: Efter bedömningarna genomförs korta workshops där lärarna gemensamt diskuterar ankartexter och gränsfall, med stöd av instrumentets textmåttsanalyser. Syftet är att fånga hur praktikgemenskapens normer kommer till uttryck och om instrumentet kan fungera som katalysator för professionell kalibrering och en gemensam repertoar.

**Analys:**

**Reliabilitet och mätpålitlighet:** CJ-modellens inre konsistens utvärderas genom att jämföra upprepade CJ-körningar (med olika slumpmässig ordning/seed) för att estimera stabilitet (jfr. Bramley, 2015; Bramley & Vitello, 2019). Överensstämmelse mellan instrumentets skattningar och lärarbedömningar beräknas med korrelation (t.ex. Spearmans rho) och kategoriöverensstämmelse (t.ex. Cohens kappa). Skillnader analyseras för systematiska mönster (t.ex. om vissa lärare konsekvent justerar upp eller ned).

**Bedömarprofiler och bias:** Med inspiration från many-facet Rasch analyseras i vilken grad variationen kan förklaras av bedömarfaktorer och bedömningsmetod (jfr. Barkaoui, 2011; Eckes, 2008). Finns indikationer på att vissa lärare är strängare i vissa aspekter än andra, och hur påverkar detta mätinstrumentets förslag? Genom instrumentets förklaringslager kan vi också identifiera om AI:ns bedömning lutar sig mot några uppenbara proxy-drag (t.ex. textlängd). Hela valideringsanalysen placeras inom ramen för konstruktvaliditet, som innebär att instrumentet utvärderas enligt hur väl det mäter det avsedda konstruktet eller om något ovidkommande påverkar resultatet (Chambers & Cunningham, 2022).

**Lärarperspektiv:** Intervju- och workshopdata analyseras tematiskt för att belysa lärarnas tillit till och förståelse av verktyget. Några intressanta aspekter att undersöka är om instrumentet upplevs som tidsbesparande, om det hjälper lärarna att få syn på elevtexter ur nya vinklar och om några etiska farhågor uppstår (t.ex. risk för för stor tillit till AI eller upplevelse av att lärarens autonomi minskar).

**Etiska överväganden:** Instrumentets utlåtanden kommer inte att användas som ensam grund för några skarpa betygsbeslut under studiens gång. Samtliga elevtexter och lärarbedömningar hanteras konfidentiellt, med informerat samtycke från både lärare och elever. Studien följer Vetenskapsrådets (2024) principer om information, samtycke, nyttjande och konfidentialitet.

**Förväntade utfall:** Delstudie 2 levererar en empirisk bedömning av instrumentets mätkvalitet (reliabilitet, validitet) i praktiken, en kartläggning av hur instrumentet interagerar med befintlig betygskultur, samt rekommendationer för förbättring eller justering inför mer storskaliga tester. Därmed skapas även en förbättrad version av instrumentet inför Delstudie 3.

### Delstudie 3: Elevperspektiv och konsekvensvaliditet

**Syfte:** Delstudie 3 undersöker instrumentets pedagogiska konsekvenser ur elevperspektiv. Fokus ligger på hur elever tolkar och använder AI-genererad återkoppling på sina texter, samt om detta påverkar deras revideringar och lärande. Studien adresserar alltså validitetsaspekten konsekvenser (Messick, 1989), genom att pröva om verktyget leder till önskvärda (eller oönskade) effekter i undervisningspraktiken.

**Design:** Delstudien är upplagd som en klassrumsnära intervention med cross-over-design. Två eller fler klasser (Engelska 5/6) deltar. Varje elev skriver två likvärdiga skrivuppgifter (A och B) med en tids mellanrum. För uppgift A får halva klassen AI-baserad återkoppling på utkast 1, medan den andra halvan fungerar som kontroll (enbart ordinarie stöd, t.ex. lärar- eller kamratrespons). För uppgift B skiftas grupperna. Därmed kan varje elev jämföra att ha fått respektive inte fått AI-stöd.

**Intervention:** Återkopplingen utformas så att den är tydligt kopplad till kursens bedömningsaspekter (innehåll/relevans, struktur/koherens, språk och uttrycksform, anpassning till syfte och mottagare). Den ges med fokus på att vara konkret och framåtsyftande (Hattie & Timperley, 2007). Läraren introducerar AI-verktyget för eleverna och klargör att återkopplingen är ett stödjande förslag – inte ett facit – samt att eleverna behåller eget ansvar för texten. Under revideringsfasen noteras vilka förslag eleverna väljer att tillämpa eller ignorera.

**Datainsamling:** För varje elev och uppgift samlas (i) utkast före/efter revidering, (ii) loggdata från AI-verktyget (vilka återkopplingspunkter som visats och klickats), (iii) en kort enkät efter varje uppgift där eleverna skattar upplevelsen (begriplighet, hjälp, motivation, känsla av rättvisa), samt (iv) fokusgruppsintervjuer med 6–8 elever (frivilligt urval) om hur de uppfattade och använde återkopplingen.

**Analys:**

**Lärandeeffekt:** För varje elevtext bedöms kvalitetsförändringen från utkast 1 till utkast 2. Detta görs antingen genom oberoende lärarbedömning eller genom instrumentets egen CJ-skattning av båda versionerna (val av metod diskuteras före studiens start). En jämförelse görs sedan mellan gruppen som fick AI-stöd och kontrollgruppen. Primärt fokus är kvalitativa förbättringar (t.ex. om texter i AI-gruppen genomgår större förbättringar i struktur eller innehåll). Revideringarna kodas även manuellt för att skilja på ytrevideringar (stavning, grammatik) och djuprevideringar (omstrukturering, förtydliganden, utveckling av innehåll).

**Elevperspektiv:** Enkätdata sammanställs kvantitativt för att ge en bild av hur återkopplingen togs emot. Intervjudata analyseras för att förstå hur eleverna resonerade kring återkopplingen: Hjälpte den dem att förstå vad som är viktigt i texten? Fokuserade de på att ”tillfredsställa AI:n” eller på reella förbättringar av innehållet? Elevernas upplevelser relateras till verktygets konstruktion för att identifiera eventuella missriktade incitament eller missförstånd (jfr. Conference on College Composition and Communication, 2022).

**Etik:** Upplägget gör att alla elever får likvärdig undervisning över tid (alla får pröva AI-stöd i minst en uppgift). Läraren övervakar revideringarna och kan ingripa om någon elev feltolkar återkopplingen. Ingen personlig information om elever sparas i verktyget, och deltagandet är frivilligt med möjlighet att avstå när som helst.

**Förväntade utfall:** Delstudie 3 ger underlag för att bedöma instrumentets konsekvensvaliditet. Jag förväntar mig att AI-återkopplingen kan stödja vissa elever i att göra meningsfulla förbättringar av deras texter, men studien är också utformad för att upptäcka eventuella negativa effekter (t.ex. att elever fokuserar oproportionerligt på formalia eller tappar tilltron till sin egen röst). Resultaten kommer att användas för att formulera rekommendationer för hur AI-verktyget bör implementeras i undervisning och vilka begränsningar som måste beaktas.

## Referenser

- Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice: Designing and developing useful language tests. Oxford University Press.
- Barkaoui, K. (2011). Effects of marking method and rater experience on ESL essay scores and rater performance. Assessment in Education: Principles, Policy & Practice, 18(3), 279–293. <https://doi.org/10.1080/0969594X.2010.526585>
- Benton, T. (2021). Comparative judgement for linking two existing scales. Frontiers in Education, 6, 775203. <https://doi.org/10.3389/feduc.2021.775203>
- Bradley, R. A., & Terry, M. E. (1952). Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, 39(3–4), 324–345. <https://doi.org/10.1093/biomet/39.3-4.324>
- Bramley, T. (2005). A rank-ordering method for equating tests by expert judgment. Journal of Applied Measurement, 6(2), 202–223.
- Bramley, T. (2015). Investigating the reliability of adaptive comparative judgment. Cambridge Assessment Research Report.
- Bramley, T., & Gill, T. (2010). Evaluating the rank-ordering method for standard maintaining. Research Papers in Education, 25(3), 293–317. <https://doi.org/10.1080/02671522.2010.498147>
- Bramley, T., & Vitello, S. (2019). The effect of adaptivity on the reliability coefficient in adaptive comparative judgement. Assessment in Education: Principles, Policy & Practice, 26(1), 43–58. <https://doi.org/10.1080/0969594X.2017.1418734>
- Chambers, L., & Cunningham, E. (2022). Exploring the validity of comparative judgement: Do judges attend to construct-irrelevant features? Frontiers in Education, 7, 802392. <https://doi.org/10.3389/feduc.2022.802392>
- Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. I Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (ss. 785–794). ACM. <https://doi.org/10.1145/2939672.2939785>
- Conference on College Composition and Communication. (2022). Writing assessment: A position statement (Reviderad april 2022). National Council of Teachers of English. <https://cccc.ncte.org/cccc/resources/positions/writingassessment>
- Eckes, T. (2008). Rater types in writing performance assessments: A classification approach to rater variability. Language Testing, 25(2), 155–185. <https://doi.org/10.1177/0265532207086780>
- Erickson, G., Borger, L., & Olsson, E. (2022). National assessment of foreign languages in Sweden: A multifaceted and collaborative venture. Language Testing, 39(3), 474–493. <https://doi.org/10.1177/02655322221075067>
- Faseeh, M., Jaleel, A., Iqbal, N., Ghani, A., Abdusalomov, A., Mehmood, A., & Cho, Y.-I. (2024). Hybrid approach to automated essay scoring: Integrating deep learning embeddings with handcrafted linguistic features for improved accuracy. Mathematics, 12(21), 3416. <https://doi.org/10.3390/math12213416>
- Gustafsson, J.-E., & Erickson, G. (2013). To trust or not to trust? – Teacher marking versus external marking of national tests. Educational Assessment, Evaluation and Accountability, 25(1), 69–87. <https://doi.org/10.1007/s11092-013-9158-x>
- Harsch, C., & Martin, G. (2013). Comparing holistic and analytic scoring methods: Issues of validity and reliability. Assessment in Education: Principles, Policy & Practice, 20(3), 281–307. <https://doi.org/10.1080/0969594X.2012.742422>
- Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81–112. <https://doi.org/10.3102/003465430298487>
- He, P., Gao, J., & Chen, W. (2021). DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543.
- Klein, S. P., Stecher, B. M., McCaffrey, D. F., & Shavelson, R. J. (1998). Analytic versus holistic scoring of science performance tasks. Applied Measurement in Education, 11(2), 121–137. <https://doi.org/10.1207/s15324818ame1102_1>
- Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. I Proceedings of the 31st International Conference on Neural Information Processing Systems (ss. 4765–4774).
- Messick, S. (1989). Validity. I R. L. Linn (Red.), Educational measurement (3 uppl., ss. 13–103). American Council on Education/Macmillan.
- Östling, R., Smolentzov, A., Tyrefors Hinnerich, B., & Höglin, E. (2013). Automated essay scoring for Swedish. I J. Tetreault, J. Burstein, & C. Leacock (Red.), Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (ss. 42–47). Association for Computational Linguistics.
- Pollitt, A. (2012). The method of adaptive comparative judgement. Assessment in Education: Principles, Policy & Practice, 19(3), 281–300. <https://doi.org/10.1080/0969594X.2012.665354>
- Sadler, D. R. (2009). Indeterminacy in the use of preset criteria for assessment and grading. Assessment & Evaluation in Higher Education, 34(2), 159–179. <https://doi.org/10.1080/02602930801956059>
- Skolverket. (2009). Bedömaröverensstämmelse vid bedömning av nationella prov (Dnr 2008:286). Skolverket.
- Skolverket. (2011). Ämne – Engelska (Gy11) [Webbsida]. Hämtad 25 januari 2026, från <https://syllabuswebb.skolverket.se/syllabuscw/jsp/subject.htm?subjectCode=ENG&tos=gy>
- Skolverket. (2020). Likvärdiga betyg och meritvärden (Rapport 2020:7). Skolverket.
- Tate, T. P., Steiss, J., Bailey, D., Graham, S., Moon, Y., Ritchie, D., Tseng, W., & Warschauer, M. (2024). Can AI provide useful holistic essay scoring? Computers and Education: Artificial Intelligence, 7, 100255. <https://doi.org/10.1016/j.caeai.2024.100255>
- Uto, M., Xie, Y., & Ueno, M. (2020). Neural automated essay scoring incorporating handcrafted features. I Proceedings of the 28th International Conference on Computational Linguistics (ss. 6077–6088). International Committee on Computational Linguistics. <https://doi.org/10.18653/v1/2020.coling-main.535>
- Vetenskapsrådet. (2024). God forskningssed 2024. Vetenskapsrådet. <https://www.vr.se/download/18.4c9f221a191e4edf9053a474/1727853946433/God%20forskningssed%202024.pdf>
- Weigle, S. C. (2002). Assessing writing. Cambridge University Press.
- Wenger, E. (1998). Communities of practice: Learning, meaning, and identity. Cambridge University Press.
- Wu, Q. (2025). Comparative judgment: Building a shared consensus over rater variation in assessing second language writing performance. SAGE Open, 15(2). <https://doi.org/10.1177/21582440251346346>
