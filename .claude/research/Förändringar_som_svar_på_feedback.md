Ersättningsförslag 1: Syfte och delstudier (att lägga i slutet av Inledning, som ersättning för ditt nuvarande stycke som börjar “Projektets övergripande syfte är därför…”)

Empiriska uppföljningar av nationella prov visar att bedömning av skrivande är tydligt bedömarberoende även när bedömare har tillgång till stödmaterial och exempeltexter. I Skolverkets studie av engelska (åk 9, Part C – skriftlig produktion) var korrelationen mellan bedömare och lärargrupp relativt hög (0,86–0,93), vilket tyder på att rangordningen av texter ofta är jämförelsevis stabil. Samtidigt var exakt överensstämmelse låg: endast 8 av 100 elevtexter bedömdes identiskt av bedömare och lärargrupp, och för sex texter skilde bedömningarna med fyra betygssteg på den tiogradiga skalan (Skolverket, 2009). Resultat av samma typ framkommer i Dalbergs statistiska analys av nationella skrivprov i svenska på gymnasiet: för npk3 (helhetsbetyg) uppgick den medianbaserade exakta överensstämmelsen till 45,7 % (Fleiss κ = 0,30) vid enskild bedömning, medan reliabiliteten ökade tydligt när fler bedömare ingick, exempelvis med inomklasskorrelation 0,85 vid sambedömning (Dalberg, 2019). Sammantaget pekar detta på ett praktikproblem: även när bedömningen i grova drag följer samma kvalitetsordning varierar nivåplaceringen, särskilt i gränsfall, vilket får direkt betydelse för likvärdighet och rättssäkerhet.

Projektets övergripande syfte är att utveckla och vetenskapligt pröva ett AI-stött bedömningsinstrument för elevuppsatser i Engelska 5–6 (Engelska 1–2 fr.o.m. HT 2025) som kan minska oönskad variation i bedömning och samtidigt stärka transparens och professionell kalibrering. Instrumentet utformas som ett beslutsstöd och ska inte ersätta lärarens betygsbeslut. Utgångspunkten är att de empiriska mönstren ovan (högre stabilitet i rangordning än i exakt nivå) gör komparativ bedömning (Comparative Judgment, CJ) metodologiskt relevant: CJ bygger på parvisa jämförelser och kan därmed minska kravet på att varje bedömare (människa eller modell) ska ”hålla hela skalan i huvudet” i varje enskilt beslut (Pollitt, 2012). Samtidigt aktualiseras ett centralt vetenskapligt problem: hur valideras ett AI-baserat CJ-instrument i en kontext där “ground truth” i form av lärarbedömningar innehåller både slumpvariation och systematiska bedömarprofiler?

Arbetet organiseras i tre delstudier med tydliga och separerade roller:
 1. Delstudie 1 utvecklar instrumentet och prövar dess mätteoretiska egenskaper (reliabilitet, konstruktvaliditet och risk för proxy-baserade beslut) samt etablerar ett benchmark-/ankarmaterial.
 2. Delstudie 2 undersöker hur lärare använder instrumentet i praktik, hur det påverkar bedömningsbeslut och hur normer och bedömningskultur förhandlas när ett sådant verktyg introduceras.
 3. Delstudie 3 analyserar etiska och policyrelaterade konsekvenser av AI-stödd bedömning och återkoppling, med fokus på rättvisa/bias, ansvarsförskjutning och risker som uppstår när ett verktyg skalas upp i ett system som eftersträvar ökad likvärdighet.

Projektet följer Vetenskapsrådets (2024) riktlinjer för god forskningssed avseende transparens, samtycke, konfidentialitet och personuppgiftshantering.

⸻

Ersättningsförslag 2: Metod och studieupplägg (som ersättning för hela ditt nuvarande avsnitt Metod och studieupplägg)

Projektet består av tre delstudier som tillsammans adresserar (i) instrumentutveckling och mätteoretisk validering, (ii) användning och påverkan i lärarpraktik och (iii) etiska och systemnivåmässiga konsekvenser av AI-stödd bedömning. Övergripande är målet att både pröva om ett AI-stött CJ-instrument kan vara tillförlitligt och att undersöka under vilka villkor det kan användas ansvarsfullt.

Delstudie 1: Instrumentutveckling, benchmarks och mätteoretisk validering
Syfte. Delstudien utvecklar bedömningsinstrumentet och etablerar en mätteoretisk grund för hur instrumentet ska utvärderas. Fokus ligger på (a) intern stabilitet i CJ-rankningen, (b) länkning från latent rangordning till betygsnivåer med hjälp av ankare/benchmarks och (c) evidens för konstruktvaliditet, inklusive analys av om instrumentet riskerar att drivas av konstruktirrelevanta proxy-signaler (t.ex. textlängd).

Datamaterial. En central designprincip är att delstudien ska vara genomförbar även utan tillgång till sekretessbelagt provmaterial. Kärnmaterialet utgörs därför av elevuppsatser från Engelska 5–6 insamlade i samarbete med gymnasieskolor (autentiska uppgifter i undervisning/prov), kompletterat med ett begränsat benchmark-/ankarmaterial som fastställs av en extern bedömarpanel eller en särskilt organiserad lärarpanel. Om äldre offentliga provuppgifter eller elevtexter kan användas som prototypmaterial kan de ingå i tidiga iterationer, men instrumentets huvudtestning knyts till material som kan hanteras rättsligt och etiskt under projektets gång.

Genomförande. Instrumentet bygger på komparativ bedömning där en språkmodell genomför ett större antal parvisa jämförelser, och en statistisk modell skattar varje texts position på en latent kvalitetsskala (Bradley & Terry, 1952; Pollitt, 2012). Ankare används för att koppla den latenta skalan till betygsnivåer på ett transparent sätt (Benton, 2021). För att möjliggöra professionell granskning utvecklas ett transparenslager som (i) kopplar beslut till bedömningsdimensioner och (ii) synliggör osäkerhet, särskilt nära gränser och i texter där modellen ger svaga eller instabila utslag.

Analys och utfall.
 • Stabilitet/reliabilitet: instrumentets rangordning och nivåförslag jämförs mellan upprepade körningar och alternativa bedömningsinställningar (t.ex. variation i urval av par jämförelser).
 • Konstruktvaliditet: analysen fokuserar på om instrumentet uppvisar systematiska mönster som är svåra att förena med skrivkonstruktet (t.ex. starka effekter av ytdrag) samt om besluten kan förklaras i termer av bedömningsdimensioner som är meningsfulla för lärare (Messick, 1989; Chambers & Cunningham, 2022).
 • Benchmark- och länkningseffekter: hur ankare påverkar nivåplacering, särskilt i gränsfall.

Delstudie 1 resulterar i (1) en validerad prototyp, (2) ett benchmark-/ankarmaterial och (3) ett utvärderingsprotokoll som kan återanvändas i delstudie 2 och 3.

Delstudie 2: Läraranvändning, påverkan på bedömningsbeslut och bedömningskultur
Syfte. Delstudie 2 undersöker hur instrumentet fungerar när det används som beslutsstöd av lärare. Fokus ligger på (a) hur verktyget påverkar bedömningsbeslut och interbedömaröverensstämmelse, (b) hur lärare tolkar och förhandlar verktygets output, och (c) hur normer och praktiker förändras när ett nytt bedömningsstöd introduceras.

Design. Delstudien genomförs som en mixed methods-studie i samarbete med ett mindre antal gymnasieskolor (t.ex. 3–6). Upplägget bygger på en jämförelse inom lärare och inom texter: samma material bedöms i två betingelser – (1) ordinarie bedömning och (2) instrumentstött bedömning. Detta minskar behovet av stora urval och ökar tolkbarheten.

Datainsamling.
 • Bedömningsdata: lärares betyg/nivåbedömningar i båda betingelserna.
 • Processdata: loggning av hur ofta lärare accepterar respektive justerar verktygets förslag (accept-/override-mönster), och i vilka typer av texter (t.ex. gränsfall).
 • Kvalitativa data: korta intervjuer och/eller fokusgrupper samt material från gemensamma kalibreringsdiskussioner där ankare och gränsfall används som utgångspunkt.

Analys och utfall.
 • Förändring i samstämmighet: jämförelse av interbedömaröverensstämmelse med och utan instrument, med särskilt fokus på variation i gränsfall och på om spridningen minskar utan att bedömningen blir mer proxy-styrd.
 • Beslutsmönster: analys av när och varför lärare justerar instrumentets förslag, och om det finns systematiska skillnader mellan lärare eller skolkontexter.
 • Tillit, transparens och professionell autonomi: tematisk analys av hur lärare uppfattar verktygets förklaringar, osäkerhetsmarkeringar och användbarhet, samt vilka risker de identifierar (t.ex. övertillit).

Delstudie 2 resulterar i evidens för praktisk användbarhet och en tydligare bild av vilka införandevillkor som krävs för att verktyget ska stärka likvärdighet snarare än skapa nya källor till variation.

Delstudie 3: Etik, rättvisa och policykonsekvenser av AI-stödd bedömning och återkoppling
Syfte. Delstudie 3 analyserar konsekvenserna av att införa ett AI-stött bedömningsinstrument i en high-stakes-nära praktik. Fokus ligger på frågor som inte kan reduceras till modellprestanda: (a) vilka bias- och rättvisemönster som kan uppstå, (b) hur ansvar för bedömning och legitimitet förflyttas mellan lärare, verktyg och systemnivå och (c) vilka risker som uppstår om ett verktyg används brett, i synnerhet om dess fel och skevheter skalar.

Design. Delstudien kombinerar (1) en systematisk rättvise- och riskanalys baserad på data och observationer från delstudie 1–2 och (2) scenario- och principbaserad analys av möjliga införandemodeller. Den empiriska delen kan inkludera kompletterande elev- och lärarfokusgrupper om hur AI-stöd upplevs i relation till rättvisa, begriplighet och återkoppling, men delstudien är primärt inriktad mot etik och systemeffekter snarare än klassrumsinterventioner.

Analys och utfall.
 • Rättviseanalys: kartläggning av om instrumentets fel, osäkerhet eller nivåförskjutningar samvarierar med elevgrupper, texttyper eller andra relevanta faktorer (t.ex. språklig profil, programkontext), samt analys av vilka typer av fel som är mest problematiska i en betygskontext.
 • Ansvar och tillit: analys av risker kopplade till automation bias, “rubric laundering” (att förklaringar upplevs som mer legitima än de är) och hur lärare och elever kan överskatta verktygets objektivitet.
 • Policy-scenarier: jämförelse mellan åtminstone två realistiska användningssätt: verktyget som lokalt beslutsstöd (för kalibrering och transparens) respektive verktyget som del av ett mer centraliserat likvärdighetssystem. I båda scenarierna analyseras vad som krävs för ansvarsfull användning (begränsningar, uppföljning, transparenskrav).

Delstudie 3 resulterar i en etisk och praktiskt orienterad rekommendation om hur AI-stött bedömningsstöd kan användas, vilka risker som måste hanteras, och vilka former av transparens och uppföljning som är nödvändiga.

Etik och datahantering (gemensamt för alla delstudier)
Instrumentets utlåtanden används inte som ensam grund för betygsbeslut inom ramen för studierna. All insamling av elevtexter och bedömningsdata sker med informerat samtycke och med konfidentiell hantering. Datamaterial avidentifieras och lagras på ett sätt som uppfyller krav på säkerhet och åtkomstkontroll. Vid användning av externa AI-tjänster säkerställs att elevtexter hanteras i enlighet med gällande regelverk och att material inte används för oavsiktlig modellträning.

⸻

Lägg till i referenslistan (ny post)

Dalberg, T. (2019). Samstämmighet i skrivbedömning: Statistisk analys vid bedömning av två nationella skrivprov. Svenska i utveckling nr 36. Uppsala universitet.

(Din Skolverketpost för 2009 täcker redan de siffror som används ovan; om du vill kan du också lägga till en separat referens till delrapporten om engelska, men det är inte nödvändigt om du primärt hänvisar till Skolverkets sammanhållna rapport Dnr 2008:286.)
