---
description: File Service - Stateful file upload service with PostgreSQL persistence, user attribution, and teacher notification projection
alwaysApply: false
---
# 020.6: File Service Architecture

## Service Identity
- **Package**: `huleedu-file-service`
- **Port**: 7001 (HTTP API), 9094 (Prometheus)
- **Stack**: Quart, SQLAlchemy (async), PostgreSQL, aiokafka, Dishka DI
- **Purpose**: Stateful file upload service with persistence and notification projection
- **Database**: PostgreSQL with file_uploads table for user attribution

## Service Structure (STATEFUL BLUEPRINT PATTERN)

### Directory Structure
```
services/file_service/
├── app.py                     # Quart app with Blueprint registration
├── startup_setup.py           # DI initialization, DB setup, lifecycle hooks
├── api/                       # Blueprint API routes
│   ├── file_routes.py         # /v1/files endpoints with user context
│   └── health_routes.py       # /healthz, /metrics endpoints
├── models_db.py               # SQLAlchemy models (FileUpload)
├── implementations/           
│   ├── file_repository_impl.py    # Database operations for file tracking
│   ├── event_publisher_impl.py    # Outbox-based event publishing
│   └── text_extractor_impl.py     # Strategy-based text extraction
├── notification_projector.py  # Teacher notification projection (3 types)
├── protocols.py               # Service behavioral contracts
├── di.py                      # Dishka providers with DB engine
├── alembic/                   # Database migrations
│   └── versions/              
│       └── 20250806_add_file_uploads_table.py
└── tests/
```

## Stateful Architecture

### Database Model (`models_db.py`)
```python
class FileUpload(Base):
    __tablename__ = "file_uploads"
    
    # Primary identifiers
    id: UUID (primary key)
    file_upload_id: str (unique)
    batch_id: str (indexed)
    user_id: str (indexed)  # Teacher attribution
    
    # File metadata
    filename: str
    file_size_bytes: int
    
    # Storage references
    raw_file_storage_id: str
    text_storage_id: str
    
    # Processing status
    processing_status: str  # PENDING/COMPLETED/FAILED
    validation_error_code: str
```

### Repository Pattern (`file_repository_impl.py`)
- **Create**: Store file upload record with user_id
- **Update**: Update processing status and storage IDs
- **Query**: Get uploads by batch_id, user_id
- **Delete**: Soft delete with status update

## Teacher Notification Projection

### Notification Projector (`notification_projector.py`)
Implements 3 teacher notifications:

1. **batch_files_uploaded**: After successful upload
   - Priority: STANDARD
   - Category: FILE_OPERATIONS
   - Triggered: After FileUploadCompletedV1 event

2. **batch_file_removed**: When file deleted
   - Priority: STANDARD  
   - Category: FILE_OPERATIONS
   - Triggered: Direct invocation after repository delete

3. **batch_validation_failed**: On validation errors
   - Priority: IMMEDIATE
   - Category: SYSTEM_ALERTS
   - Triggered: After BatchValidationErrorsV1 event

### Projection Pattern
```python
# Direct invocation (no Kafka round-trip):
await notification_projector.handle_file_upload_completed(event)
# Publishes TeacherNotificationRequestedV1 to Kafka
```

## API Endpoints

### POST /v1/files/batch
- **Auth**: JWT required (extracts user_id)
- **Process**:
  1. Extract user_id from JWT token
  2. Create FileUpload records with user attribution
  3. Process files concurrently
  4. Update DB with processing results
  5. Publish domain events
  6. Project teacher notifications
- **Response**: 202 Accepted with batch_id

### GET /v1/files/batch/{batch_id}
- **Auth**: JWT required (validates ownership)
- **Returns**: File upload records for batch
- **Security**: Only returns files for authenticated user

## Event Architecture

### Domain Events Published
- `FileUploadCompletedV1`: After successful processing
- `BatchValidationErrorsV1`: On validation failures
- `EssayContentProvisionedV1`: Legacy event for BOS

### Notification Events
- `TeacherNotificationRequestedV1`: Via notification projector

## Database Configuration

### Connection
- **URL**: `postgresql+asyncpg://user:pass@file_service_db:5432/huledu_file_service`
- **Pool**: 5 connections, 10 overflow
- **Engine**: SQLAlchemy async with asyncpg

### Migrations
```bash
# From service directory:
pdm run alembic upgrade head
```

## Dependency Injection (`di.py`)

### Key Providers
```python
@provide(scope=Scope.APP)
async def provide_database_engine() -> AsyncEngine:
    # PostgreSQL connection with pooling

@provide(scope=Scope.REQUEST)  
async def provide_file_repository() -> FileRepositoryProtocol:
    # Repository with DB session management

@provide(scope=Scope.APP)
def provide_notification_projector() -> NotificationProjector:
    # Projector with event publisher
```

## Testing

### Database Tests
- **Unit**: Mock repository with in-memory implementation
- **Integration**: TestContainers PostgreSQL
- **E2E**: Full stack with real database

### Test Coverage
- Repository operations: 95%
- Notification projection: 100%
- API endpoints: 91%

## Production Patterns

✅ **IMPLEMENTED**:
- Stateful with PostgreSQL persistence
- User attribution on all operations
- Teacher notification projection
- Transactional outbox for events
- Database connection pooling
- Comprehensive error handling

❌ **NOT YET**:
- File versioning
- Soft delete with audit trail
- File content deduplication
- S3/blob storage integration

## Key Changes from Stateless

1. **Added PostgreSQL**: Full persistence layer
2. **User Attribution**: All files tracked to teacher_id
3. **Notification Projection**: Direct teacher notifications
4. **Repository Pattern**: Clean data access layer
5. **Database Migrations**: Alembic for schema management

## Security & Validation

- **JWT Required**: All endpoints need authentication
- **User Isolation**: Teachers only see their own files
- **Validation Codes**: Structured error reporting
- **Status Tracking**: Complete processing audit trail
