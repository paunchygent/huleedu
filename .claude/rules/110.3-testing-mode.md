---
id: "110.3-testing-mode"
type: "workflow"
created: 2025-05-25
last_updated: 2025-11-17
scope: "all"
parent_rule: "110-ai-agent-interaction-modes"
---
# 110.3: Testing Mode

## 1. Core Principles
- **MUST** follow [070-testing-and-quality-assurance.md](mdc:070-testing-and-quality-assurance.md)
- **MUST** follow [075-test-creation-methodology.md](mdc:075-test-creation-methodology.md) when creating or modifying tests
- **MUST** use the root-aware runner: `pdm run pytest-root <path-or-nodeid>`
- **Alt (any dir)**: `pyp <path-or-nodeid>` after `source scripts/dev-aliases.sh`, or `pdmr pytest-root <path-or-nodeid>`
- **FORBIDDEN**: Fixing tests to make them pass - fix underlying code issues

## 2. Test Priorities
- Prioritize unit and contract tests
- Update contract tests when Pydantic models change
- Write meaningful tests based on requirements, not coverage

## 3. Mocking Boundaries
- **MUST** apply mocking rules from [070-testing-and-quality-assurance.md](mdc:070-testing-and-quality-assurance.md) ยง3 and [075-test-creation-methodology.md](mdc:075-test-creation-methodology.md) ยง7.2
- **SHOULD** propose protocol-based DI mocks for external boundaries and avoid introducing `@patch` for service protocol dependencies

## 4. Debugging
- Add `-s` to the standard runner for debug output, e.g. `pdm run pytest-root -s <path>`
- Analyze test failures and correlate with code changes
- Fix application code, not tests

## 5. Patterns
- Follow [075-test-creation-methodology.md](mdc:075-test-creation-methodology.md) Sections 3 and 9
- Follow [051-pydantic-v2-standards.md](mdc:051-pydantic-v2-standards.md) Section 8.2

## 6. Enum Usage in Tests

- **MUST** respect enum usage rules from [070-testing-and-quality-assurance.md](mdc:070-testing-and-quality-assurance.md) ยง8 when proposing or modifying tests.

## 7. Integration Testing

- **MUST** follow integration-testing standards from [070-testing-and-quality-assurance.md](mdc:070-testing-and-quality-assurance.md) ยง9 and any relevant 020.x service rules when suggesting or modifying integration tests.

## 8. AI Agent Testing Coordination

### 8.1. Agent Behavior Expectations

#### Test Creation Agents (test-engineer)
- **Compliance**: Strict adherence to Rules 075 and 075.1 methodologies
- **Pattern Following**: Use established protocol-based mocking from existing test files
- **DI Principles**: AsyncMock(spec=Protocol) only; avoid `@patch`, and never use `@patch` for protocol dependencies (see 070/075)
- **Domain Context**: Swedish character support, HuleEdu education patterns
- **Error Handling**: Structured HuleEduError patterns, behavioral testing focus

#### Architecture Review Agents (lead-architect-planner)  
- **Mandatory Execution**: After each Rule 075.1 batch regardless of test results
- **Analysis Scope**: Test quality, implementation validation, root cause identification
- **Quality Gates**: Verify Rule 075 compliance, DI patterns, Swedish locale support
- **Bug Classification**: Distinguish test creation issues from implementation bugs

### 8.2. User Collaboration Pattern
- **Transparency**: Report all validation results (pass/fail status, quality issues)
- **Issue Resolution**: Present architect analysis before proposing fixes
- **Progress Communication**: Use TodoWrite for visible progress tracking
- **Implementation Priority**: Fix code bugs before test modifications

### 8.3. Common AI Pitfalls - Avoidance Protocol
- **Avoid**: Use @patch decorators in test files
- **Avoid**: Test implementation details over business behavior  
- **Avoid**: Assume test failures are test issues without architect analysis
- **Always**: Validate DI compliance after test creation
- **Always**: Include Swedish character testing in domain-specific tests
- **Always**: Launch architect review regardless of apparent test success

### 8.4. Quality Assurance Behavior
- **Batch Validation**: Execute complete quality gate sequence per Rule 075.1
- **Type Safety**: Verify zero type errors before batch completion
- **Pattern Consistency**: Follow exact patterns from existing codebase test files
- **Implementation Feedback**: Report discovered code bugs to user for resolution priority

## 9. Debugging and Error Analysis

### 9.1. Test Failure Analysis
- **MUST** run tests with `-s` flag during debugging to see print output
- **REQUIRED**: Read actual error messages completely before attempting fixes
- **FORBIDDEN**: Simplifying tests to make them pass without fixing root cause

### 9.2. Context7 Integration
- **SHOULD** research known issues for complex errors (SQLAlchemy, async, etc.)
- **PATTERN**: Include reference links in comments for documented solutions
- **EXAMPLE**: Document GitHub issues and Stack Overflow solutions

```python
# Reference: https://github.com/googleapis/python-bigquery-sqlalchemy/issues/844
class StatusEnum(str, enum.Enum):  # str inheritance required for SQLAlchemy
    PENDING = "pending"
```

## 10. Session Management for AI Agents

### 10.1. Todo List Usage
- **MUST** use TodoWrite (or equivalent session tooling) to track test creation and debugging progress.
- **MUST** mark tasks completed immediately upon achieving a 100% pass rate for the associated tests.
- **SHOULD** keep Todo items aligned with Rule 075/075.1 phases (pre-work, implementation, validation).

### 10.2. Incremental Validation
- **MUST** validate each new or modified test file independently before expanding scope.
- **MUST** ensure no regression in existing tests when introducing new test files (run targeted suites as needed).
- **SHOULD** maintain an accurate view of cumulative test coverage/progress in collaboration with the user.
