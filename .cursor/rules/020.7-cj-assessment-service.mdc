---
description: Defines the CJ Assessment Service. A Kafka worker performing Comparative Judgement on essays via LLMs. It consumes assessment requests and publishes ranking or failure events.
alwaysApply automatically when editing files in the cj_assessment_service directory

globs: 
  - "services/cj_assessment_service/**"
alwaysApply: false
---

# 020.7: CJ Assessment Service Architecture

## 1. Service Identity
- **Package**: `huleedu-cj-assessment-service`
- **Folders**: `services/cj_assessment_service/`
- **Ports**: 9090 (HTTP health / metrics), **Kafka**: Consumer/Producer, **DB**: postgres (configurable)
- **Stack**: Quart (health API), aiokafka, async SQLAlchemy, Dishka DI, aiohttp, Prometheus
- **Purpose**: Perform Comparative Judgement (CJ) assessments on essay batches using one or more LLM providers, publish rankings/failure events.

## 2. Integrated Application Architecture

### 2.1. Single Entrypoint (`app.py`)
- **HTTP Health API**: GET /healthz (JSON health status), GET /metrics (Prometheus)
- **Kafka Consumer**: Managed via @app.before_serving/@app.after_serving lifecycle hooks
- **Pattern**: Integrated Quart application following Rule 042 HTTP Service Pattern
- **Blueprint Registration**: Single health_bp with shared DI container

### 2.2. Event Processing
- **Consumes**: `huleedu.els.cj_assessment.requested.v1`
- **Produces**: `huleedu.cj_assessment.completed.v1`, `huleedu.cj_assessment.failed.v1`
- **Flow**: Event deserialization → `event_processor.process_single_message()` → `run_cj_assessment_workflow()` → publish result/failure

## 3. Core Components

| Module | Responsibility |
| ------ | -------------- |
| `event_processor.py` | Validates envelope, orchestrates workflow, constructs outgoing events |
| `cj_core_logic/*` | Pair generation, comparison workflow, Bradley-Terry scoring |
| `implementations/llm_provider_service_client.py` | HTTP client for centralized LLM Provider Service with 200/202 handling |
| `implementations/llm_interaction_impl.py` | Orchestrates concurrent LLM requests via service client |
| `implementations/db_access_impl.py` | Async SQLAlchemy repository (PostgreSQL) |
| `implementations/event_publisher_impl.py` | Kafka publishing abstraction |
| `protocols.py` | All **typing.Protocol** contracts (ContentClient, Cache, LLMProvider, Repository, EventPublisher, RetryManager, LLMInteraction) |
| `di.py` | `CJAssessmentServiceProvider` supplying Dishka providers & single `make_async_container` entry |

## 4. Dependency Injection (Dishka)
- DI container instantiated **before** Blueprint registration (see `app.py`) following Memory `55c2a6bd...`.
- **Scope**: `Scope.APP` for all provided components.
- Providers include LLM provider map and optional `MockLLMInteractionImpl` (controlled by `USE_MOCK_LLM`).

## 5. Event Contracts
- **Consumed**: `ELS_CJAssessmentRequestV1` (thin event, payload includes essay refs + overrides).
- **Published**:
  - `CJAssessmentCompletedV1` with rankings list.
  - `CJAssessmentFailedV1` with detailed `error_info`.
- **Envelope**: `EventEnvelope` with **top-level** `schema_version` (see Rule 052).
- **Correlation IDs**: Always `uuid.UUID`; test mocks must expect UUID objects (Memory 234cf4fe...).

## 6. Database Schema (Async SQLAlchemy)
- Tables: `cj_batches`, `cj_processed_essays`, `cj_comparison_pairs`, `cj_rankings`.
- Migration: Automatic `initialize_db_schema()` on worker startup.
- Connection: URL `DATABASE_URL_CJ` (default sqlite for dev, Postgres in prod), with pool settings in `config.py`.

## 7. LLM Interaction Flow
1. `pair_generation.py` selects comparison pairs.
2. For each pair a prompt is rendered from `ASSESSMENT_PROMPT_TEMPLATE`.
3. `LLMProviderServiceClient` makes HTTP request to centralized LLM Provider Service
   - **200 Response**: Immediate result processed directly
   - **202 Response**: Automatic polling with exponential backoff until completion
4. `LLMInteractionImpl.perform_comparisons()` orchestrates concurrent requests via client
5. `scoring_ranking.py` aggregates pairwise results using Bradley-Terry to final rankings.

## 8. Configuration (env vars / Settings)
Key vars from `Settings` (env prefix `CJ_ASSESSMENT_SERVICE_`):
- `LOG_LEVEL`, `USE_MOCK_LLM`, `KAFKA_BOOTSTRAP_SERVERS`, `DEFAULT_LLM_PROVIDER`.
- `DATABASE_URL_CJ`, pool tuning vars.
- `CONTENT_SERVICE_URL` for text fetch via `ContentClientImpl`.
- `LLM_PROVIDER_SERVICE_URL` for centralized LLM provider access.
- Queue polling settings: `LLM_QUEUE_POLLING_*` for 202 response handling.
- `METRICS_PORT` (default 9090).

## 9. Metrics & Logging
- Prometheus `CollectorRegistry` injected via DI; `/metrics` Blueprint exposes default & custom metrics.
- Core counters: processed messages, LLM calls, cache hits/misses, workflow duration.
- Logging through `huleedu_service_libs.logging_utils`; all log entries include `correlation_id` extra.

## 10. Security & Limits (Walking Skeleton)
- No auth; internal service only.
- DB credentials via env vars; no secrets in repo.
- Rate limit for LLM calls enforced by `RetryManagerImpl` back-off.
- Future: OAuth for external API, essay text PII scrubbing.

## 11. Deployment
- **Docker**: `python:3.11-slim`, multi-stage.
- **Entrypoint**: `app.py` (unified Quart application with integrated Kafka consumer)
- **Commands**: `pdm run start` for production, `pdm run dev` for development
- **Ports**: 9090 (metrics/health), no external HTTP API.
- **Lifecycle**: Service manages both HTTP API and Kafka consumer in single process

## 12. Limitations & Future Work
- Current DB schema minimal; migrate to dedicated migrations tool (e.g., Alembic).
- Ranking stability threshold heuristic to be tuned (`SCORE_STABILITY_THRESHOLD`).
- LLM provider selection static; future: dynamic provider fail-over.

---

**CRITICAL IMPLEMENTATION NOTES**
- DI container **must** be created *before* Blueprint registration – see Memory `55c2a6bd-...`.
- When serializing envelopes, use `json.dumps(envelope.model_dump(mode="json"))` to ensure UUIDs encode (Rule 026 §12.1 pattern applies here too).
- Tests must mock protocols rather than concrete classes (Rule 070 QA enhancements).