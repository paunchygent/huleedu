---
description: True Transactional Outbox Pattern for database-event consistency
globs: 
alwaysApply: false
---
# 042.1: True Transactional Outbox Pattern

## 1. Core Principle: Solving the Dual-Write Problem

**PURPOSE**: The outbox pattern exists to solve the **dual-write problem** - ensuring atomic consistency between database operations and event publishing. Events MUST be stored in the database within the same transaction as business data.

**CRITICAL**: This is NOT about Kafka resilience - it's about **transactional safety** and **data consistency**.

## 2. The Dual-Write Problem

**Problem Scenario**:
```python
# BROKEN: Non-atomic dual writes
async def process_order(order_data):
    await database.save_order(order_data)      # ✅ Succeeds
    await kafka.publish_order_event(order_data)  # ❌ Fails
    # Result: Order saved but no event published = INCONSISTENT STATE
```

**Solution**: Store events in database atomically, publish asynchronously via relay worker.

## 3. Implementation Requirements

### 3.1. Database Schema
```python
from huleedu_service_libs.outbox import EventOutbox
# Standard schema: id, aggregate_id, aggregate_type, event_type, event_data, event_key, topic, created_at, published_at, retry_count, last_error
# Required index: ix_event_outbox_unpublished_topic WHERE published_at IS NULL
```

### 3.2. TRUE OUTBOX PATTERN - Always Use Outbox
```python
async def publish_event(self, event_data, correlation_id, session=None):
    """CORRECT: Always use outbox for transactional safety."""
    
    # 1. Create event envelope
    envelope = EventEnvelope(
        event_type="domain.event.v1",
        source_service=self.settings.SERVICE_NAME,
        correlation_id=correlation_id,
        data=event_data,
    )
    
    # 2. ALWAYS store in outbox for atomic consistency
    await self.outbox_manager.publish_to_outbox(
        aggregate_type="domain_entity",
        aggregate_id=str(event_data.entity_id),
        event_type="domain.event.v1",
        event_data=envelope,  # Pass original Pydantic envelope
        topic=topic,
    )
    
    # 3. Relay worker publishes from outbox asynchronously
    # No direct Kafka calls - that defeats the purpose!
```

### 3.3. Unit of Work Pattern (Atomic Business + Events)
```python
# CORRECT: Business data and events in same transaction
async def handle_business_operation(self, data, correlation_id):
    async with self.session_factory() as session:
        async with session.begin():
            # 1. Business logic - update database
            result = await self.repository.update_entity(
                data, session=session
            )
            
            # 2. Store event in outbox within SAME transaction
            await self.event_publisher.publish_event(
                event_data=result,
                correlation_id=correlation_id,
                session=session  # Share transaction for atomicity
            )
            
            # Both committed atomically or both rolled back
```

### 3.4. Service Implementation Pattern

**MANDATORY shared implementation:**
```python
# di.py
from huleedu_service_libs.outbox.manager import OutboxManager

@provide(scope=Scope.APP)
def provide_outbox_manager(
    self, outbox_repository: OutboxRepositoryProtocol,
    redis_client: AtomicRedisClientProtocol, settings: Settings
) -> OutboxManager:
    return OutboxManager(
        outbox_repository=outbox_repository,
        redis_client=redis_client,
        service_name=settings.SERVICE_NAME
    )
```

**Publisher Pattern:**
```python
class EventPublisher:
    def __init__(self, outbox_manager: OutboxManager): ...
    
    async def publish_event(self, event_data, correlation_id, session=None):
        await self.outbox_manager.publish_to_outbox(
            event_data=EventEnvelope(...), session=session
        )
```

## 4. Event Relay Worker

**Purpose**: Asynchronously publish events from outbox to Kafka.

**Architecture**:
- **Wake-up**: Redis BLPOP on `outbox:wake:{service_name}` for instant processing
- **Polling**: Adaptive intervals (0.1s → 5s) based on ENVIRONMENT  
- **Batch**: Up to 100 events per batch with exponential backoff retry
- **Idempotency**: Handle duplicate events gracefully

## 5. Required Patterns

✅ **MANDATORY**: Always use outbox for event publishing
✅ **MANDATORY**: Store events in same transaction as business data
✅ **MANDATORY**: Pass original Pydantic envelopes to outbox
✅ **MANDATORY**: Relay worker publishes from outbox asynchronously
✅ **MANDATORY**: Redis notification for immediate relay worker wake-up

❌ **FORBIDDEN**: Direct Kafka publishing from business logic
❌ **FORBIDDEN**: Kafka-first with outbox fallback
❌ **FORBIDDEN**: Exception-based fallback patterns
❌ **FORBIDDEN**: Reconstructing events from error details
❌ **FORBIDDEN**: Dual-mode interfaces (dict + Pydantic)

## 6. Anti-Patterns to Avoid

### 6.1. Kafka-First Anti-Pattern (WRONG)
```python
# ❌ ANTI-PATTERN: Defeats the purpose of outbox pattern
try:
    await kafka_bus.publish(envelope)  # Try Kafka first
    return  # Success - no outbox needed
except Exception:
    await outbox.store(envelope)  # Only fallback on failure
```

### 6.2. Exception-Based Data Transport (WRONG)
```python
# ❌ ANTI-PATTERN: Using exceptions to pass data
try:
    await kafka_publish()
except KafkaError as e:
    # Passing envelope data through exception details
    await handle_fallback(e.envelope_data)
```

## 7. Testing Patterns

### 7.1. Test Expectations
```python
# CORRECT: Always expect outbox usage
assert mock_outbox_repository.add_event.called
assert not mock_kafka_bus.publish.called  # No direct Kafka

# Verify outbox contains correct data
call_args = mock_outbox_repository.add_event.call_args
assert call_args[1]["event_type"] == "expected.event.v1"
```

### 7.2. Integration Test Pattern
```python
async def test_business_operation_with_outbox():
    """Test that business data and events are atomically stored."""
    
    # Execute business operation
    result = await handler.process_business_data(data, correlation_id)
    
    # Verify business data persisted
    entity = await repository.get_entity(entity_id)
    assert entity.status == "processed"
    
    # Verify event stored in outbox atomically
    events = await outbox_repository.get_unpublished_events()
    assert len(events) == 1
    assert events[0].event_type == "business.processed.v1"
```

## 8. Metrics to Monitor

- **outbox_events_stored_total**: Total events stored in outbox
- **outbox_events_published_total**: Total events published by relay worker
- **outbox_processing_delay_seconds**: Time from store to publish
- **outbox_relay_worker_errors_total**: Relay worker failure count
- **business_operation_success_rate**: Overall business operation success
- **outbox_database_transaction_duration**: Transaction performance

## 9. Infrastructure Requirements

**Consolidated Implementation**: All 11 services use `huleedu_service_libs.outbox.manager.OutboxManager`

**Development Infrastructure**:
```toml
# pyproject.toml
[dependency-groups]
dev = ["ruff>=0.11.11", "mypy>=1.15.0", "pytest>=8.3.5", "pytest-asyncio>=0.26.0"]

[tool.pdm.scripts]
migrate = "alembic upgrade head"
start-dev = {composite = ["migrate", "start"]}
```

**Docker Environment Awareness**:
```dockerfile
CMD ["sh", "-c", "if [ \"$HULEEDU_ENVIRONMENT\" = 'production' ]; then pdm run start; else pdm run start-dev; fi"]
```

**Services Using Pattern**: batch_orchestrator, nlp, result_aggregator, spellchecker, class_management, cj_assessment, email, entitlements, essay_lifecycle, file, identity (11/11)