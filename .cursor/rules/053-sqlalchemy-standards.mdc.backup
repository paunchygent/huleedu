---
description: Read and follow SQLAlchemy implementations standards found here if your code contains SQL
globs: 
alwaysApply: false
---
# 053: SQLAlchemy Standards

## 1. Database Enums
- **MUST** use `str, enum.Enum` inheritance for database enums
- **FORBIDDEN**: Plain `enum.Enum` inheritance (causes SQLAlchemy KeyError)

```python
# ✅ CORRECT
class StatusEnum(str, enum.Enum):
    PENDING = "pending"
    COMPLETED = "completed"

# ❌ FORBIDDEN
class StatusEnum(enum.Enum):
    PENDING = "pending"
```

## 2. PostgreSQL Timestamps
- **MUST** use naive UTC timestamps for `TIMESTAMP WITHOUT TIME ZONE`
- **PATTERN**: Use `.replace(tzinfo=None)` to convert timezone-aware datetimes

```python
# ✅ CORRECT
updated_at=datetime.now(timezone.utc).replace(tzinfo=None)

# ❌ FORBIDDEN 
updated_at=datetime.now(timezone.utc)  # Causes timezone errors
```

## 3. Enum Field Configuration
- **REQUIRED**: Use `values_callable=lambda obj: [e.value for e in obj]` for PostgreSQL enums

```python
status: Mapped[StatusEnum] = mapped_column(
    SQLAlchemyEnum(
        StatusEnum,
        name="status_enum",
        values_callable=lambda obj: [e.value for e in obj]
    ),
    nullable=False,
)
```

## 4. JSON Field Datetime Serialization
- **MUST** convert datetime objects to ISO strings before storing in JSON fields
- **PATTERN**: Use `.isoformat()` for datetime serialization in update operations

```python
# ✅ CORRECT - Convert datetime to string for JSON storage
timeline_for_db = {k: v.isoformat() for k, v in timeline.items()}
stmt = update(Table).values(timeline=timeline_for_db)

# ❌ FORBIDDEN - Direct datetime storage causes JSON serialization error
stmt = update(Table).values(timeline=timeline)  # TypeError: datetime not JSON serializable
```

## 5. Database Migration Standards

All PostgreSQL services **MUST** follow the consolidated Alembic migration pattern established across the HuleEdu platform.

### Required Migration Infrastructure

**Service Structure**:
```
services/{service_name}/
├── alembic.ini                    # Service-specific configuration
├── alembic/
│   ├── __init__.py               # Package initialization
│   ├── env.py                    # Async SQLAlchemy environment
│   └── versions/                 # Migration files
│       └── YYYYMMDD_NNNN_description.py
├── models_db.py                  # SQLAlchemy models
├── config.py                     # Must include database_url property
└── pyproject.toml               # Must include alembic dependency and PDM scripts
```

### Required Dependencies

**pyproject.toml**:
```toml
dependencies = [
    "sqlalchemy[asyncio]",
    "asyncpg",
    "alembic",  # Required for migrations
]

[tool.pdm.scripts]
# Standardized migration commands (identical across all services)
migrate-upgrade = "alembic upgrade head"
migrate-downgrade = "alembic downgrade -1"
migrate-history = "alembic history"
migrate-current = "alembic current"
migrate-revision = "alembic revision --autogenerate"
migrate-stamp = "alembic stamp head"
```

### Configuration Integration

**config.py pattern**:
```python
class Settings(BaseSettings):
    # Database connection fields
    DB_HOST: str = Field(default="localhost")
    DB_PORT: int = Field(default=5432)
    DB_NAME: str = Field(default="service_db")
    DB_USER: str = Field(default="huleedu_user")
    DB_PASSWORD: str = Field(default="REDACTED_DEFAULT_PASSWORD")
    
    @property
    def database_url(self) -> str:
        """Database URL for Alembic migration configuration."""
        return f"postgresql+asyncpg://{self.DB_USER}:{self.DB_PASSWORD}@{self.DB_HOST}:{self.DB_PORT}/{self.DB_NAME}"
```

### Async Migration Environment

**alembic/env.py pattern**:
```python
import asyncio
from alembic import context
from sqlalchemy import pool
from sqlalchemy.ext.asyncio import async_engine_from_config

from services.{service_name}.config import settings
from services.{service_name}.models_db import Base

config = context.config
config.set_main_option("sqlalchemy.url", settings.database_url)
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(url=url, target_metadata=target_metadata, literal_binds=True)
    with context.begin_transaction():
        context.run_migrations()

async def run_async_migrations() -> None:
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    async with connectable.connect() as connection:
        await connection.run_sync(lambda conn: context.configure(
            connection=conn, target_metadata=target_metadata
        ))
        await connection.run_sync(lambda conn: context.run_migrations())
    await connectable.dispose()

def run_migrations_online() -> None:
    asyncio.run(run_async_migrations())
```

### Service-Specific Configuration

**alembic.ini**:
```ini
[alembic]
script_location = alembic
# Database URL dynamically loaded from service configuration

[loggers]
keys = root,sqlalchemy,alembic
# ... standard logging configuration
```

### Usage Commands

**Development workflow**:
```bash
# Apply pending migrations
pdm run migrate-upgrade

# Generate new migration after model changes
pdm run migrate-revision "description_of_changes"

# View migration history
pdm run migrate-history

# Check current migration
pdm run migrate-current

# Rollback one migration (use with caution)
pdm run migrate-downgrade
```

### Implementation Requirements

- **Service Isolation**: Each service has independent migration history
- **Async Consistency**: All migration environments use async SQLAlchemy patterns
- **Configuration Integration**: Dynamic database URLs from service settings
- **Naming Standardization**: All services use identical `migrate-*` PDM commands
- **Zero Disruption**: Baseline migrations preserve existing schemas exactly
- **Outbox Integration**: Services using outbox pattern **MUST** include `EventOutbox` in migrations

## 6. Transactional Outbox Pattern

### 6.1. EventOutbox Table Requirements

**MUST** include `event_outbox` table for reliable event publishing:

```python
from huleedu_service_libs.outbox import EventOutbox

# Import in models_db.py for Alembic detection
class EventOutbox(Base):
    __tablename__ = "event_outbox"
    
    id: Mapped[UUID] = mapped_column(PostgreSQL_UUID(as_uuid=True), primary_key=True)
    aggregate_id: Mapped[str] = mapped_column(String(255), nullable=False)
    aggregate_type: Mapped[str] = mapped_column(String(100), nullable=False)
    event_type: Mapped[str] = mapped_column(String(255), nullable=False)
    event_data: Mapped[dict[str, Any]] = mapped_column(JSON, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
    published_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)
    retry_count: Mapped[int] = mapped_column(Integer, default=0, server_default="0")
```

### 6.2. Required Performance Indexes

**MUST** include these indexes for optimal relay worker performance:

```python
# In Alembic migration
def upgrade() -> None:
    # ... table creation ...
    
    # Critical index for relay worker polling
    op.create_index(
        'ix_event_outbox_unpublished', 
        'event_outbox', 
        ['published_at', 'created_at'],
        postgresql_where=sa.text('published_at IS NULL')
    )
    
    # Index for aggregate-based queries
    op.create_index(
        'ix_event_outbox_aggregate', 
        'event_outbox', 
        ['aggregate_type', 'aggregate_id']
    )
```

### 6.3. Session Sharing Pattern

**MUST** share database session between business logic and outbox:

```python
async def business_operation_with_event(self, data, correlation_id):
    async with self.repository.get_session() as session:
        # Business logic
        result = await self.repository.update_entity(
            entity_id=data.entity_id,
            updates=data.updates,
            session=session,  # Share session
        )
        
        # Store event in same transaction
        await self.outbox_repository.add_event(
            aggregate_id=str(data.entity_id),
            event_data={"result": result.model_dump()},
            session=session,  # Same session for atomicity
        )
        
        await session.commit()  # Atomic commit
```

===

