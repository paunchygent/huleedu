---
id: "020.13-llm-provider-service-architecture"
type: "service"
created: 2025-07-03
last_updated: 2025-11-19
scope: "backend"
parent_rule: "020-architectural-mandates"
service_name: "llm_provider_service"
---

# 020.13: LLM Provider Service Architecture

## 1. Service Identity
- **Package**: `huleedu-llm-provider-service`
- **Folders**: `services/llm_provider_service/`
- **Port**: 8080, **Stack**: Quart + Redis + Circuit Breakers + OpenTelemetry + Connection Pools
- **Purpose**: Centralized LLM provider abstraction with queue-based resilience, model manifest management, and connection optimization
- **Critical**: NO response caching - preserves psychometric validity

## 1.5. Model Manifest (`model_manifest.py`)

### 1.5.1. Single Source of Truth
- **File**: `services/llm_provider_service/model_manifest.py:74-350`
- **Purpose**: Centralized registry of all model versions, capabilities, and metadata across providers
- **Usage**: `get_model_config(ProviderName, model_id)` returns `ModelConfig` with model metadata

### 1.5.2. Integration Pattern
```python
# Upstream services query manifest before publishing events
config = get_model_config(ProviderName.ANTHROPIC, "claude-haiku-4-5-20251001")
overrides = LLMConfigOverrides(
    provider_override=LLMProviderType.ANTHROPIC,
    model_override=config.model_id,
    temperature_override=0.3,
)
```

### 1.5.3. Validation Pattern (CLI/Batch Runners)
- **CLI validation**: `scripts/cj_experiments_runners/eng5_np/cli.py:45-140`
- **Pattern**: Validate model against manifest BEFORE publishing Kafka events
- **Graceful degradation**: Warn if manifest unavailable, proceed with runtime validation

## 2. Response Pattern (202 + Kafka callback)

### 2.1. Request Flow (post-refactor truth)
```text
POST /api/v1/comparison → ALWAYS 202 queued + queue_id
Callback delivered to caller-supplied topic (LLMComparisonResultV1 envelope)
```

- There is **no immediate 200 path** after the refactor; `LLMOrchestrator.perform_comparison` always enqueues and the Queue Processor publishes callbacks.
- There are **no HTTP polling endpoints** (`/status` or `/results`). Consumers must consume the Kafka topic provided in `callback_topic` on the request.
- Callback metadata preserves all caller-supplied `metadata` keys and appends `prompt_sha256`, provider/model used, queue timing, and prompt-cache usage metrics when available.

## 3. Queue Resilience Architecture

### 3.1. Resilient Queue Manager
```text
Redis Queue (persistent) → Local Queue (fallback) → 503 Rejection
ResilientQueueManagerImpl: Automatic failover with migration tracking
```

### 3.2. Queue Implementation Details
- **Primary**: `RedisQueueRepositoryImpl` with persistent storage
- **Fallback**: `LocalQueueManagerImpl` with 100MB memory limit  
- **Health Monitoring**: Automatic queue health checks and failover
- **Migration Tracking**: Request transfer between queue implementations

### 3.3. Circuit Breaker Integration
- **Provider Check**: Circuit breaker state checked BEFORE processing
- **Queue Trigger**: Unavailable providers → automatic queuing
- **Recovery**: Background `QueueProcessorImpl` handles queued requests when providers recover
- **Metrics**: Circuit breaker metrics bridge with observability integration

## 4. Client Integration Pattern

### 4.1. Callback-Only Flow (Required)
```python
# Publish comparison request
payload = LLMComparisonRequest(
    user_prompt=rendered_prompt,
    prompt_blocks=prompt_blocks,
    callback_topic="huleedu.llm_provider.comparison_result.v1",
    llm_config_overrides=overrides,
    metadata={"essay_a_id": a, "essay_b_id": b, **extra_meta},
)
resp = await http_client.post("/api/v1/comparison", json=payload.model_dump())
assert resp.status == 202
queue_id = resp.json()["queue_id"]

# Consume Kafka callback
async for envelope in kafka_consumer:
    event = LLMComparisonResultV1.model_validate(envelope.data)
    if event.request_id == str(queue_id):
        handle_result(event)
```

- Consumers **must** subscribe to the `callback_topic` they send; there is no HTTP polling path.
- `request_metadata` is echoed verbatim on the callback with additive fields (`prompt_sha256`, provider/model, queue timings, prompt cache usage).

## 5. Provider Protocol Requirements

### 5.1. Structured Output (ALL Providers)
```python
# Required response format
{
    "winner": "Essay A" | "Essay B",  
    "justification": "string (50-500 chars)",
    "confidence": 1.0-5.0  # float, not integer
}
```

### 5.2. Provider Implementation Pattern
- **Anthropic**: Tool use/function calling for structured responses
- **OpenAI**: `response_format={"type": "json_object"}` with schema
- **Google**: `response_mime_type="application/json"` with generation config
- **OpenRouter**: Conditional logic based on model capabilities

### 5.3. Callback Metadata Echo (NEW)
- **Echo Back Request Metadata**: Whatever arrives in `LLMComparisonRequest.metadata` **must** be copied into `LLMComparisonResultV1.request_metadata` when publishing callbacks. Do not mutate or drop caller-supplied keys (e.g., `essay_a_id`, `essay_b_id`).
- **Prompt Hash Requirement**: Each provider implementation **must** compute `prompt_sha256 = sha256(full_prompt.encode("utf-8"))` immediately after rendering the final prompt string actually sent to the LLM, and store it in `LLMProviderResponse.metadata`. The queue processor merges this hash into the callback metadata.
- **Determinism**: The hash must be computed on the exact prompt text delivered to the provider to keep replay/validation deterministic.

## 6. Critical Configuration

### 6.1. Queue Settings
```bash
LLM_PROVIDER_SERVICE_QUEUE_MAX_SIZE=1000
LLM_PROVIDER_SERVICE_QUEUE_MAX_MEMORY_MB=100
LLM_PROVIDER_SERVICE_QUEUE_REQUEST_TTL_HOURS=4
```

### 6.2. Mock Provider Controls
```bash
# Make mock provider always available (alongside real providers)
LLM_PROVIDER_SERVICE_ALLOW_MOCK_PROVIDER=true
# Deterministic seed for mock responses
LLM_PROVIDER_SERVICE_MOCK_PROVIDER_SEED=42
# Full mock mode (replaces all providers with mock)
LLM_PROVIDER_SERVICE_USE_MOCK_LLM=false
# Optional realism knobs (all default to no-op for CI)
LLM_PROVIDER_SERVICE_MOCK_LATENCY_MS=0
LLM_PROVIDER_SERVICE_MOCK_LATENCY_JITTER_MS=0
LLM_PROVIDER_SERVICE_MOCK_ERROR_RATE=0.0
LLM_PROVIDER_SERVICE_MOCK_ERROR_CODES="429,503,500"
LLM_PROVIDER_SERVICE_MOCK_CACHE_ENABLED=true
LLM_PROVIDER_SERVICE_MOCK_CACHE_HIT_RATE=0.0
LLM_PROVIDER_SERVICE_MOCK_TOKENIZER=simple   # or tiktoken
LLM_PROVIDER_SERVICE_MOCK_TOKENS_PER_WORD=0.75
LLM_PROVIDER_SERVICE_MOCK_CONFIDENCE_BASE=4.5
LLM_PROVIDER_SERVICE_MOCK_CONFIDENCE_JITTER=0.3
LLM_PROVIDER_SERVICE_MOCK_STREAMING_METADATA=false

Example dev profile (.env.local):
```bash
LLM_PROVIDER_SERVICE_ALLOW_MOCK_PROVIDER=true
LLM_PROVIDER_SERVICE_USE_MOCK_LLM=false
LLM_PROVIDER_SERVICE_MOCK_PROVIDER_SEED=42
LLM_PROVIDER_SERVICE_MOCK_LATENCY_MS=80
LLM_PROVIDER_SERVICE_MOCK_LATENCY_JITTER_MS=40
LLM_PROVIDER_SERVICE_MOCK_ERROR_RATE=0.02
LLM_PROVIDER_SERVICE_MOCK_ERROR_CODES="429,503"
LLM_PROVIDER_SERVICE_MOCK_CACHE_ENABLED=true
LLM_PROVIDER_SERVICE_MOCK_CACHE_HIT_RATE=0.5
LLM_PROVIDER_SERVICE_MOCK_TOKENIZER=simple
LLM_PROVIDER_SERVICE_MOCK_TOKENS_PER_WORD=0.75
LLM_PROVIDER_SERVICE_MOCK_CONFIDENCE_BASE=4.4
LLM_PROVIDER_SERVICE_MOCK_CONFIDENCE_JITTER=0.3
LLM_PROVIDER_SERVICE_MOCK_STREAMING_METADATA=false
```
```

### 6.2. Development Settings
```bash
LLM_PROVIDER_SERVICE_USE_MOCK_LLM=false
LLM_PROVIDER_SERVICE_RECORD_LLM_RESPONSES=false  # Dev only, never cache
```

## 7. Performance Optimization

### 7.1. Connection Pool Management
```python
# ConnectionPoolManagerImpl provides optimized HTTP sessions
- Provider-specific connection pools with keep-alive
- Automatic connection cleanup and resource management
- Concurrent request limits per provider
- Session-level retry and timeout configuration
```

### 7.2. OpenTelemetry Tracing
- **Span Creation**: Full span tracking for LLM requests and queue operations
- **Trace Context**: Preserves trace context across queue processing
- **Queue Tracing**: Request journey tracking from queue to completion
- **Provider Tracing**: Individual provider call instrumentation

## 8. Key Implementation Rules

### 8.1. File Structure (Critical)
- **API Models**: `api_models.py`
- **Internal Models**: `internal_models.py`
- **Blueprints**: `health_routes.py`, `llm_routes.py`
- **Implementations**: Provider-specific classes in `implementations/`

### 8.2. NO Caching Policy
- **FORBIDDEN**: Response caching that returns identical results
- **Required**: Fresh LLM response for every request
- **Development**: Use response recorder for API validation, not caching
- **Prompt caching only**: The Anthropic/OpenAI implementations may cache **prompt blocks** to reuse provider-side prompt caches; this does **not** cache model outputs and is safe for psychometric validity.

### 8.3. Queue Processing
- **TTL Enforcement**: 4-hour expiration with automatic cleanup
- **Retry Logic**: Up to 3 attempts with exponential backoff
- **Status Tracking**: QUEUED → PROCESSING → COMPLETED/FAILED/EXPIRED
- **Watermarks**: High 80%, Low 60% for memory management

### 8.4. Error Handling
- **503**: Queue at capacity or service unavailable
- **410**: Queue result expired
- **404**: Queue ID not found
- **202**: Request queued (from comparison) or not ready (from results)

---

**CRITICAL IMPLEMENTATION NOTES**
- **Port**: Service runs on 8080
- **Connection Pools**: `ConnectionPoolManagerImpl` provides provider-specific optimized sessions
- **Queue Resilience**: `ResilientQueueManagerImpl` automatically handles Redis/Local queue failover
- **202 Pattern**: All consumers MUST implement polling logic with exponential backoff
- **NO Caching**: Service preserves psychometric validity by never caching LLM responses
- **Tracing**: Full OpenTelemetry integration maintains trace context across queue operations
