---
description: Defines the CJ Assessment Service. An integrated Quart application with Kafka consumer performing Comparative Judgement on essays via centralized LLM Provider Service. Includes circuit breaker resilience and comprehensive observability.
alwaysApply automatically when editing files in the cj_assessment_service directory

globs: 
  - "services/cj_assessment_service/**"
alwaysApply: false
---

# 020.7: CJ Assessment Service Architecture

## 1. Service Identity
- **Package**: `huleedu-cj-assessment-service`
- **Folders**: `services/cj_assessment_service/`
- **Ports**: 9095 (HTTP health / metrics), **Kafka**: Consumer/Producer, **DB**: PostgreSQL (port 5434)
- **Stack**: Integrated Quart application, aiokafka, async SQLAlchemy, Dishka DI, OpenTelemetry, Circuit Breakers, Redis
- **Purpose**: Perform Comparative Judgement (CJ) assessments on essay batches using centralized LLM Provider Service, publish rankings/failure events with comprehensive resilience patterns.

## 2. Integrated Application Architecture

### 2.1. Single Entrypoint (`app.py`)
- **HTTP Health API**: GET /healthz (JSON health status), GET /metrics (Prometheus)
- **Kafka Consumer**: Managed via @app.before_serving/@app.after_serving lifecycle hooks
- **Pattern**: Integrated Quart application following Rule 042 HTTP Service Pattern
- **Blueprint Registration**: Single health_bp with shared DI container

### 2.2. Event Processing
- **Consumes**: `huleedu.els.cj_assessment.requested.v1`
- **Produces**: `huleedu.cj_assessment.completed.v1`, `huleedu.cj_assessment.failed.v1`
- **Flow**: Event deserialization → `event_processor.process_single_message()` → `run_cj_assessment_workflow()` → publish result/failure

## 3. Core Components

| Module | Responsibility |
| ------ | -------------- |
| `event_processor.py` | Validates envelope, orchestrates workflow, constructs outgoing events |
| `cj_core_logic/*` | Pair generation, comparison workflow, Bradley-Terry scoring |
| `implementations/llm_provider_service_client.py` | HTTP client for centralized LLM Provider Service with 200/202 handling |
| `implementations/llm_interaction_impl.py` | Orchestrates concurrent LLM requests via service client |
| `implementations/db_access_impl.py` | Async SQLAlchemy repository (PostgreSQL) |
| `implementations/event_publisher_impl.py` | Kafka publishing abstraction |
| `protocols.py` | All **typing.Protocol** contracts (ContentClient, Cache, LLMProvider, Repository, EventPublisher, RetryManager, LLMInteraction) |
| `di.py` | `CJAssessmentServiceProvider` supplying Dishka providers & single `make_async_container` entry |

## 4. Dependency Injection (Dishka)
- DI container instantiated **before** Blueprint registration (see `app.py`) following Memory `55c2a6bd...`.
- **Scope**: `Scope.APP` for all provided components.
- Providers include LLM provider map and optional `MockLLMInteractionImpl` (controlled by `USE_MOCK_LLM`).

## 5. Event Contracts
- **Consumed**: `ELS_CJAssessmentRequestV1` (thin event, payload includes essay refs + overrides).
- **Published**:
  - `CJAssessmentCompletedV1` with rankings list.
  - `CJAssessmentFailedV1` with detailed `error_info`.
- **Envelope**: `EventEnvelope` with **top-level** `schema_version` (see Rule 052).
- **Correlation IDs**: Always `uuid.UUID`; test mocks must expect UUID objects (Memory 234cf4fe...).

## 6. Database Schema & Ranking Strategy (Async SQLAlchemy)
- **Tables**: `cj_batch_uploads`, `cj_processed_essays`, `cj_comparison_pairs`
- **Key Relationships**: ELS essay IDs as string primary keys, CASCADE foreign keys
- **Ranking Strategy**: Rankings generated dynamically from `ProcessedEssay.current_bt_score` via `get_essay_rankings()`
- **No Persistent Rankings**: Rankings computed on-demand, not stored separately (intentional design)
- **Rationale**: Rankings deterministically derivable from Bradley-Terry scores, avoids data duplication
- **Migration**: Automatic `initialize_db_schema()` on service startup
- **Connection**: `DATABASE_URL_CJ` (SQLite dev, PostgreSQL prod), **Pool Config**: size=10, max_overflow=20, pre_ping=True, recycle=3600

## 7. LLM Interaction Flow
1. `pair_generation.py` selects comparison pairs.
2. For each pair a prompt is rendered from `ASSESSMENT_PROMPT_TEMPLATE`.
3. `LLMProviderServiceClient` makes HTTP request to centralized LLM Provider Service
   - **200 Response**: Immediate result processed directly
   - **202 Response**: Automatic polling with exponential backoff until completion
4. `LLMInteractionImpl.perform_comparisons()` orchestrates concurrent requests via client
5. `scoring_ranking.py` aggregates pairwise results using Bradley-Terry to final rankings.

## 8. Configuration (env vars / Settings)
Key vars from `Settings` (env prefix `CJ_ASSESSMENT_SERVICE_`):
- **Core**: `LOG_LEVEL`, `USE_MOCK_LLM`, `KAFKA_BOOTSTRAP_SERVERS`, `DEFAULT_LLM_PROVIDER`
- **Database**: `DATABASE_URL_CJ`, `DATABASE_POOL_SIZE`, `DATABASE_MAX_OVERFLOW`, `DATABASE_POOL_PRE_PING`, `DATABASE_POOL_RECYCLE`
- **Services**: `CONTENT_SERVICE_URL`, `LLM_PROVIDER_SERVICE_URL`
- **Queue Polling** (8 settings): `LLM_QUEUE_POLLING_ENABLED`, `*_INITIAL_DELAY_SECONDS`, `*_MAX_DELAY_SECONDS`, `*_EXPONENTIAL_BASE`, `*_MAX_ATTEMPTS`, `*_TOTAL_TIMEOUT_SECONDS`
- **Circuit Breaker**: `CIRCUIT_BREAKER_ENABLED`, `*_FAILURE_THRESHOLD`, `*_RECOVERY_TIMEOUT`, `*_SUCCESS_THRESHOLD`
- **Metrics**: `METRICS_PORT` (default 9095), Redis idempotency TTL settings

## 9. Resilience & Observability
### 9.1. Circuit Breaker Protection
- **Kafka Publishing**: `ResilientKafkaPublisher` with circuit breaker delegation
- **Configuration**: Failure threshold, recovery timeout, success threshold
- **Fallback**: Failed messages queued for retry when circuit opens
- **Metrics**: Circuit breaker state changes and call results

### 9.2. Metrics & Tracing
- **Prometheus**: `CollectorRegistry` injected via DI, exposed on `/metrics`
- **Core Counters**: Processed messages, LLM calls, queue latency, workflow duration, circuit breaker metrics
- **OpenTelemetry**: Full span creation and trace context propagation
- **Logging**: `huleedu_service_libs.logging_utils` with `correlation_id` extra, idempotency tracking

## 10. Security & Limits (Walking Skeleton)
- No auth; internal service only.
- DB credentials via env vars; no secrets in repo.
- Rate limit for LLM calls enforced by `RetryManagerImpl` back-off.
- Future: OAuth for external API, essay text PII scrubbing.

## 11. Deployment
- **Docker**: `python:3.11-slim`, multi-stage.
- **Entrypoint**: `app.py` (unified Quart application with integrated Kafka consumer)
- **Commands**: `pdm run start` for production, `pdm run dev` for development
- **Ports**: 9095 (metrics/health), no external HTTP API.
- **Lifecycle**: Service manages both HTTP API and Kafka consumer in single process

## 12. Limitations & Future Work
- Current DB schema minimal; migrate to dedicated migrations tool (e.g., Alembic).
- Ranking stability threshold heuristic to be tuned (`SCORE_STABILITY_THRESHOLD`).
- LLM provider selection static; future: dynamic provider fail-over.

---

**CRITICAL IMPLEMENTATION NOTES**
- **DI Container**: Must be created *before* Blueprint registration in `app.py`
- **Idempotency**: Redis-based with 24-hour TTL using deterministic event IDs
- **Queue Polling**: Exponential backoff (1.5x base, max 60s, 15min total timeout) for 202 responses
- **Circuit Breaker**: Optional but recommended for Kafka publishing resilience
- **UUID Serialization**: Use `json.dumps(envelope.model_dump(mode="json"))` for proper UUID encoding
- **Testing**: Mock protocols, not implementations; expect UUID objects in correlation IDs
