---
id: "020.7-cj-assessment-service"
type: "service"
created: 2025-06-27
last_updated: 2025-11-30
scope: "backend"
parent_rule: "020-architectural-mandates"
service_name: "cj_assessment_service"
---

# 020.7: CJ Assessment Service Architecture

## 1. Service Identity
- **Package**: `huleedu-cj-assessment-service`
- **Folders**: `services/cj_assessment_service/`
- **Ports**: 9090 (HTTP health / metrics), **Kafka**: Consumer/Producer, **DB**: PostgreSQL (port 5434)
- **Stack**: Integrated Quart application, aiokafka, async SQLAlchemy, Dishka DI, OpenTelemetry, Circuit Breakers, Redis
- **Purpose**: Perform Comparative Judgement (CJ) assessments on essay batches using centralized LLM Provider Service, publish rankings/failure events with comprehensive resilience patterns. Implements a psychometrically robust 8-anchor Swedish national exam grade system.

## 2. Integrated Application Architecture

### 2.1. Single Entrypoint (`app.py`)
- **HTTP Health API**: GET /healthz (JSON health status), GET /metrics (Prometheus)
- **Kafka Consumer**: Managed via @app.before_serving/@app.after_serving lifecycle hooks
- **Pattern**: Integrated Quart application following Rule 042 HTTP Service Pattern
- **Blueprint Registration**: Single health_bp with shared DI container
- **Legacy Removal**: `worker_main.py` was removed in the refactor; all runtime surfaces flow through `app.py` + `kafka_consumer.py`.

### 2.2. Event Processing
- **Consumes**: 
  - `huleedu.els.cj_assessment.requested.v1` (assessment requests)
  - `huleedu.llm_provider.comparison_result.v1` (LLM callbacks)
- **Produces**: `huleedu.cj_assessment.completed.v1`, `huleedu.cj_assessment.failed.v1`
- **Flow**: 
  - Requests: Event → `process_single_message()` → `run_cj_assessment_workflow()` → submit comparisons
  - Callbacks: Event → `process_llm_result()` → `continue_cj_assessment_workflow()` → state updates → publish result
  - Batch preparation hydrates `student_prompt_storage_id` from the authoritative
    `assessment_instructions` record when the incoming request lacks a prompt
    reference but the instruction provides one. The hydrated storage reference is
    subsequently resolved to prompt text via the Content Service during batch
    metadata construction.

## 3. Core Components

| Module | Responsibility |
| ------ | -------------- |
| `event_processor.py` | Validates envelope, orchestrates workflow, constructs outgoing events, processes callbacks |
| `cj_core_logic/*` | Pair generation, comparison workflow, Bradley-Terry scoring, workflow state management |
| `cj_core_logic/workflow_context.py` | Builds `ContinuationContext` from batch state, processing metadata, BT SE diagnostics, and coverage/small-net flags |
| `cj_core_logic/workflow_decision.py` | Pure continuation decision logic (`ContinuationDecision`, `decide(ctx)`) implementing PR‑2/PR‑7 semantics |
| `cj_core_logic/workflow_diagnostics.py` | Side-effect-only diagnostics layer for continuation metrics (BT SE batch quality, per-decision counters) derived from `ContinuationContext` and `ContinuationDecision` |
| `cj_core_logic/workflow_continuation.py` | Orchestrates scoring/resampling/finalization; calls into `workflow_context`, `workflow_decision`, `workflow_diagnostics`, and `BatchFinalizer` |
| `implementations/llm_provider_service_client.py` | HTTP client for LLM Provider Service (event-driven, no polling) |
| `implementations/llm_interaction_impl.py` | Orchestrates concurrent LLM requests via service client |
| `implementations/batch_repository.py` | Batch + state persistence; FOR UPDATE paths disable eager relationships with `noload()` |
| `implementations/essay_repository.py` | Essay persistence and processing metadata merge helpers |
| `implementations/comparison_repository.py` | Comparison pair storage and callback correlation lookups |
| `implementations/instruction_repository.py` | Assessment instruction + student prompt reference persistence |
| `implementations/anchor_repository.py` | Anchor reference storage and grade-scale validation |
| `implementations/grade_projection_repository.py` | Grade projection persistence and lookup helpers |
| `implementations/event_publisher_impl.py` | Kafka publishing abstraction |
| `batch_monitor.py` | Background monitoring for stuck batch detection and recovery |
| `protocols.py` | All **typing.Protocol** contracts (ContentClient, Cache, LLMProvider, Repository, EventPublisher, RetryManager, LLMInteraction) |
| `di.py` | `CJAssessmentServiceProvider` supplying Dishka providers & single `make_async_container` entry |
| `api/admin_routes.py` | Admin endpoints for instructions (`/admin/v1/assessment-instructions`) and student prompts (`/admin/v1/student-prompts`) including storage-by-reference upload and retrieval workflows |
| `cli_admin.py` | Typer CLI surface providing `prompts upload|get` commands that wrap the admin API and integrate with instruction creation |

### 3.1. Comparison Generation & Batching
- **Wave size**: Per-wave pair counts are determined by the active `PairMatchingStrategyProtocol`. The default `OptimalGraphMatchingStrategy` produces up to `n_essays // 2` pairs per wave (perfect matching) after odd-count handling and exclusion of existing pairs. `compute_wave_size(n_essays)` **must** return this theoretical maximum; callers use it for observability/metrics, not hard caps.
- **Global caps**: Global safety caps **must** use `Settings.MAX_PAIRWISE_COMPARISONS` (and any per-request `max_comparisons_override`). `generate_comparison_tasks` enforces this by short-circuiting when the batch reaches the configured limit. Budgets are **caps**, not obligations – early stop on stability is allowed.
- **Upstream hints**: When upstream tooling (e.g., ENG5 runner) supplies a `max_comparisons` hint, capture it in batch/request metadata and map it onto `MAX_PAIRWISE_COMPARISONS` and the **stability cadence** knobs (`COMPARISONS_PER_STABILITY_CHECK_ITERATION`, `MIN_COMPARISONS_FOR_STABILITY_CHECK`) within CJ – never assume the runner sliced essays ahead of time.
- **Batching modes**: `Settings.LLM_BATCHING_MODE` defaults to `serial_bundle`. `per_request` is reserved for legacy/testing only, and `provider_batch_api` is for true provider-native batch calls. Mode selection is resolved via `resolve_effective_llm_batching_mode()` with provider guardrails.
- **Serial bundles**: In `serial_bundle` mode CJ submits comparisons in waves, waits for callbacks, recomputes BT scores, and uses the **stability and cadence configuration** (`MIN_COMPARISONS_FOR_STABILITY_CHECK`, `COMPARISONS_PER_STABILITY_CHECK_ITERATION`, `SCORE_STABILITY_THRESHOLD`, `MAX_ITERATIONS`, and BT SE / success-rate thresholds) to decide whether to finalize early or submit another wave. LLM Provider Service uses `cj_llm_batching_mode` (and optional `comparison_iteration`) in request metadata to aggregate multiple comparisons into single provider calls for rate-limit efficiency.

#### 3.1.1 Stability Cadence vs Wave Size (Design Note)

- **Wave size (supply)**:
  - Defined by the matching strategy for the current wave: after odd-count handling and existing-pair exclusion, the default strategy returns up to `floor(n_essays / 2)` new pairs.
  - `compute_wave_size(n)` exposes this theoretical maximum and is used for logging/metrics; it MUST NOT be used as a hard cap by callers.
- **Stability cadence (demand)**:
  - `COMPARISONS_PER_STABILITY_CHECK_ITERATION` is a **hint** that controls how many *additional* comparisons CJ aims to accumulate between stability checks, not “comparisons per wave”.
  - Together with `MIN_COMPARISONS_FOR_STABILITY_CHECK`, it determines when `workflow_continuation.trigger_existing_workflow_continuation` evaluates convergence.
- **Finalization criteria (PR 1 alignment)**:
  - Stability evaluation (in continuation) MUST consider:
    - `callbacks_received >= MIN_COMPARISONS_FOR_STABILITY_CHECK`
    - `max_score_change <= SCORE_STABILITY_THRESHOLD`
    - `mean_bt_se <= TARGET_MEAN_SE`
    - `success_rate >= MIN_SUCCESS_RATE_THRESHOLD`
  - `COMPARISONS_PER_STABILITY_CHECK_ITERATION` feeds into the “how often do we re-check?” loop (e.g., after roughly that many new comparisons) but **does not** constrain how many pairs the matching strategy may propose per wave.
  - `MAX_PAIRWISE_COMPARISONS` remains the only hard global cap; reaching it is always a valid reason to stop requesting new waves even if stability has not yet been achieved (CJ may then route into retry/failure flows as defined in PR‑2).

### 3.2. Continuation Context, Decisions, and Diagnostics
- **ContinuationContext**:
  - Defined in `cj_core_logic/workflow_context.py` as a JSON-safe dataclass capturing all derived continuation state:
    - Core counters and caps: `callbacks_received`, `denominator`, `pairs_submitted`, `max_pairs_cap`, `pairs_remaining`, `budget_exhausted`, `callbacks_reached_cap`.
    - PR‑2 fields: `max_score_change`, `success_rate`, `success_rate_threshold`, `zero_successes`, `below_success_threshold`, `should_fail_due_to_success_rate`, `stability_passed`, `should_finalize`.
    - PR‑7 fields: `expected_essay_count`, `is_small_net`, `max_possible_pairs`, `successful_pairs_count`, `unique_coverage_complete`, `resampling_pass_count`, `small_net_resampling_cap`, `small_net_cap_reached`.
    - BT SE / coverage fields: `bt_se_summary`, `bt_quality_flags`, `bt_se_inflated`, `comparison_coverage_sparse`, `has_isolated_items`, plus `metadata_updates` for `CJBatchState.processing_metadata`.
  - `build_bt_metadata_updates(...)` and `build_small_net_context(...)` are pure helpers that derive these fields and metadata from `CJBatchState`, BT SE summaries, and coverage metrics without performing I/O.
- **ContinuationDecision**:
  - Enum defined in `cj_core_logic/workflow_decision.py` with values:
    - `WAIT_FOR_CALLBACKS`, `FINALIZE_SCORING`, `FINALIZE_FAILURE`, `REQUEST_MORE_COMPARISONS`, `NO_OP`.
  - `decide(ctx: ContinuationContext) -> ContinuationDecision` is **pure** and encodes:
    - PR‑2 stability-first behaviour (finalize on stability / denominator / budget / small-net cap).
    - PR‑2 success-rate guards (route batches with low/zero success rates into failure when caps fire).
    - PR‑7 small-net resampling semantics (only request more comparisons when small-net caps are not reached and pairs remain).
- **Diagnostics layer**:
  - `workflow_diagnostics.record_bt_batch_quality(ctx)`:
    - Increments BT SE batch-quality counters **diagnostically only**:
      - `cj_bt_se_inflated_batches_total` when `ctx.bt_se_summary` is present and `ctx.bt_se_inflated` is True.
      - `cj_bt_sparse_coverage_batches_total` when `ctx.bt_se_summary` is present and `ctx.comparison_coverage_sparse` is True.
  - `workflow_diagnostics.record_workflow_decision(ctx, decision)`:
    - Increments `cj_workflow_decisions_total{decision=...}` using `ContinuationDecision.value` as the `decision` label.
    - Called from `workflow_continuation.trigger_existing_workflow_continuation` immediately after `decision = decide(ctx)`, ensuring metrics and the `“Continuation decision evaluated”` structured log share the same vocabulary.
  - **Guardrail**: Diagnostics are strictly observational:
    - They MUST NOT alter `ContinuationContext`, PR‑2 thresholds (stability/success-rate), PR‑7 small-net caps, or BT SE / coverage semantics.
    - Continuation decisions and resampling behaviour remain fully determined by pure logic in `workflow_context` + `workflow_decision`.

## 4. Dependency Injection (Dishka)
- DI container instantiated **before** Blueprint registration (see `app.py`) following Memory `55c2a6bd...`.
- **Scope**: `Scope.APP` for all provided components.
- Providers include LLM provider map and optional `MockLLMInteractionImpl` (controlled by `LLM_PROVIDER_SERVICE_USE_MOCK_LLM`).
- **Session Provider**: `CJSessionProviderImpl.session()` rolls back on exception and closes the session but **never auto-commits**. Any workflow that mutates data must call `await session.commit()` before the context manager exits; cross-session verification requires a fresh session.

## 5. Event Contracts
- **Consumed**: `ELS_CJAssessmentRequestV1` (thin event, payload includes essay refs + overrides).
- **Published**:
  - `CJAssessmentCompletedV1` with rankings list.
  - `CJAssessmentFailedV1` with detailed `error_info`.
- **Envelope**: `EventEnvelope` with **top-level** `schema_version` (see Rule 052).
- **Correlation IDs**: Always `uuid.UUID`; test mocks must expect UUID objects (Memory 234cf4fe...).

## 6. Database Schema & Ranking Strategy (Async SQLAlchemy)
- **Tables**: `cj_batch_uploads`, `cj_processed_essays`, `cj_comparison_pairs`, `cj_batch_states`, `grade_projections`
- **Key Relationships**: 
  - ELS essay IDs as string primary keys, CASCADE foreign keys
  - One-to-one relationship between `cj_batch_uploads` and `cj_batch_states`
  - Comparison pairs linked to callbacks via `request_correlation_id`
- **State Management**: `CJBatchState` tracks real-time processing progress (INITIALIZING → GENERATING_PAIRS → WAITING_CALLBACKS → SCORING → COMPLETED/FAILED)
- **Ranking Strategy**: Rankings generated dynamically from `ProcessedEssay.current_bt_score` via `get_essay_rankings()`
- **No Persistent Rankings**: Rankings computed on-demand, not stored separately (intentional design)
- **Rationale**: Rankings deterministically derivable from Bradley-Terry scores, avoids data duplication

### 6.1. Swedish 8-Grade System
- **Model**: The service implements a psychometrically robust 8-anchor Swedish national exam grade system.
- **Anchor Grades**: `F, E, D, D+, C, C+, B, A`.
- **Derived Grades**: "Minus" grades (`B-`, `C-`, etc.) are derived for scores in the lower quartile of a grade band.
- **Calibration**: The system uses population priors from historical data, not anchor frequency, to avoid sampling bias. Shrinkage estimation is used for sparse anchors.
- **Confidence**: Confidence scores are based on the entropy of the 8-grade probability distribution.

- **Migration**: Alembic migrations for schema evolution
- **Connection**: `DATABASE_URL_CJ` (SQLite dev, PostgreSQL prod), **Pool Config**: size=10, max_overflow=20, pre_ping=True, recycle=3600

## 7. LLM Interaction Flow (Event-Driven)
1. `pair_generation.py` selects comparison pairs.
2. For each pair a prompt is rendered from `ASSESSMENT_PROMPT_TEMPLATE`.
3. `LLMProviderServiceClient` makes HTTP request with callback topic
   - **200 Response**: Immediate result processed directly
   - **202 Response**: Request queued, result arrives via Kafka callback
4. `LLMInteractionImpl.perform_comparisons()` orchestrates concurrent requests
5. Callbacks arrive on `huleedu.llm_provider.comparison_result.v1` topic
6. `workflow_logic.py` processes callbacks, updates state, checks progress
7. When sufficient comparisons complete, `workflow_continuation.py` recomputes BT scores, builds `ContinuationContext`, evaluates `ContinuationDecision` via `decide(ctx)`, records diagnostics via `workflow_diagnostics`, and then drives `BatchFinalizer` or additional comparison requests. Final rankings are calculated in `scoring_ranking.py`.

### 7.1. LLM Request Metadata (Required)
- **Essay IDs**: Every comparison request **must** include `essay_a_id` and `essay_b_id` inside the metadata dict passed to `LLMProviderServiceClient.generate_comparison`. Use the authoritative ELS IDs from `ComparisonTask`.
- **Propagation**: The metadata dict is echoed back in `LLMComparisonResultV1.request_metadata`; consumers (ENG5 runners, analytics) rely on it for artefact hydration. Never drop or overwrite these fields.
- **Prompt Hash**: The LLM Provider appends `prompt_sha256`; do not attempt to recompute it in CJ. Treat the provider value as the source of truth for the exact prompt string that was sent upstream.

## 8. Configuration (env vars / Settings)
Key vars from `Settings` (env prefix `CJ_ASSESSMENT_SERVICE_`):
- **Core**: `LOG_LEVEL`, `LLM_PROVIDER_SERVICE_USE_MOCK_LLM`, `KAFKA_BOOTSTRAP_SERVERS`, `DEFAULT_LLM_PROVIDER`
- **Database**: `DATABASE_URL_CJ`, `DATABASE_POOL_SIZE`, `DATABASE_MAX_OVERFLOW`, `DATABASE_POOL_PRE_PING`, `DATABASE_POOL_RECYCLE`
- **Services**: `CONTENT_SERVICE_URL`, `LLM_PROVIDER_SERVICE_URL`
- **Callback Configuration**: `LLM_PROVIDER_CALLBACK_TOPIC` (default: `huleedu.llm_provider.comparison_result.v1`)
- **Batch Processing**: `BATCH_TIMEOUT_HOURS` (4), `BATCH_MONITOR_INTERVAL_MINUTES` (5), `MIN_SUCCESS_RATE_THRESHOLD` (0.8)
- **Score Stability**: `SCORE_STABILITY_THRESHOLD` (0.05), `MAX_ITERATIONS` (5), `MAX_CONCURRENT_COMPARISONS` (100)
- **Circuit Breaker**: `CIRCUIT_BREAKER_ENABLED`, `*_FAILURE_THRESHOLD`, `*_RECOVERY_TIMEOUT`, `*_SUCCESS_THRESHOLD`
- **Metrics**: `HTTP_PORT` (default 9090), Redis idempotency TTL settings
- **Admin Surface**: `CJ_ASSESSMENT_SERVICE_ENABLE_ADMIN_ENDPOINTS` (gates both
  instructions and student prompt endpoints); requires shared JWT validation
  settings to authenticate admin operators before prompt uploads are allowed.

## 9. Resilience & Observability
### 9.1. Circuit Breaker Protection
- **Kafka Publishing**: `ResilientKafkaPublisher` with circuit breaker delegation
- **Configuration**: Failure threshold, recovery timeout, success threshold
- **Fallback**: Failed messages queued for retry when circuit opens
- **Metrics**: Circuit breaker state changes and call results

### 9.2. Metrics & Tracing
- **Prometheus**: `CollectorRegistry` injected via DI, exposed on `/metrics`
- **Core Counters**: Processed messages, LLM calls, callback latency, workflow duration, circuit breaker metrics
- **Batch Metrics**: Batch state distribution, progress percentages, stuck batch detection
- **Callback Metrics**: Processing latency, success/error rates, batch completion times
- **OpenTelemetry**: Full span creation and trace context propagation
- **Logging**: `huleedu_service_libs.logging_utils` with `correlation_id` extra, idempotency tracking

## 10. Security & Limits (Walking Skeleton)
- Admin surface gated by shared JWT validation; `/admin/v1/**` routes require
  Identity-issued tokens with `roles` containing `admin`. Prompt uploads are
  rejected unless the assignment instruction already exists (prevents prompt
  ingestion without authoritative metadata).
- DB credentials via env vars; no secrets in repo.
- Rate limit for LLM calls via semaphore-based concurrency control.
- Batch monitoring with automatic recovery for stuck batches.
- Future: OAuth for external API, essay text PII scrubbing.

## 11. Deployment
- **Docker**: `python:3.11-slim`, multi-stage.
- **Entrypoint**: `app.py` (unified Quart application with integrated Kafka consumer)
- **Commands**: `pdm run start` for production, `pdm run dev` for development
- **Ports**: 9095 (metrics/health), no external HTTP API.
- **Lifecycle**: Service manages both HTTP API and Kafka consumer in single process

## 12. Limitations & Future Work
- Current DB schema minimal; migrate to dedicated migrations tool (e.g., Alembic).
- Ranking stability threshold heuristic to be tuned (`SCORE_STABILITY_THRESHOLD`).
- LLM provider selection static; future: dynamic provider fail-over.

---

**CRITICAL IMPLEMENTATION NOTES**
- **DI Container**: Must be created *before* Blueprint registration in `app.py`
- **Idempotency**: Redis-based with 24-hour TTL using deterministic event IDs
- **Callback Processing**: Event-driven via Kafka, no polling. Callbacks linked via `request_correlation_id`
- **State Management**: Persistent batch state with optimistic locking for concurrent callback processing
- **Batch Monitoring**: Background task detects stuck batches (> 4 hours idle), auto-recovers or fails
- **Circuit Breaker**: Optional but recommended for Kafka publishing resilience
- **UUID Serialization**: Use `json.dumps(envelope.model_dump(mode="json"))` for proper UUID encoding
- **Testing**: Mock protocols, not implementations; expect UUID objects in correlation IDs
