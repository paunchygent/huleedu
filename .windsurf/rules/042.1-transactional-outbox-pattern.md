---
description: Transactional Outbox Pattern as fallback for Kafka failures
globs: 
alwaysApply: false
---
# 042.1: Transactional Outbox Pattern (Fallback Mechanism)

## 1. Core Principle
**Primary Path (99.9%)**: Direct Kafka publishing.  
**Fallback Path**: Store events in database outbox ONLY when Kafka is unavailable. Redis BLPOP ensures zero-delay processing when events enter outbox.

## 2. Required Components

### 2.1. Database Schema
**MUST** include `EventOutbox` model from `huleedu_service_libs.outbox`:

```python
from huleedu_service_libs.outbox import EventOutbox
# Table: event_outbox with indexes for unpublished events polling
```

### 2.2. Event Publisher Integration (Kafka-First Pattern)
```python
class DefaultEventPublisher(EventPublisherProtocol):
    def __init__(self, kafka_bus: KafkaPublisherProtocol, 
                 outbox_repository: OutboxRepositoryProtocol,
                 redis_client: RedisClientProtocol):
        self.kafka_bus = kafka_bus
        self.outbox_repository = outbox_repository
        self.redis_client = redis_client
    
    async def publish_event(self, event_data, correlation_id, aggregate_id):
        envelope = EventEnvelope(...)
        
        # PRIMARY PATH: Try Kafka first
        try:
            await self.kafka_bus.publish(
                topic=self._get_topic_for_event_type(envelope.event_type),
                key=aggregate_id.encode("utf-8"),
                value=envelope.model_dump_json(mode="json").encode("utf-8"),
                correlation_id=correlation_id,
            )
            return  # Success - no outbox needed
        
        except Exception as e:
            # FALLBACK PATH: Store in outbox only on Kafka failure
            await self.outbox_repository.add_event(
                aggregate_id=aggregate_id,
                aggregate_type="entity",
                event_type=envelope.event_type,
                event_data=envelope.model_dump(mode="json"),
                topic=self._get_topic_for_event_type(envelope.event_type),
                event_key=aggregate_id,
            )
            
            # Wake up relay worker immediately via Redis
            await self.redis_client.lpush("outbox:wakeup", "1")
```

### 2.3. Business Logic Pattern
**Primary approach**: Publish directly to Kafka after business operation.
```python
async def business_operation(self, data, correlation_id):
    # 1. Business logic with transaction
    async with self.repository.get_session() as session:
        result = await self.repository.update(data, session=session)
        await session.commit()
    
    # 2. Publish event (Kafka-first, outbox on failure)
    await self.event_publisher.publish_event(
        event_data={"result": result.model_dump()},
        correlation_id=correlation_id,
        aggregate_id=str(data.entity_id),
    )
```

**Special case - Atomic guarantee required**:
```python
async def critical_operation(self, data, correlation_id):
    async with self.repository.get_session() as session:
        # 1. Business logic
        result = await self.repository.update(data, session=session)
        
        # 2. Store event in outbox (same transaction)
        # Only for operations requiring absolute atomicity
        await self.outbox_repository.add_event(
            aggregate_id=str(data.entity_id),
            event_data={"result": result.model_dump()},
            session=session,  # Share transaction
        )
        
        await session.commit()  # Atomic commit
        
        # 3. Wake up relay worker
        await self.redis_client.lpush("outbox:wakeup", "1")
```

## 3. Event Relay Worker (Redis-Driven)

### 3.1. Configuration & Startup
```python
# In di.py
@provide(scope=Scope.APP)
async def provide_event_relay_worker(
    outbox_repository: OutboxRepositoryProtocol,
    kafka_bus: KafkaPublisherProtocol,
    redis_client: RedisClientProtocol,
    settings: Settings,
) -> EventRelayWorker:
    # Configuration centralized in huleedu_service_libs based on ENVIRONMENT
    from huleedu_service_libs.outbox import create_relay_worker
    
    return create_relay_worker(
        outbox_repository=outbox_repository,
        kafka_bus=kafka_bus,
        redis_client=redis_client,
        service_name=settings.SERVICE_NAME,
        environment=settings.ENVIRONMENT,  # dev/staging/prod
    )

# In startup_setup.py
async def start_event_relay_worker(container: AsyncContainer):
    relay_worker = await container.get(EventRelayWorker)
    await relay_worker.start()
```

### 3.2. Worker Behavior
- **Primary**: Waits on Redis BLPOP for instant wake-up notifications
- **Adaptive Polling**: 0.1s → 1s → 5s intervals when idle (configured by ENVIRONMENT)
- **Zero-delay**: Processes events immediately when Redis notification received
- **Batch Processing**: Up to 100 events per batch
- **Retry Logic**: 5 retries with exponential backoff
- **Centralized Config**: All timing/intervals configured in library based on ENVIRONMENT

## 4. Settings Configuration
```python
class Settings(BaseSettings):
    ENVIRONMENT: str = Field(default="dev")  # dev/staging/prod
    SERVICE_NAME: str = Field(...)
    
    # Outbox settings are centralized in huleedu_service_libs
    # based on ENVIRONMENT value:
    # - dev: aggressive polling (0.1s start), small batches
    # - staging: moderate polling (1s start), medium batches  
    # - prod: conservative polling (5s start), large batches
```

## 5. Migration Template
```bash
# Generate migration
pdm run migrate-revision "add_event_outbox_table"
```

```python
def upgrade() -> None:
    op.create_table('event_outbox',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('aggregate_id', sa.String(255), nullable=False),
        sa.Column('aggregate_type', sa.String(100), nullable=False),
        sa.Column('event_type', sa.String(255), nullable=False),
        sa.Column('event_data', sa.JSON(), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('published_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('retry_count', sa.Integer(), server_default='0'),
        sa.Column('last_error', sa.Text(), nullable=True),
    )
    
    # Critical performance index
    op.create_index('ix_event_outbox_unpublished', 'event_outbox', 
                    ['published_at', 'created_at'], 
                    postgresql_where=sa.text('published_at IS NULL'))
```

## 6. Testing Patterns
```python
@pytest.mark.asyncio
async def test_outbox_stores_event():
    outbox_repo = PostgreSQLOutboxRepository(test_engine)
    event_id = await outbox_repo.add_event(
        aggregate_id="test-123",
        event_type="test.event.v1",
        event_data={"test": "data"},
        topic="test.events",
    )
    
    event = await outbox_repo.get_event_by_id(event_id)
    assert event.published_at is None  # Not yet published
```

## 7. Required Patterns

✅ **PRIMARY PATTERN - Kafka-first with fallback**:
```python
# Business operation completes first
await repository.save(entity)

# Then publish (Kafka-first, outbox on failure)
await event_publisher.publish_event(...)  # Handles fallback internally
```

✅ **SPECIAL CASE - Atomic guarantee required**:
```python
# Only for operations requiring absolute atomicity
async with session.begin():
    await repository.update(entity, session=session)
    await outbox_repository.add_event(..., session=session)
# Wake up relay worker
await redis_client.lpush("outbox:wakeup", "1")
```

❌ **FORBIDDEN - Always using outbox**:
```python
# WRONG - Outbox is fallback only, not primary path
await outbox_repository.add_event(...)  # Should try Kafka first
```

## 8. Implementation Checklist
- ✅ Event publisher tries Kafka FIRST, outbox on failure
- ✅ Redis LPUSH notification sent when events enter outbox
- ✅ Relay worker uses Redis BLPOP for instant wake-up
- ✅ Adaptive polling intervals based on ENVIRONMENT
- ✅ Configuration centralized in huleedu_service_libs
- ✅ Outbox table includes performance indexes
- ✅ 99.9% of events go directly to Kafka (monitor this metric)

## 9. Key Metrics to Monitor
- **kafka_publish_success_rate**: Should be >99.9%
- **outbox_fallback_rate**: Should be <0.1%
- **outbox_processing_delay**: Should be <100ms with Redis wake-up
- **outbox_queue_depth**: Should remain near zero
