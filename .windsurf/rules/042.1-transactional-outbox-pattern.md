---
description: Transactional Outbox Pattern as fallback for Kafka failures
globs: 
alwaysApply: false
---
# 042.1: Transactional Outbox Pattern (Hybrid Approach)

## 1. Core Principle
**Primary Path (99.9%)**: Direct Kafka publishing with dual Redis publishing for real-time UI updates.  
**Fallback Path**: Store events in database outbox ONLY when Kafka is unavailable. Redis BLPOP ensures zero-delay processing when events enter outbox.
**Session Integration**: Support for Unit of Work pattern with session-aware publishing for atomic guarantees.

## 2. Required Components

### 2.1. Database Schema
**MUST** include `EventOutbox` model from `huleedu_service_libs.outbox`:

```python
from huleedu_service_libs.outbox import EventOutbox
# Table: event_outbox with indexes for unpublished events polling
```

### 2.2. Event Publisher Integration (Kafka-First with Dual Publishing)
```python
class DefaultEventPublisher(EventPublisherProtocol):
    def __init__(self, kafka_bus: KafkaPublisherProtocol, 
                 outbox_repository: OutboxRepositoryProtocol,
                 redis_client: AtomicRedisClientProtocol,
                 batch_tracker: BatchEssayTracker):
        self.kafka_bus = kafka_bus
        self.outbox_repository = outbox_repository
        self.redis_client = redis_client
        self.batch_tracker = batch_tracker
    
    async def publish_status_update(self, essay_ref, status, correlation_id):
        envelope = EventEnvelope(...)
        topic = "essay.status.events"
        key = str(essay_ref.entity_id)
        
        # PRIMARY PATH: Try Kafka first
        try:
            await self.kafka_bus.publish(
                topic=topic,
                envelope=envelope,
                key=key,
            )
            logger.info("Event published directly to Kafka")
        except Exception as kafka_error:
            # FALLBACK PATH: Store in outbox only on Kafka failure
            await self._publish_to_outbox(
                aggregate_type="essay",
                aggregate_id=str(essay_ref.entity_id),
                event_type="essay.status.updated.v1",
                event_data=envelope,
                topic=topic,
            )
            # Wake up relay worker immediately via Redis
            await self._notify_relay_worker()
        
        # DUAL PUBLISHING: Redis real-time notifications (always attempted)
        await self._publish_essay_status_to_redis(essay_ref, status, correlation_id)
```

### 2.3. Business Logic Patterns

**Primary approach**: Publish directly to Kafka after business operation.
```python
async def business_operation(self, data, correlation_id):
    # 1. Business logic with transaction
    async with self.repository.session() as session:
        result = await self.repository.update(data, session=session)
        # Session commits automatically via context manager
    
    # 2. Publish event (Kafka-first, outbox on failure, dual Redis)
    await self.event_publisher.publish_status_update(
        essay_ref=result.entity_ref,
        status=result.status,
        correlation_id=correlation_id,
    )
```

**Unit of Work pattern**: For coordinated multi-entity operations.
```python
async def coordinated_operation(self, batch_data, correlation_id):
    # Session-aware operations with explicit session management
    async with self.session_factory() as session:
        async with session.begin():
            # 1. Business logic across multiple entities
            for essay_data in batch_data.essays:
                await self.repository.update_essay(
                    essay_data, session=session
                )
            
            # 2. Publish events within same transaction (outbox)
            await self.event_publisher.publish_batch_event(
                event_data=batch_data,
                correlation_id=correlation_id,
                session=session,  # Atomic with business changes
            )
            # Commit handled by context manager
        
        # 3. Wake up relay worker after successful commit
        await self._notify_relay_worker()
```

### 2.4. Redis Integration Patterns

**Dual Publishing**: Kafka for service-to-service + Redis for real-time UI.
```python
async def _publish_essay_status_to_redis(
    self, essay_ref, status, correlation_id
) -> None:
    """Publish essay status update to Redis for real-time UI notifications."""
    try:
        user_id = await self.batch_tracker.get_user_id_for_essay(essay_ref.entity_id)
        
        if user_id:
            await self.redis_client.publish_user_notification(
                user_id=user_id,
                event_type="essay_status_updated",
                data={
                    "essay_id": essay_ref.entity_id,
                    "status": status.value,
                    "timestamp": datetime.now(UTC).isoformat(),
                    "correlation_id": str(correlation_id),
                },
            )
    except Exception as e:
        # Re-raise as structured error for proper observability
        if hasattr(e, "error_detail"):
            raise
        else:
            raise_external_service_error(
                service="essay_lifecycle_service",
                operation="_publish_essay_status_to_redis",
                external_service="Redis",
                message=f"Failed to publish essay status to Redis: {e.__class__.__name__}",
                correlation_id=correlation_id,
            )

async def _notify_relay_worker(self) -> None:
    """Notify the relay worker that new events are available in the outbox."""
    try:
        # Use service-specific wake-up key for isolation
        await self.redis_client.lpush("outbox:wake:essay_lifecycle_service", "1")
        logger.debug("Relay worker notified via Redis")
    except Exception as e:
        # Log but don't fail - the relay worker will still poll eventually
        logger.warning(
            "Failed to notify relay worker via Redis",
            extra={"error": str(e)},
        )
```

**Session-aware Publishing (Unit of Work Integration)**:
```python
async def critical_operation(self, data, correlation_id, session=None):
    # Unit of Work pattern - session passed from coordinator
    if session:
        # Business logic and outbox in same transaction
        result = await self.repository.update(data, session=session)
        
        # Store event in outbox (same transaction for atomicity)
        await self.outbox_repository.add_event(
            aggregate_id=str(data.entity_id),
            aggregate_type="entity",
            event_type="entity.updated.v1",
            event_data=envelope.model_dump(mode="json"),
            topic=topic,
            event_key=str(data.entity_id),
            session=session,  # Share transaction
        )
        
        # Session commit handled by coordinator
        # Wake up relay worker after commit
        await self._notify_relay_worker()
    else:
        # Fallback to Kafka-first pattern
        await self.publish_event_kafka_first(data, correlation_id)
```

## 3. Event Relay Worker (Redis-Driven)

### 3.1. Configuration & Startup
```python
# In di.py
@provide(scope=Scope.APP)
async def provide_event_relay_worker(
    outbox_repository: OutboxRepositoryProtocol,
    kafka_bus: KafkaPublisherProtocol,
    redis_client: RedisClientProtocol,
    settings: Settings,
) -> EventRelayWorker:
    # Configuration centralized in huleedu_service_libs based on ENVIRONMENT
    from huleedu_service_libs.outbox import create_relay_worker
    
    return create_relay_worker(
        outbox_repository=outbox_repository,
        kafka_bus=kafka_bus,
        redis_client=redis_client,
        service_name=settings.SERVICE_NAME,
        environment=settings.ENVIRONMENT,  # dev/staging/prod
    )

# In startup_setup.py
async def start_event_relay_worker(container: AsyncContainer):
    relay_worker = await container.get(EventRelayWorker)
    await relay_worker.start()
```

### 3.2. Worker Behavior
- **Primary**: Waits on Redis BLPOP for instant wake-up notifications
- **Adaptive Polling**: 0.1s → 1s → 5s intervals when idle (configured by ENVIRONMENT)
- **Zero-delay**: Processes events immediately when Redis notification received
- **Batch Processing**: Up to 100 events per batch
- **Retry Logic**: 5 retries with exponential backoff
- **Centralized Config**: All timing/intervals configured in library based on ENVIRONMENT

## 4. Settings Configuration
```python
class Settings(BaseSettings):
    ENVIRONMENT: str = Field(default="dev")  # dev/staging/prod
    SERVICE_NAME: str = Field(...)
    
    # Outbox settings are centralized in huleedu_service_libs
    # based on ENVIRONMENT value:
    # - dev: aggressive polling (0.1s start), small batches
    # - staging: moderate polling (1s start), medium batches  
    # - prod: conservative polling (5s start), large batches
```

## 5. Migration Template
```bash
# Generate migration
pdm run migrate-revision "add_event_outbox_table"
```

```python
def upgrade() -> None:
    op.create_table('event_outbox',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('aggregate_id', sa.String(255), nullable=False),
        sa.Column('aggregate_type', sa.String(100), nullable=False),
        sa.Column('event_type', sa.String(255), nullable=False),
        sa.Column('event_data', sa.JSON(), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('published_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('retry_count', sa.Integer(), server_default='0'),
        sa.Column('last_error', sa.Text(), nullable=True),
    )
    
    # Critical performance index
    op.create_index('ix_event_outbox_unpublished', 'event_outbox', 
                    ['published_at', 'created_at'], 
                    postgresql_where=sa.text('published_at IS NULL'))
```

## 6. Testing Patterns
```python
@pytest.mark.asyncio
async def test_outbox_stores_event():
    outbox_repo = PostgreSQLOutboxRepository(test_engine)
    event_id = await outbox_repo.add_event(
        aggregate_id="test-123",
        event_type="test.event.v1",
        event_data={"test": "data"},
        topic="test.events",
    )
    
    event = await outbox_repo.get_event_by_id(event_id)
    assert event.published_at is None  # Not yet published
```

## 7. Required Patterns

✅ **PRIMARY PATTERN - Kafka-first with dual Redis publishing**:
```python
# Business operation completes first
async with repository.session() as session:
    entity = await repository.save(entity_data, session=session)

# Then publish (Kafka-first, outbox on failure, Redis for UI)
await event_publisher.publish_status_update(
    essay_ref=entity.entity_ref,
    status=entity.status,
    correlation_id=correlation_id,
)  # Handles both Kafka/outbox fallback AND Redis real-time
```

✅ **UNIT OF WORK PATTERN - Session-aware publishing**:
```python
# Coordinated operations with explicit session management
async with session_factory() as session:
    async with session.begin():
        # Business logic
        await repository.update_entities(batch_data, session=session)
        
        # Atomic event publishing (outbox + session)
        await event_publisher.publish_batch_event(
            event_data=batch_data,
            correlation_id=correlation_id,
            session=session,  # Share transaction for atomicity
        )
        # Commit handled by context manager
    
    # Wake up relay worker after successful commit
    await event_publisher._notify_relay_worker()
```

✅ **SPECIALIZED DISPATCH PATTERN - Service-to-service with outbox**:
```python
# Direct outbox usage for service dispatch coordination
await self.outbox_repository.add_event(
    aggregate_id=essay_ref.essay_id,
    aggregate_type="essay",
    event_type=envelope.event_type,
    event_data=event_data,
    topic=topic,
    event_key=essay_ref.essay_id,
    session=session,  # Transaction coordination
)
```

❌ **FORBIDDEN - Always using outbox**:
```python
# WRONG - Outbox is fallback only, not primary path
await outbox_repository.add_event(...)  # Should try Kafka first
```

❌ **FORBIDDEN - Missing Redis real-time notifications**:
```python
# WRONG - Only Kafka, no real-time UI updates
await kafka_bus.publish(topic, envelope)  # Missing Redis dual publishing
```

❌ **FORBIDDEN - Ignoring session parameter**:
```python
# WRONG - Session passed but not used for atomicity
async def publish_event(self, data, session=None):
    await kafka_bus.publish(...)  # Should use outbox with session
```

## 8. Implementation Checklist
- ✅ Event publisher tries Kafka FIRST, outbox on failure
- ✅ **Dual publishing**: Kafka for service-to-service + Redis for real-time UI
- ✅ **Session-aware publishing**: Support for Unit of Work pattern
- ✅ **Service-specific wake-up keys**: `outbox:wake:{service_name}` for isolation
- ✅ Redis LPUSH notification sent when events enter outbox
- ✅ Relay worker uses Redis BLPOP for instant wake-up
- ✅ Adaptive polling intervals based on ENVIRONMENT
- ✅ Configuration centralized in huleedu_service_libs
- ✅ Outbox table includes performance indexes
- ✅ **Structured error handling** with HuleEduError for observability
- ✅ 99.9% of events go directly to Kafka (monitor this metric)

## 9. Key Metrics to Monitor
- **kafka_publish_success_rate**: Should be >99.9%
- **outbox_fallback_rate**: Should be <0.1%
- **outbox_processing_delay**: Should be <100ms with Redis wake-up
- **outbox_queue_depth**: Should remain near zero
- **redis_notification_success_rate**: For real-time UI updates
- **session_aware_publish_rate**: Unit of Work pattern usage
- **relay_worker_wake_up_latency**: Redis notification effectiveness
- **dual_publishing_consistency**: Kafka vs Redis publishing alignment
