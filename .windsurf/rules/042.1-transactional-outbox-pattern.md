---
description: Transactional Outbox Pattern for reliable event publishing
globs: 
alwaysApply: false
---
# 042.1: Transactional Outbox Pattern

## 1. Core Principle
Store events in database within same transaction as business operations. Separate relay worker publishes events to Kafka asynchronously.

## 2. Required Components

### 2.1. Database Schema
**MUST** include `EventOutbox` model from `huleedu_service_libs.outbox`:

```python
from huleedu_service_libs.outbox import EventOutbox
# Table: event_outbox with indexes for unpublished events polling
```

### 2.2. Event Publisher Integration
```python
class DefaultEventPublisher(EventPublisherProtocol):
    def __init__(self, outbox_repository: OutboxRepositoryProtocol, ...):
        self.outbox_repository = outbox_repository
    
    async def publish_event(self, event_data, correlation_id, aggregate_id):
        envelope = EventEnvelope(...)
        await self.outbox_repository.add_event(
            aggregate_id=aggregate_id,
            aggregate_type="entity",
            event_type=envelope.event_type,
            event_data=envelope.model_dump(mode="json"),
            topic=self._get_topic_for_event_type(envelope.event_type),
            event_key=aggregate_id,
        )
```

### 2.3. Business Logic Pattern
**MUST** share database transaction:
```python
async def business_operation(self, data, correlation_id):
    async with self.repository.get_session() as session:
        # 1. Business logic
        result = await self.repository.update(data, session=session)
        
        # 2. Store event (same transaction)
        await self.outbox_repository.add_event(
            aggregate_id=str(data.entity_id),
            event_data={"result": result.model_dump()},
            session=session,  # Share transaction
        )
        
        await session.commit()  # Atomic commit
```

## 3. Event Relay Worker

### 3.1. Configuration & Startup
```python
# In di.py
@provide(scope=Scope.APP)
async def provide_event_relay_worker(
    outbox_repository: OutboxRepositoryProtocol,
    kafka_bus: KafkaPublisherProtocol,
    settings: Settings,
) -> EventRelayWorker:
    return EventRelayWorker(
        outbox_repository=outbox_repository,
        kafka_bus=kafka_bus,
        settings=OutboxSettings(
            poll_interval_seconds=5.0,
            batch_size=100,
            max_retries=5,
        ),
        service_name=settings.SERVICE_NAME,
    )

# In startup_setup.py
async def start_event_relay_worker(container: AsyncContainer):
    relay_worker = await container.get(EventRelayWorker)
    await relay_worker.start()
```

### 3.2. Worker Behavior
- Polls outbox every 5 seconds for unpublished events
- Processes up to 100 events per batch
- Retries failed events up to 5 times
- Marks events as failed after max retries exceeded

## 4. Settings Configuration
```python
class Settings(BaseSettings):
    OUTBOX_POLL_INTERVAL_SECONDS: float = Field(default=5.0)
    OUTBOX_BATCH_SIZE: int = Field(default=100)
    OUTBOX_MAX_RETRIES: int = Field(default=5)
    OUTBOX_ERROR_RETRY_INTERVAL_SECONDS: float = Field(default=30.0)
```

## 5. Migration Template
```bash
# Generate migration
pdm run migrate-revision "add_event_outbox_table"
```

```python
def upgrade() -> None:
    op.create_table('event_outbox',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('aggregate_id', sa.String(255), nullable=False),
        sa.Column('aggregate_type', sa.String(100), nullable=False),
        sa.Column('event_type', sa.String(255), nullable=False),
        sa.Column('event_data', sa.JSON(), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
        sa.Column('published_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('retry_count', sa.Integer(), server_default='0'),
        sa.Column('last_error', sa.Text(), nullable=True),
    )
    
    # Critical performance index
    op.create_index('ix_event_outbox_unpublished', 'event_outbox', 
                    ['published_at', 'created_at'], 
                    postgresql_where=sa.text('published_at IS NULL'))
```

## 6. Testing Patterns
```python
@pytest.mark.asyncio
async def test_outbox_stores_event():
    outbox_repo = PostgreSQLOutboxRepository(test_engine)
    event_id = await outbox_repo.add_event(
        aggregate_id="test-123",
        event_type="test.event.v1",
        event_data={"test": "data"},
        topic="test.events",
    )
    
    event = await outbox_repo.get_event_by_id(event_id)
    assert event.published_at is None  # Not yet published
```

## 7. Required Patterns

✅ **Atomic transactions**:
```python
async with session.begin():
    await repository.update(entity, session=session)
    await outbox_repository.add_event(..., session=session)
```

❌ **FORBIDDEN - Direct Kafka in business logic**:
```python
# WRONG - No atomicity guarantee
await kafka_bus.publish(event)
await repository.save(entity)
```

## 8. Compliance
- **MUST** use outbox for all business-critical events
- **MUST** share database session between business logic and outbox
- **MUST** start relay worker in application lifecycle
- **MUST** include outbox table in migrations with performance indexes
