# Rule 071.5: LLM Debugging Patterns for HuleEdu Observability

**Status**: Active
**Last Updated**: 2025-11-20
**Dependencies**: 071.1, 071.3, 071.4

## Core Pattern: Label-First Filtering

```logql
# CORRECT: Label filter → JSON parse → field filter
{service="cj_assessment_service", level="error"} | json | correlation_id="abc-123"

# WRONG: Field as label (will fail)
{correlation_id="abc-123"}
```

**Indexed labels**: `service`, `level`
**JSON body fields**: `correlation_id`, `trace_id`, `span_id`, `batch_id`, `event`, `logger_name`

---

## Anti-Patterns

### ❌ High-Cardinality Label Queries
```logql
{correlation_id="..."}  # FAILS: not a label
{trace_id="..."}        # FAILS: not a label
{batch_id="33"}         # FAILS: not a label
```

### ❌ Multi-Service docker logs
```bash
# WRONG: Grep each service individually
docker logs huleedu_cj_assessment_service | grep "batch_id"
docker logs huleedu_batch_orchestrator | grep "batch_id"
```

**Correct**: Single Loki query across all services:
```logql
{service=~".*"} | json | batch_id="33"
```

### ❌ Assuming Trace Context Always Exists
```logql
{service="api_gateway"} | json | trace_id!=""  # May return 0 if idle
```

**Correct**: Check first, then fall back:
```bash
# Try trace_id
COUNT=$(logcli query '{service="..."} | json | trace_id!=""' --limit=1 | wc -l)
# If 0, use correlation_id
[[ $COUNT -eq 0 ]] && logcli query '{service="..."} | json | correlation_id!=""'
```

---

## Debugging Workflows

### Workflow 1: Trace Request Across Services

```bash
# 1. Extract correlation_id from initial log
CID=$(logcli query '{service="api_gateway"} | json | line_format "{{.correlation_id}}"' --limit=1)

# 2. Query all services
logcli query "{service=~\".*\"} | json | correlation_id=\"${CID}\"" --limit=100

# 3. If trace_id exists, view in Jaeger
TID=$(logcli query "{service=~\".*\"} | json | correlation_id=\"${CID}\"" \
  --limit=1 --output=jsonl | jq -r '.trace_id // empty')
[[ -n "$TID" ]] && open "http://localhost:16686/trace/${TID}"
```

### Workflow 2: Export Logs for Analysis

```bash
# 1. Query with label filter (fast)
logcli query '{service="cj_assessment_service", level="error"}' \
  --from="2025-11-20T10:00:00Z" --to="2025-11-20T20:00:00Z" \
  --limit=500 --output=jsonl > errors.jsonl

# 2. Filter by high-cardinality field
jq 'select(.batch_id == "33")' < errors.jsonl > batch33.jsonl

# 3. Extract for LLM context
jq -r '[.timestamp, .event, .correlation_id, .trace_id] | @tsv' < batch33.jsonl
```

### Workflow 3: Correlation Priority

**Priority order**:
1. **trace_id** (precise timing, span relationships) - Use for latency/performance
2. **correlation_id** (full workflow) - Use for business logic/multi-service

```bash
# When both exist, check Jaeger first for timing
TRACE_ID=$(jq -r '.trace_id // empty' < workflow.jsonl | head -1)
[[ -n "$TRACE_ID" ]] && curl -s "http://localhost:16686/api/traces/${TRACE_ID}" | jq '.data[0].spans | length'
```

---

## Field Reference

| Field | Cardinality | Type | Query Pattern |
|-------|-------------|------|---------------|
| `service.name` | LOW | Label | `{service="..."}` |
| `level` | LOW | Label | `{level="error"}` |
| `correlation_id` | HIGH | JSON | `\| json \| correlation_id="..."` |
| `trace_id` | HIGH | JSON | `\| json \| trace_id="..."` |
| `span_id` | HIGH | JSON | `\| json \| span_id="..."` |
| `batch_id` | HIGH | JSON | `\| json \| batch_id="..."` |
| `event` | HIGH | JSON | `\| json \| event="..."` |

**Trace Format**:
- `trace_id`: 32-char hex (e.g., `73db05229e4a5d4a0728a76f164d192b`)
- `span_id`: 16-char hex (e.g., `0728a76f164d192b`)

---

## Common Scenarios

### Scenario: Batch Processing Stuck

```bash
# 1. Query batch logs
logcli query '{service="cj_assessment_service"} | json | batch_id="33"' --limit=100

# 2. Extract correlation_id
CID=$(logcli query '{service="cj_assessment_service"} | json | batch_id="33" | event="batch_submitted"' \
  --limit=1 --output=jsonl | jq -r '.correlation_id')

# 3. Trace across services
logcli query "{service=~\".*\"} | json | correlation_id=\"${CID}\""

# 4. Check for errors
logcli query "{service=~\".*\", level=\"error\"} | json | correlation_id=\"${CID}\""

# 5. Check metrics
curl -s 'http://localhost:9090/api/v1/query?query=llm_provider_queue_depth' | jq '.data.result[0].value[1]'
```

### Scenario: High API Error Rate

```bash
# 1. Export errors
logcli query '{service="llm_provider_service", level="error"}' \
  --from="..." --to="..." --limit=1000 --output=jsonl > errors.jsonl

# 2. Group by error type
jq -r '.error_type' < errors.jsonl | sort | uniq -c | sort -rn

# 3. Check Prometheus
curl -s 'http://localhost:9090/api/v1/query?query=rate(anthropic_api_requests_total{status=~"4..|5.."}[5m])'
```

### Scenario: Latency Spike

```bash
# 1. Find slow traces in Jaeger (UI: Min Duration: 5s)
# 2. Extract trace_id: 73db05229e4a5d4a0728a76f164d192b

# 3. Query Loki for context
logcli query '{service=~".*"} | json | trace_id="73db05229e4a5d4a0728a76f164d192b"'

# 4. Check Prometheus
curl -s 'http://localhost:9090/api/v1/query?query=histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))'
```

---

## Performance Optimization

1. **Always filter labels first** (uses index):
   ```logql
   {service="...", level="..."} | json | batch_id="..."  # FAST
   {service=~".*"} | json | batch_id="..."              # SLOW
   ```

2. **Use absolute time ranges** (reproducible):
   ```bash
   --from="2025-11-20T10:00:00Z" --to="2025-11-20T11:00:00Z"  # GOOD
   --since=1h                                                  # AMBIGUOUS
   ```

3. **Bound result sets** (prevent context overflow):
   ```bash
   --limit=500
   ```

4. **Filter JSON lines before jq** (handles mixed log formats):
   ```bash
   docker logs huleedu_identity_service 2>&1 | grep -a '^{' | jq '.'
   ```

---

## Loki → Jaeger Correlation

```bash
# 1. Query Loki for errors
logcli query '{service="api_gateway"} | json | duration_ms > 5000' --limit=1 --output=jsonl > slow.jsonl

# 2. Extract trace_id
TRACE_ID=$(jq -r '.trace_id' < slow.jsonl)

# 3. Open in Jaeger
open "http://localhost:16686/trace/${TRACE_ID}"

# 4. Identify bottleneck span (longest duration)
# 5. Query Loki with trace_id for detailed logs
logcli query "{service=~\".*\"} | json | trace_id=\"${TRACE_ID}\""
```

---

## Validation Script Usage

```bash
# Validate OTEL trace context in running services
./scripts/validate_otel_trace_context.sh

# Validate single service
./scripts/validate_otel_trace_context.sh api_gateway_service
```

**Expected Output**:
- Services with active traces: PASS (trace_id/span_id present, valid format)
- Services without traces: WARN (no traced operations occurred)
- Services not running: SKIP

**Note**: Mixed log formats (console + JSON) require `grep -a '^{'` before jq parsing.

---

## References

- Rule 071.1: Observability Core Patterns
- Rule 071.3: Jaeger Tracing Patterns
- Rule 071.4: Grafana Loki Patterns
- `.claude/skills/loki-logql/reference.md`: LogQL syntax
- `scripts/validate_otel_trace_context.sh`: Trace validation tool
