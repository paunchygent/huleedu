Start-of-TASK Prompt for Next Codex Session

  ---
  CRITICAL: Before starting ANY work, you MUST read the following in order:
  1. .claude/rules/000-rule-index.mdc - Full rule index
  2. .claude/rules/090-documentation-standards.mdc - Documentation requirements
  3. .claude/rules/010-foundational-principles.mdc - Core principles (YAGNI, DRY, SOLID)
  4. CLAUDE.md and CLAUDE.local.md - User-specific instructions
  5. TASKS/TASK-052L-feature-pipeline-integration-master.md - Master plan
  6. TASKS/TASK-052L.0-spell-normalizer-shared-helper.md - Current phase status

  DO NOT proceed until you have read ALL these files.

  ---
  Context: NLP Feature Pipeline - Phase 1 Implementation

  What Has Been Completed (TASK-052L.0)

  We successfully extracted the spell normalization pipeline from Spellchecker Service to a shared library:

  ✅ COMPLETED:
  - Created libs/huleedu_nlp_shared/ with SpellNormalizer class
  - Service integration via DI in services/spellchecker_service/di.py
  - All tests passing (had to fix Docker/pytest marker issues)
  - Documentation updated (removed AI slop from README)

  Key Files Created/Modified:
  - libs/huleedu_nlp_shared/src/huleedu_nlp_shared/normalization/spell_normalizer.py - Core implementation
  - libs/huleedu_nlp_shared/README.md - Concise technical documentation
  - services/spellchecker_service/implementations/spell_logic_impl.py - Now uses shared normalizer
  - services/spellchecker_service/di.py - Provides SpellNormalizer instance

  Important Deviation: We deleted core_logic.py entirely instead of keeping a backward compatibility wrapper.

  What Is Missing (Critical Path Blockers)

  ❌ NOT STARTED but REQUIRED for pipeline:

  1. CLI Infrastructure (scripts/ml/ directory doesn't exist)
    - Need scripts/ml/normalize_dataset.py to use SpellNormalizer on datasets
    - This blocks ALL downstream feature extraction
  2. Misplaced Data Preparation Script
    - scripts/data_preparation/prepare_ielts_task2_dataset.py exists but in WRONG location
    - Should be in scripts/ml/ per TASK-052L plans
    - Only does CSV→Parquet conversion, no normalization
  3. Feature Pipeline (TASK-052L.1-3)
    - No feature extraction scaffolding
    - No 50-feature implementation
    - No build_nlp_features.py

  Current Pipeline State vs Plan

  Planned Pipeline:
  CSV data → prepare_ielts_task2_dataset.py → Parquet
           ↓
  Parquet → normalize_dataset.py (SpellNormalizer) → Normalized text
           ↓
  Normalized → build_nlp_features.py → 50 features → Model training

  Actual State:
  CSV data → prepare_ielts_task2_dataset.py → Parquet ✅ (wrong location)
           ↓
           ❌ BLOCKED - no normalize_dataset.py
           ❌ BLOCKED - no feature pipeline

  ---
  YOUR IMMEDIATE TASK

  Primary Objective

  Create the missing CLI infrastructure to unblock the NLP feature pipeline, specifically implementing
  scripts/ml/normalize_dataset.py that uses the SpellNormalizer we've already built.

  Specific Steps Required

  1. Create scripts/ml/ directory structure
  mkdir -p scripts/ml
  2. Move data preparation script to correct location
  mv scripts/data_preparation/prepare_ielts_task2_dataset.py scripts/ml/
  rmdir scripts/data_preparation  # If empty after move
  3. Implement scripts/ml/normalize_dataset.py
    - Import SpellNormalizer from huleedu_nlp_shared
    - Read Parquet files from prepare_ielts_task2_dataset.py output
    - Apply normalization to essay text
    - Output normalized Parquet with original + corrected text + metrics
    - Use async patterns consistent with the codebase
  4. Test the pipeline end-to-end
    - Ensure data flows: CSV → Parquet → Normalized Parquet
    - Verify SpellNormalizer integration works outside service context

  Implementation Requirements

  MUST follow these patterns:
  - Use argparse for CLI arguments (see prepare_ielts_task2_dataset.py for pattern)
  - Use pandas for Parquet I/O
  - Use asyncio for SpellNormalizer calls
  - Include proper logging
  - Handle configuration via CLI args (not env vars for simplicity)

  MUST NOT:
  - Add speculative features not in the plan
  - Create "helpful" abstractions not required
  - Add migration guides or future enhancements sections
  - Create documentation unless explicitly needed

  Critical Implementation Details

  Resource Files (Both MANDATORY)

  - L2 Dictionary: services/spellchecker_service/data/l2_error_dict/filtered_l2_dictionary.txt (89KB)
  - Whitelist: services/spellchecker_service/data/whitelist/combined_whitelist.txt (46MB)
  - Why Both Are Required: L2 fixes learner errors, whitelist preserves proper nouns (Shakespeare, Microsoft, etc.)

  Exact Implementation Pattern

  #!/usr/bin/env python3
  """Normalize IELTS essays using the shared spell normalizer."""

  import argparse
  import asyncio
  from pathlib import Path
  import pandas as pd
  from huleedu_nlp_shared.normalization import SpellNormalizer

  # Import L2 loader from service (coupling acceptable for CLI tool)
  from services.spellchecker_service.spell_logic.l2_dictionary_loader import load_l2_errors


  class SimpleWhitelist:
      """Minimal whitelist implementation for CLI usage."""

      def __init__(self, path: str):
          self.words = set()
          with open(path, 'r', encoding='utf-8') as f:
              for line in f:
                  word = line.strip().lower()
                  if word:
                      self.words.add(word)

      def is_whitelisted(self, word: str) -> bool:
          return word.lower() in self.words


  class CLISettings:
      """Minimal settings implementation for SpellNormalizer."""

      ENABLE_PARALLEL_PROCESSING = False
      MAX_CONCURRENT_CORRECTIONS = 1
      SPELLCHECK_BATCH_SIZE = 100
      PARALLEL_TIMEOUT_SECONDS = 5.0
      PARALLEL_PROCESSING_MIN_WORDS = 1000
      ENABLE_CORRECTION_LOGGING = False

      @property
      def effective_correction_log_dir(self) -> str:
          return "/tmp/spell_corrections"


  async def normalize_essay(text: str, normalizer: SpellNormalizer) -> dict:
      """Normalize a single essay text."""
      result = await normalizer.normalize_text(text=text)
      return {
          "corrected_text": result.corrected_text,
          "total_corrections": result.total_corrections,
          "l2_corrections": result.l2_dictionary_corrections,
          "spell_corrections": result.spellchecker_corrections,
          "correction_density": result.correction_density,
      }


  async def main():
      parser = argparse.ArgumentParser(description="Normalize essays using spell checker")
      parser.add_argument("--input", type=Path, required=True, help="Input parquet file")
      parser.add_argument("--output", type=Path, required=True, help="Output parquet file")
      parser.add_argument(
          "--l2-dict",
          type=Path,
          default=Path("services/spellchecker_service/data/l2_error_dict/filtered_l2_dictionary.txt"),
          help="Path to L2 dictionary"
      )
      parser.add_argument(
          "--whitelist",
          type=Path,
          default=Path("services/spellchecker_service/data/whitelist/combined_whitelist.txt"),
          help="Path to whitelist"
      )
      args = parser.parse_args()

      # Load resources
      print(f"Loading L2 dictionary from {args.l2_dict}")
      l2_errors = load_l2_errors(str(args.l2_dict), filter_entries=False)
      print(f"Loaded {len(l2_errors)} L2 corrections")

      print(f"Loading whitelist from {args.whitelist}")
      whitelist = SimpleWhitelist(str(args.whitelist))
      print(f"Loaded {len(whitelist.words)} whitelist entries")

      # Create normalizer
      normalizer = SpellNormalizer(
          l2_errors=l2_errors,
          whitelist=whitelist,
          parallel_processor=None,
          settings=CLISettings(),
      )

      # Process dataframe
      print(f"Reading {args.input}")
      df = pd.read_parquet(args.input)

      # Add normalized columns
      normalized_data = []
      for idx, row in df.iterrows():
          if idx % 100 == 0:
              print(f"Processing essay {idx}/{len(df)}")

          norm_result = await normalize_essay(row["essay"], normalizer)
          normalized_data.append(norm_result)

      # Add results to dataframe
      for key in normalized_data[0].keys():
          df[key] = [d[key] for d in normalized_data]

      # Save output
      print(f"Writing {args.output}")
      df.to_parquet(args.output)
      print(f"Done! Normalized {len(df)} essays")


  if __name__ == "__main__":
      asyncio.run(main())

  Expected Outcome

  After completing this task:
  1. scripts/ml/ directory exists with both scripts
  2. Can run: pdm run python scripts/ml/normalize_dataset.py --input data/.../processed/train.parquet --output 
  data/.../processed/train_normalized.parquet
  3. Output Parquet contains:
    - All original columns (prompt, essay, band, cefr_label, etc.)
    - corrected_text: Normalized essay text
    - total_corrections, l2_corrections, spell_corrections: Metrics
    - correction_density: Corrections per 100 words
  4. Proper nouns are preserved (Shakespeare stays Shakespeare)
  5. Pipeline unblocked for feature extraction (TASK-052L.1-3)

  ---
  WARNINGS - READ CAREFULLY

  1. NO AI SLOP: Do not add migration guides, future enhancements, or philosophical sections
  2. YAGNI: Implement ONLY what's specified in TASK-052L plans
  3. Resource Paths: Use exact paths to L2 dict and whitelist files
  4. Test Your Work: Ensure the script actually runs with real IELTS data
  5. Documentation: Update TASK-052L.0 to mark CLI complete, compress per rule 090
  6. No Over-Engineering: Simple, direct implementation that works

  Remember: The goal is to unblock the pipeline, not to create the perfect normalization tool.

  Critical Path Summary

  You are at the bottleneck of the entire NLP pipeline. Without normalize_dataset.py:
  - Feature extraction (TASK-052L.1-3) cannot begin
  - 50 features cannot be implemented
  - IELTS scoring model cannot be trained

  This is the ONE THING that must be done to unblock everything else. Focus on getting it working with the real
  resource files. Make it work, make it clean, ship it.

  Verification Steps

  After implementation, verify:
  1. Script runs without errors
  2. Proper nouns preserved (check a few known names in output)
  3. L2 corrections applied (e.g., "recieve" → "receive")
  4. Metrics make sense (correction density typically 1-5 per 100 words for IELTS essays)
  5. Output Parquet readable by pandas

  Once verified, mark TASK-052L.0 CLI section as complete and move to TASK-052L.1 (feature pipeline scaffolding).
