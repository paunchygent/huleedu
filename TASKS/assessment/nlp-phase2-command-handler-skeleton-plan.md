---
id: nlp-phase2-command-handler-skeleton-plan
title: 'ğŸ§  ULTRATHINK: NLP Phase 2 Implementation Status'
type: task
status: proposed
priority: medium
domain: assessment
service: ''
owner_team: agents
owner: ''
program: ''
created: '2025-09-19'
last_updated: '2026-02-01'
related: []
labels: []
---
## ğŸ“‹ REFACTORING SUMMARY

**Phase 2 NLP analyzer completely refactored from monolithic `SpacyNlpAnalyzer` to protocol-based DI architecture:**

- **Core**: `NlpAnalyzerRefactored` class with 6 injected protocol dependencies
- **DI**: `NlpDependencyProvider` provides all implementations via Dishka
- **Metrics**: All 6 advanced linguistic metrics implemented (Zipf, MTLD/HDD, dependency distance, cohesion, PMI/NPMI, language detection)
- **Tests**: 317-line unit tests + 450-line dependency tests + 243-line integration tests
- **Patterns**: Full HuleEdu DDD compliance, skeleton mode fallback, structured error handling
- **Legacy**: Old implementation deprecated with warnings

## ğŸ“Š IMPLEMENTATION STATUS (Updated: 2025-08-13)

### âœ… COMPLETED

**Core Implementation (DI Pattern)**

- âœ… `NlpAnalyzerRefactored` with full DI architecture following HuleEdu patterns
- âœ… Protocol-based dependency injection: 6 dependency protocols (`SpacyModelLoaderProtocol`, `LanguageDetectorProtocol`, `ZipfCalculatorProtocol`, `LexicalDiversityCalculatorProtocol`, `PhraseologyCalculatorProtocol`, `SyntacticComplexityCalculatorProtocol`)
- âœ… Concrete implementations: `SpacyModelLoader`, `LanguageDetector`, `ZipfCalculator`, `LexicalDiversityCalculator`, `PhraseologyCalculator`, `SyntacticComplexityCalculator`
- âœ… `NlpDependencyProvider` with proper Dishka configuration
- âœ… Full DI container integration in `worker_main.py`

**Advanced Linguistic Metrics**

- âœ… Lexical sophistication: mean Zipf frequency, % tokens Zipf<3 (wordfreq)
- âœ… Lexical diversity: MTLD/HDD (lexical-diversity)
- âœ… Syntactic complexity: dependency distance, cohesion scores (TextDescriptives)
- âœ… Phraseology: PMI/NPMI bigrams/trigrams (gensim)
- âœ… Language detection with heuristic fallback (langdetect)
- âœ… Skeleton mode fallback for library failures

**Test Coverage**

- âœ… Unit tests: `test_nlp_analyzer_refactored.py` (317 lines, comprehensive mocking)
- âœ… Unit tests: `test_nlp_dependencies.py` (450 lines, isolated dependency testing)
- âœ… Integration tests: `test_nlp_analyzer_integration.py` (243 lines, real library integration)
- âœ… Test separation: unit (mocked) vs integration (real dependencies)
- âœ… Test utilities with mock builders following HuleEdu standards

**Deprecated Legacy**

- âœ… `nlp_analyzer_impl.py` marked DEPRECATED with proper DI violation warnings
- âœ… Legacy implementation preserved for reference during transition

### ğŸš§ IN PROGRESS

- Nothing currently in progress

### â³ PENDING

- BatchNlpAnalysisHandler completion (using NlpAnalyzerRefactored)  
- Language Tool Service microservice creation
- Event publisher integration for NLP completion events
- End-to-end integration tests
- Docker spaCy model provisioning
- Performance optimization for <500ms requirement

## âš ï¸ CRITICAL VALIDATION UPDATE

**Original plan contained multiple hallucinations. This is the corrected and ALIGNED version based on actual codebase analysis.**

## ğŸš¨ CRITICAL SUCCESS FACTORS (MANDATORY COMPLIANCE)

The implementation MUST adhere to these non-negotiable requirements:

### 1. âœ… Outbox Pattern Compliance

- **MUST** use outbox pattern exclusively for ALL event publishing
- **FORBIDDEN**: Direct Kafka publishing from business logic
- **FORBIDDEN**: Passing `kafka_bus` to handlers or publishers
- **Pattern**: Publisher â†’ OutboxManager â†’ Database â†’ Relay Worker â†’ Kafka

### 2. âœ… Structured Error Handling

- **MUST** use `huleedu_service_libs.error_handling` factories
- **FORBIDDEN**: Generic `Exception` handling without structured errors
- **Required imports**:

  ```python
  from huleedu_service_libs.error_handling import (
      HuleEduError,
      raise_external_service_error,
      raise_processing_error,
      raise_validation_error,
  )
  ```

### 3. âœ… Protocol-Based DI Patterns

- **MUST** define all dependencies as Protocols
- **MUST** use Dishka DI framework with proper scoping
- **FORBIDDEN**: Direct concrete class dependencies in business logic

### 4. âœ… Event Topic Mapping

- **MUST** add `ESSAY_NLP_COMPLETED` to `_TOPIC_MAPPING` in event_enums.py
- **Critical**: Without this, events cannot be published

## Executive Summary

This document outlines the implementation plan for NLP Phase 2 text analysis capabilities. **CRITICAL DISCOVERY**: Phase 2 infrastructure already exists - the task is to modify existing `StudentMatchingHandler` to perform text analysis instead of student matching.

## Architectural Reality Check âœ…

**Current Handler Architecture:**

- âœ… **Phase 1**: `EssayStudentMatchingHandler` handles `BATCH_STUDENT_MATCHING_REQUESTED` (working correctly)
- âš ï¸ **Phase 2**: `StudentMatchingHandler` handles `BATCH_NLP_INITIATE_COMMAND` (misnamed & wrong logic)

**What Already Exists:**

- âœ… Phase 1 student matching infrastructure (complete and working)
- âœ… Phase 2 event handling infrastructure (routing works)
- âœ… `BatchServiceNLPInitiateCommandDataV1` input event model (complete)
- âœ… DI container with dual handler mapping
- âœ… Kafka consumer subscribed to both Phase 1 and Phase 2 topics

**What Needs Correction:**

- ğŸ”„ **RENAME** `StudentMatchingHandler` â†’ `BatchNlpAnalysisHandler`
- ğŸ”„ **REPLACE** student matching logic with NLP analysis logic
- âŒ Output event topic mapping for `ESSAY_NLP_COMPLETED`
- âŒ Output data models for NLP results  
- âŒ spaCy integration
- âŒ Language Tool Service integration

**Integration Flow:**

```
Phase 1: Batch Orchestrator â†’ EssayStudentMatchingHandler â†’ Class Management (unchanged âœ…)
Phase 2: Batch Orchestrator â†’ BatchNlpAnalysisHandler â†’ Language Tool Service (grammar)
                                                      â†’ Result Aggregator (new events)
```

**Key Constraints:**

- âœ… **RENAME handler and replace business logic**
- âœ… **PRESERVE Phase 1 student matching (no changes)**
- âœ… Use existing `BatchServiceNLPInitiateCommandDataV1` event model
- âœ… Work within existing dual-handler DI structure
- âŒ No complex linguistic analysis (CEFR assessment later)
- âŒ No overengineered preprocessing pipelines

## Implementation Plan

## Technical Implementation Details

**Event Architecture Requirements**

```python
# libs/common_core/src/common_core/event_enums.py
ProcessingEvent.ESSAY_NLP_COMPLETED: "huleedu.essay.nlp.completed.v1"  # Required topic mapping

# Event models: EssayNlpCompletedV1, NlpMetrics (with 13 linguistic fields), GrammarAnalysis
# Input exists: BatchServiceNLPInitiateCommandDataV1 + topic mapping
```

**Core Protocol Interfaces**

```python
# NlpAnalyzerProtocol.analyze_text(text, language) -> NlpMetrics
# LanguageToolClientProtocol.check_grammar(text, language, session, correlation_id) -> GrammarAnalysis
# 6 dependency protocols: SpacyModelLoader, LanguageDetector, ZipfCalculator, LexicalDiversityCalculator, PhraseologyCalculator, SyntacticComplexityCalculator
```

**Handler Implementation Pattern**

```python
# BatchNlpAnalysisHandler (existing skeleton)
# Dependencies: NlpAnalyzerProtocol, LanguageToolClientProtocol 
# Pattern: fetch content â†’ analyze with NlpAnalyzerRefactored â†’ grammar check â†’ publish results
# Uses structured error handling from huleedu_service_libs.error_handling
```

**DI Integration Requirements**

```python
# services/nlp_service/di.py needs:
# @provide NlpAnalyzerProtocol â†’ return NlpAnalyzerRefactored (from di_nlp_dependencies)
# Update provide_batch_nlp_handler() to inject NlpAnalyzerProtocol
```

## File Structure Status

**âœ… IMPLEMENTED FILES:**

```
services/nlp_service/implementations/
â”œâ”€â”€ nlp_analyzer_refactored.py        # Main NLP analyzer with DI
â”œâ”€â”€ nlp_dependencies.py               # Concrete dependency implementations
â”œâ”€â”€ language_tool_client_impl.py      # Language Tool Service client

services/nlp_service/
â”œâ”€â”€ nlp_dependency_protocols.py       # Protocol definitions for dependencies
â”œâ”€â”€ di_nlp_dependencies.py           # DI provider for NLP dependencies

services/nlp_service/tests/unit/
â”œâ”€â”€ test_nlp_analyzer_refactored.py   # Unit tests with mocking
â”œâ”€â”€ test_nlp_dependencies.py          # Unit tests for dependencies
â””â”€â”€ test_utils.py                     # Test utilities

services/nlp_service/tests/integration/
â””â”€â”€ test_nlp_analyzer_integration.py  # Integration tests with real libraries
```

**âš ï¸ DEPRECATED:**

```
services/nlp_service/implementations/
â””â”€â”€ nlp_analyzer_impl.py              # DEPRECATED - violates DI principles
```

**â³ FILES NEEDING COMPLETION:**

```
services/nlp_service/command_handlers/
â””â”€â”€ batch_nlp_analysis_handler.py     # Needs to use NlpAnalyzerRefactored

services/nlp_service/
â”œâ”€â”€ di.py                             # Needs NlpAnalyzerProtocol provider
â””â”€â”€ protocols.py                      # May need language tool protocol
```

## Dependencies

**Add to pyproject.toml:**

```toml
dependencies = [
    # ... existing dependencies
    "spacy>=3.4.0",
    # Models to be installed separately:
    # python -m spacy download en_core_web_sm
    # python -m spacy download sv_core_news_sm
]
```

## Testing Strategy

**Unit Tests (Skeleton):**

```python
# tests/unit/test_batch_nlp_analysis_handler.py
async def test_handler_processes_batch_successfully():
    """Test command handler processes batch correctly."""
    
async def test_handler_handles_individual_essay_failures():
    """Test partial failure handling."""

# tests/unit/test_spacy_nlp_analyzer.py  
async def test_basic_text_metrics():
    """Test basic spaCy metrics extraction."""
    
# tests/unit/test_language_tool_client.py
async def test_grammar_analysis_integration():
    """Test Language Tool Service integration."""
```

**Integration Tests (Skeleton):**

```python
# tests/integration/test_phase2_nlp_flow.py
async def test_end_to_end_nlp_analysis():
    """Test complete Phase 2 NLP analysis flow."""
```

## Key Achievements

**âœ… NLP Analyzer Refactoring Complete**

- Protocol-based DI architecture with 6 dependency interfaces
- Full spaCy + linguistic libraries integration (wordfreq, lexical-diversity, gensim, textdescriptives)
- Comprehensive test coverage: 317-line unit tests + 450-line dependency tests + 243-line integration tests
- Skeleton mode fallback for library failures

**â³ Remaining Work**

- Complete BatchNlpAnalysisHandler to use NlpAnalyzerRefactored
- Integrate NlpAnalyzerProtocol provider in main DI configuration
- End-to-end integration testing
- Language Tool Service microservice creation

## Architectural Compliance âœ…

**HuleEdu Pattern Adherence**

- âœ… Protocol-based DI with 6 dependency interfaces (Dishka scoping)
- âœ… Structured error handling via `huleedu_service_libs.error_handling`
- âœ… Outbox pattern for transactional event publishing
- âœ… Clean separation: unit tests (mocked), integration tests (real dependencies)
- âœ… Skeleton mode fallback for library failures

## Language Tool Service Implementation Requirements

### Service Architecture Decision

**Create as Separate Microservice** following HuleEdu's bounded context principles:

- **Location**: `services/language_tool_service/`
- **Technology**: Java-based Language Tool with Python FastAPI/Quart wrapper
- **Pattern**: Similar to LLM Provider Service architecture
- **Port**: 8085 (avoiding conflicts with existing services)

### Implementation Requirements

#### 1. Service Structure

```
services/language_tool_service/
â”œâ”€â”€ Dockerfile          # Multi-stage: Java + Python wrapper
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml      # Python dependencies for HTTP wrapper
â”œâ”€â”€ config.py           # Service configuration with BaseSettings
â”œâ”€â”€ app.py              # Quart/FastAPI HTTP wrapper
â”œâ”€â”€ di.py               # Dishka DI container setup
â”œâ”€â”€ protocols.py        # Protocol interfaces
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ grammar_routes.py   # POST /v1/check endpoint
â”‚   â””â”€â”€ health_routes.py    # Health checks with DatabaseHealthChecker
â”œâ”€â”€ implementations/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ language_tool_wrapper.py  # Java process management
â”‚   â””â”€â”€ outbox_manager.py         # Outbox pattern if needed
â”œâ”€â”€ metrics.py          # Prometheus metrics
â””â”€â”€ tests/
```

#### 2. Required Patterns (MANDATORY)

- **Dishka DI Container**: All dependencies via protocols
- **Structured Error Handling**: Use `huleedu_service_libs.error_handling`
- **Observability**: Full stack integration (logging, tracing, metrics)
- **Health Endpoints**: `/healthz` with dependency checks
- **Metrics Endpoint**: `/metrics` for Prometheus
- **Configuration**: Environment-based via Pydantic BaseSettings

#### 3. API Contract

```python
# POST /v1/check
{
    "text": str,
    "language": str,  # "en", "sv", "auto"
    "correlation_id": UUID
}

# Response
{
    "errors": [
        {
            "rule_id": str,
            "message": str,
            "short_message": str,
            "offset": int,
            "length": int,
            "replacements": list[str],
            "category": str,  # "grammar", "spelling", "style"
            "severity": str   # "error", "warning", "info"
        }
    ],
    "error_count": int,
    "errors_per_100_words": float,
    "category_breakdown": dict[str, int],
    "language": str,
    "processing_time_ms": int
}
```

#### 4. Docker Configuration

```dockerfile
# Multi-stage Dockerfile
FROM openjdk:11-jre-slim as java-base
RUN apt-get update && apt-get install -y wget unzip
RUN wget https://languagetool.org/download/LanguageTool-stable.zip
RUN unzip LanguageTool-stable.zip

FROM python:3.11-slim
COPY --from=java-base /LanguageTool-* /opt/languagetool/
RUN apt-get update && apt-get install -y openjdk-11-jre-headless
# Python app setup with PDM...
```

#### 5. Integration with NLP Service

- HTTP client implementation in `LanguageToolServiceClient`
- Circuit breaker pattern for resilience
- Timeout handling (30s max)
- Fallback to empty grammar analysis on failure
- Structured error propagation

## Advanced NLP Metrics Implementation

### Required Linguistic Metrics (STRICT REQUIREMENTS)

1. **Lexical sophistication**: mean Zipf, % tokens Zipf<3 (wordfreq)
2. **Lexical diversity**: MTLD/HDD (lexical_diversity)
3. **Syntactic complexity**: mean dependency distance (TextDescriptives), phrasal/clausal indices
4. **Cohesion**: first/second-order sentence similarity (TextDescriptives)
5. **Grammar accuracy**: From Language Tool Service
6. **Phraseology**: avg PMI/npmi of essay bigrams/trigrams (gensim/textacy)
   - Temporary limitation: metrics return `0.0` until corpus-backed frequency
     tables are shipped (documented in `PhraseologyCalculator`).

### Dependencies to Add

```toml
# services/nlp_service/pyproject.toml
dependencies = [
    # ... existing ...
    "spacy>=3.7.0",
    "wordfreq>=3.0.0",
    "lexical-diversity>=0.1.1",
    "textdescriptives>=2.8.0",
    "langdetect>=1.0.9",
    "gensim>=4.3.0",  # For phraseology metrics
]
```

## Next Steps After Implementation

1. **Phase 1**: Implement real spaCy analysis with specified metrics
2. **Phase 2**: Create Language Tool Service following HuleEdu patterns
3. **Phase 3**: Integration testing with full pipeline
4. **Phase 4**: Performance optimization for <500ms requirement
5. **Phase 5**: Comprehensive unit and integration tests

This plan ensures full compliance with HuleEdu architectural patterns while implementing the exact linguistic metrics required.
