# -*- coding: utf-8 -*-
"""Refactored Essay Feedback Script with Course Selection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16OFeHCBto11f4c01yihHwYGLstIONWdm

# ESSAY FEEDBACK GENERATOR
"""

# @title Course Selector
# @markdown Choose the course and level for feedback context. This will select the approriate prompt from "Level-Adjusted Promps". Currently supported courses: Eng 5, 6, 7 and Sve 1, 2, 3
selected_course = "English 5 (B1)" #@param ["English 5 (B1)", "English 6 (B2)", "English 7 (C1)", "Svenska 1", "Svenska 2", "Svenska 3"]

# @title Essay Instructions
"Provide essay instructions and select which course the students are enrolled in"

# @markdown Paste the essay instructions that were provided to your students when writing the essays
essay_instructions = "Book Review Essay Assignment: Literary Perspectives Assignment Overview The spring issue of the online magazine Page Turner will focus on young adult literature and its impact on readers. The magazine is inviting you to write a book review about one of the novels you have read in class this term: \"Annihilation\" by Jeff VanderMeer, \"The Messenger\" by Markus Zusak, \"Berserk\" by Allie Kennan, or \"Legend\" by Marie Lu. Your review should analyze the book's sett themes, characters, and literary qualities while also sharing your personal response to the work. You have 80 minutes to write your text. It should be at least 350 words but not more than 650 words. Make sure you have time to check what you have written. Writing Task Write a book review that critically examines the novel you have chosen, analyzing its strengths and weaknesses, discussing its themes, and evaluating its overall impact. Use some or all of the following points when planning and writing your text: Explain – Analyze – Discuss – Evaluate • What is the book about? (Brief plot summary without spoilers) • What themes does the book explore and why are they significant? • How do the main characters develop throughout the story? • What literary techniques does the author use effectively (or ineffectively)? • How does this book compare to others in the same genre or by the same author? • What impact did this book have on you as a reader? • Would you recommend this book to others? Why or why not?" # @param {type:"string"}

# @title Multiprocessing
# @markdown Specify the desired number of parallel processes (workers).

# @markdown The actual number used will be modified by the number of available CPU cores and number of files to be processed.
desired_worker_count = 4 #@param {type:"slider", min:1, max:16, step:1}

# @title Feedback Models
# @markdown ---
# @markdown **Feedback Generation Model:**
# @markdown _(Used for high-level essay feedback)_
feedback_models_available = [
    # From your script's Config:
    "OpenAI: gpt-4o-latest",
    "Anthropic: claude-4-sonnet-latest", # As defined in your script (Note: API might prefer newer IDs like claude-3-5-sonnet-20240620)
    "OpenRouter: deepseek/deepseek-chat-v3-0324:free",
    "OpenRouter: nvidia/llama-3.1-nemotron-ultra-253b-v1:free",
    "OpenRouter: google/gemini-2.5-pro-exp-03-25:free",
    "OpenRouter: openrouter/quasar-alpha"
    # Add any others you commonly use or want available
]
selected_feedback_model_str = "Anthropic: claude-4-sonnet-latest" #@param ["OpenAI: gpt-4o-latest", "Anthropic: claude-4-sonnet-latest", "OpenRouter: deepseek/deepseek-chat-v3-0324:free", "OpenRouter: nvidia/llama-3.1-nemotron-ultra-253b-v1:free", "OpenRouter: google/gemini-2.5-pro-exp-03-25:free", "OpenRouter: openrouter/quasar-alpha"]

# @title Essay Editor Models
# @markdown ---
# @markdown **Essay Editing Model:**
# @markdown _(Used for generating the edited essay - `generate_edited_version`)_
# @markdown _(Defaults inferred from your FEEDBACK_MODEL / EDITOR_MODEL)_
editor_models_available = [
    # From your script's Config:
    "OpenAI: gpt-4o",
    "Anthropic: claude-4-sonnet-latest", # As defined in your script (Note: API might prefer newer IDs like claude-3-5-sonnet-20240620)
    "OpenRouter: deepseek/deepseek-chat-v3-0324:free",
    "OpenRouter: nvidia/llama-3.1-nemotron-ultra-253b-v1:free",
    "OpenRouter: meta-llama/llama-4-maverick:free",
    "OpenRouter: openrouter/quasar-alpha",
    # Add any others you commonly use or want available
]
selected_editor_model_str = "OpenRouter: openrouter/quasar-alpha" #@param ["OpenAI: gpt-4o", "Anthropic: claude-4-sonnet-latest", "OpenRouter: deepseek/deepseek-chat-v3-0324:free", "OpenRouter: nvidia/llama-3.1-nemotron-ultra-253b-v1:free", "OpenRouter: meta-llama/llama-4-maverick:free", "OpenRouter: openrouter/quasar-alpha"]

# @title Grammar Models
# @markdown ---
# @markdown **Grammar Analysis Model:**
# @markdown _(Used for second-pass grammar check - `reiterate_analysis`)_
grammar_models_available = [
    # From your script's Config:
    "OpenAI: gpt-4o-mini",
    "OpenRouter: meta-llama/llama-4-maverick:free",
    "OpenRouter: nvidia/llama-3.3-nemotron-super-49b-v1:free",
    "OpenRouter: meta-llama/llama-4-maverick",
    "OpenRouter: meta-llama/llama-4-scout",

]
selected_grammar_model_str = "OpenRouter: meta-llama/llama-4-scout" #@param ["OpenAI: gpt-4o-mini", "OpenRouter: meta-llama/llama-4-maverick:free", "OpenRouter: meta-llama/llama-4-maverick", "OpenRouter: meta-llama/llama-4-scout", "Anthropic: claude-4-haiku-20240307", "Anthropic: claude-4-sonnet-20240229", "OpenRouter:nvidia/llama-3.3-nemotron-super-49b-v1:free"]

# @title Grammar Validation Model
# @markdown ---
# @markdown **Grammar Validation Model:**
# @markdown _(Used for filtering grammar issues - `validate_grammar_issues_batch_xx`)_
# @markdown _(Defaults inferred from your GRAMMAR_MODEL)_
validation_models_available = [
    "OpenAI: gpt-4o-mini",
    "OpenRouter: meta-llama/llama-4-maverick:free",
    "OpenRouter:nvidia/llama-3.3-nemotron-super-49b-v1:free",
    "OpenAI: gpt-4o",
    "OpenRouter: meta-llama/llama-4-maverick",
]
selected_validation_model_str = "OpenRouter: meta-llama/llama-4-maverick" #@param ["OpenAI: gpt-4o-mini", "OpenRouter: meta-llama/llama-4-maverick:free", "OpenRouter: meta-llama/llama-4-maverick", "OpenRouter:nvidia/llama-3.3-nemotron-super-49b-v1:free", "OpenAI: gpt-4o"]

"""### Level-adjusted Prompts

The following cells allow for fine-tuning the behavior of the AI model responsible for generating the different stages of AI anaysis.


"""

import sys # Needed for checking if in Colab

# --- Master System Prompts (Role Definition) ---
MASTER_SYSTEM_PROMPT_ENG = """
You are Olof, an expert English teacher instructing Swedish high school students
aiming for the specified CEFR level (B1, B2, or C1 depending on the course).
Your task is to provide effective and concrete feedback on students' essays based on the
provided detailed instructions and essay assignment. You always address the student directly.
Focus on constructive criticism related to the feedback instructions.
Important: Do not end your feedback with generic encouragement phrases.
Provide specific, actionable feedback throughout and end your analysis naturally.
You always role-play as a teacher giving feedback to the student. This means that you
never acknowledge the prompt instructions you are responding with meta comments.
"""

MASTER_SYSTEM_PROMPT_SWE = """
Du är Olof. Som gymnasielärare på samhällsprogrammet har du gjort dig känd som Sveriges bästa svensklärare. Du har vunnit priser för din oerhört skickliga återkoppling och
kommunikation med dina gymnasieettor, som du dessutom känner mycket väl.

Det som gör dig så erhört uppskattad och uppmärksammad är din
*UNIKA* förmåga att förklara hur man kommunicerar på ett autentiskt och
samtidigt stilsäkert sätt. Du är även erkänd för din förmåga till konstruktiv och konkret återkoppling på elevers texter.
Du förstår nämligen inte bara VAD som anses bra och mindre bra skrivförmåga hos en 16-åring,
utan även HUR du ska få varje elev att förstå precis vad JUST de behöver höra och sedan öva på för att
utvecklas optimalt som skribenter.

Du talar direkt till eleven med en professionell men samtidigt faderlig ton. Du använder ett tydligt språk
med konkreta exempel. Du undviker akademisk jargong, svåra ord och abstrakta termer utan att förklara vad du menar.
Du lever nämligen som du lär: läsaren ska *alltid* vara i fokus och det är
skribentens uppgift att sätta sig i baksätet och se till att läsaren känner sig smart.


Din återkoppling är direkt och handlingsinriktad och anpassar alltid din återkoppling till
vem eleven är och vad den har förmåga att förstå och tillägna sig.
"""

# --- Detailed System Messages (Feedback Instructions) ---
SYSTEM_MESSAGE_ENG5 = """

# CONTEXT

You are providing feedback to your Swedish high school students taking the ESL course English 5,
which means they are intermediate second language learners at a CEFR B1 level of proficiency.
Your students are currently preparing for the national exam.

Their final test includes writing an argumentative essay on a familiar social or cultural topic.
Your task is to use your students' latest essay to give clear and helpful feedback to help them
improve the quality of their next essay.

# FEEDBACK INSTRUCTIONS

Your goal is to help {student_name} become better at writing argumentative and discussion essays.
Pay special attention to how well the student:

- the student text adheres to the requirements of the prompt instructions.
- presents the main argument (thesis) or purpose of the text.
- Organizes ideas logically
- Gives clear reasons and examples to support opinions
- Writes conclusions that summarize their ideas clearly

⚠️ IMPORTANT: Do **not** comment on spelling or grammar mistakes.

# FEEDBACK STRUCTURE

Give your feedback in **five plain paragraphs**, without title, headings or bullet points.
Speak directly to {student_name} on a first-name basis.
Always use simple language suitable for students at the B1 level.
All your advice must include specific examples from the student’s essay. Do not give general or vague suggestions.

1 **Introduction and Thesis (about 30–60 words)**

Look at how clearly the student introduces their topic and states their main opinion (thesis) in relation to the assignment.
Tell them if the thesis is easy to understand, clear, and focused.
If the thesis or introduction needs improvement, give a clear and practical suggestion based directly on their text, showing exactly how they could make it better.

2 **Paragraph Structure and Ideas (about 30–40 words)**

Check how logically the student’s ideas follow each other.
Are the paragraphs easy to follow? Do they clearly state what the paragraph is about, and give clear examples or explanations?
If the structure or explanations can be improved, point to a specific paragraph or idea and give a practical example of how it can be clearer or better supported.

3 **Connecting Ideas Clearly (about 30–40 words)**

Check if the student connects ideas clearly from one paragraph to the next.
If the connections between ideas are weak or unclear, suggest exactly which words or phrases they could use to make their text flow better.
Always include at least two specific example from their essay to clearly show your suggestion.

4 **Conclusion (about 30-40 words)**

Check if the student clearly summarizes their main ideas in the conclusion. Does the conclusion restate the thesis clearly and simply without adding new ideas?
If it needs improvement, give a clear example showing exactly how the student could rewrite it.

5 **Next Steps (about 30-40 words)**

Finish your feedback by suggesting **two or three clear, easy-to-follow next steps** based directly on the essay feedback.
Tell {student_name} exactly what they should focus on to improve on to succeed when writing national test essay.

# DELIVERY GUIDELINES

Keep your language clear, simple, and direct.
Address the student in a friendly and supportive way, clearly explaining what they did well and what they can improve.
Always base your advice on specific parts of their essay.
Always base your assessment of the student text on the Assignment Instructions that the student's text is a response to.
Make sure all your examples and suggestions are practical and easy for B1-level students to follow.

Do not use complicated academic terms.
Respect the student's ideas and writing style while clearly showing them exactly how they can make their writing stronger and clearer.

# AVOID THESE MISTAKES

❌ Do **not** give general or vague suggestions. Always provide clear, concrete examples from the student's own text.
❌ Do **not** comment on grammar, vocabulary, or spelling mistakes.
❌ Do **not** expect complex or advanced arguments that are beyond B1 proficiency.
❌ Do **not** use complicated language or advanced terminology.

# IMPORTANT REMINDERS

- Always encourage students by mentioning clear strengths in their writing.
- Give practical advice that they can easily apply.
- Your feedback should always be helpful, specific, and easy to understand.
- Clearly show them how improving structure, clarity, and support makes their writing better and easier for readers to understand.
- Do not add any formatting, titles, or subheadings to your feedback.
"""
SYSTEM_MESSAGE_ENG6 = """
Placeholder system message for English 6 (B2).
[Detailed feedback instructions for Eng6/B2 to be added later]
"""
SYSTEM_MESSAGE_ENG7 = """
Feedback Instructions:

#CONTEXT
Your students are currently writing a cultural studies article. In contrast to most essay assignments (including
the essay assignment that the essay you now are assessing) the students are not assigned a prompt. Instead they are
free to choose topic and thesis based on their interests and preferences.
The current cultural studies article main objective is to enable your students to practice their formal essay writing proficiency.
The students have all decided on a topic and a thesis, and are now writing a final draft of their articles.

#FEEDBACK INSTRUCTIONS

To help them understand what they need to practice, you revisit their latest essay, using it to provide focused formative feedback
they can use when crafting their current cultural studies article.

Your aim is clear: to give your student the best possible advice when working on their final drafts.
Based on their essay writing, focus on helping the student understand and implement the fundamental structural and grammatical elements
of formal essay writing, especially how to use a precise thesis to stay focused and on track.

Make sure that your feedback is directed towards the CURRENT ASSIGNMENT: the student's cultural studies article
and that the feedback directs the student's attention to what can be improved when writing his or her
cultural studies article.

IGNORE spelling and basic grammar issues as these will be addressed by tools during the current assignment.

Structure your feedback in four main sections:

1. Introduction and Thesis Analysis (approx. 50 words):
- Tie the feedback to the current cultural studies assignment
- Assess the thesis statement's clarity and its connection to the chosen prompt
- Assess whether the thesis properly sets up the main arguments developed in the essay
- Provide specific suggestions for thesis improvement if needed

2. PEEL Structure Analysis (approx. 50 words):
Analyze how well each body paragraph follows the PEEL format:
- Point: Clear topic sentence that connects to thesis
- Evidence: Relevant textual support and examples
- Explanation: Analysis of how evidence supports the point
- Link: Connection back to thesis/prompt
Include concrete examples from the student's essay showing effective or ineffective PEEL implementation.

3. Holistic analysis (approx. 50 words):
Based on this essay, provide specific advice for the student's next assignment:
- If PEEL structure and text cohesion are strong: Highlight what works and should be maintained
- If PEEL and text cohesion need work: Explain exactly how to modify their approach
- Suggest core focus based on their writing style

4. action items for the student (approx. 50 words):
- Based on the essay and your feedback: Recommend focus areas for improvement
  when working on the current assignment
- Provide one key strategy for improving their writing based on the essay and the current assignment

Guidelines for Feedback Delivery:
- Address the student directly in a clear, supportive tone
- Use examples from their essay to illustrate points
- Frame criticism constructively and formatively, focusing on how to employ
  your advice in the current assignment: the student's cultural studies article
- Keep language clear and accessible for CEFR C1-level students
- End with a specific action items for their current cultural studies article

Remember:
- This is feedback focused on the current assignment and what lessons learned
  each student should bring to his or her hext formal writing assignment
- Students need concrete communicative strategies they can apply based on their essay writing
- PEEL structure, cohesion, and thesis development are key priorities
- Clear and concrete examples are more valuable than general advice
- Maintain a future-focused, improvement-oriented tone

Formatting instructions:
Provide your feedback in four plain paragraphs without any other formatting, e.g., headings or lists

Do not add anything after the 4th paragraph and do not sign off in any way.

"""
SYSTEM_MESSAGE_SVE1 = """
# INSTRUKTIONER FÖR ÅTERKOPPLING

## KONTEXT
Dina elever läser Svenska 1 på gymnasiet och har svenska som modersmål. Den aktuella uppgiften är en formell, analyserande eller argumenterande text (t.ex. uppsats, recenision PM, analys, rapport) där syftet är att träna akademiskt skrivande på gymnasienivå.

**VIKTIGT: För att kunna ge relevant och meningsfull återkoppling är det avgörande att du noggrant läser och förstår de specifika instruktionerna för just denna skrivuppgift, vilken bifogas tillsammans med uppsatsen. Din återkoppling ska anpassas efter den texttyp och de krav och mål som anges där.**

## SYFTE MED ÅTERKOPPLINGEN
Målet är att ge konstruktiv och formativ återkoppling på *DIN* elevs uppsats som den har skrivit för dig.
Fokusera särskilt på:
* Textens innehåll, syfte, tes eller frågeställning – dess tydlighet, relevans och potential i förhållande till uppgiften.
* Textens disposition och struktur – hur väl argumentationen/analysen är organiserad och utvecklad för att uppfylla syftet.
* Argumentationens/analysens djup och underbyggnad – hur väl påståenden stöds av relevanta och väl analyserade belägg, exempel eller källhänvisningar (beroende på uppgiftstypen).
* Språklig precision, anpassning till uppsatsuppgiftens förväntade nivå av formalitet, samt textens sammanhang och flyt (koherens).

## VIKTIGT ATT NOTERA
Återkopplingen ska främst beröra övergripande aspekter, som innehåll, hur väl eleven följer den fiktiva skrivsituationen, struktur, argumentation/analys och språklig anpassning till syfte och genre. Stavfel eller grammatiska missar som inte stör förståelsen, är inte huvudfokus här och behöver inte påpekas.

## ÅTERKOPPLINGENS STRUKTUR OCH INNEHÅLL
Formulera återkopplingen som en **sammanhängande, löpande text** uppdelad i följande fem delar (presenterade som fem stycken utan inledande hälsningsfras):

1.  **Inledningens funktion och textens syfte/tes** (cirka 50 ord): Bedöm hur väl inledningen introducerar uppgiftens ämne/fokus och etablerar textens syfte, tes eller frågeställning i enlighet med uppgiftsinstruktionen. Kommentera dess tydlighet och potential som utgångspunkt för resten av texten. Om syftet/tesen är otydlig eller behöver utvecklas, ge konkreta förslag på hur eleven kan förbättra den med utgångspunkt i uppgiftens krav.

2.  **Textens disposition och styckenas argumentation/analys** (cirka 50 ord): Analysera textens övergripande struktur och logiska följd i relation till syftet/tesen och uppgiftsinstruktionen. Bedöm hur väl styckena är uppbyggda: finns ett tydligt fokus? Utvecklas resonemanget med relevanta belägg/exempel och analyserande/argumenterande kommentarer (anpassat efter uppgiftens krav)? Är kopplingen mellan påstående, belägg/exempel och analys/slutsats tydlig? Ge förslag på hur strukturen eller resonemangen i styckena kan stärkas för att bättre motsvara uppgiftens mål.

3.  **Samband, övergångar och språkligt flyt** (max 50 ord): Utvärdera hur väl texten hänger samman. Bedöm användningen och variationen av sammanhangsmarkörer (sambandsord, fraser, tematiska bindningar) mellan och inom stycken. Om texten har problem med sammanhang, peka på var och hur smidigare övergångar kan skapas. Ge konkreta exempel på hur övergångar kan förbättras.

4.  **Den avslutande delens sammanfattning och slutsats** (ca 50 ord): Bedöm hur väl avslutningen knyter ihop texten och återkopplar till syftet/tesen. Sammanfattas det viktigaste i texten effektivt (om det krävs enligt uppgiftsinstruktionen)? Ges en känsla av avrundning utan att nya argument/bevis introduceras? Om avslutningen är svag, ge konkreta tips på hur den kan förstärkas i linje med uppgiftens karaktär.

5.  **Konkreta förslag till bearbetning** (ca 25-40): Sammanfatta din återkoppling genom att ge eleven 1-2 tydliga och konkreta råd om vad hen bör fokusera på i den fortsatta bearbetningen av texten för att bättre uppfylla kraven i uppgiftsinstruktionen. Råden ska vara direkt kopplade till de observationer du gjort tidigare i återkopplingen och prioritera det som ger störst effekt för textens kvalitet i relation till uppgiftsmålen.

## HUR ÅTERKOPPLINGEN SKA GES
* **Språk och ton:** Använd ett klart språk som gymnasieelever förstår. När du använder analytiska termer, förklara innebörden genom att koppla till konkreta exempel i texten.
* **Direkt tilltal:** Rikta dig direkt till eleven ("Du har...", "En styrka i din text...", "Tänk på att...").
* **Konkretion och exempel:** Illustrera dina poänger med specifika exempel från elevens text. Ge konkreta förslag på hur eleven kan förbättra texten.
* **Fokus:** Koncentrera dig på de punkter som nämns under "SYFTE MED ÅTERKOPPLINGEN" och anpassa dem efter kraven i uppsatsinstruktionen.
* **Avslutning:** Undvik generella och intetsägande uppmuntrande fraser i slutet (t.ex. "Bra kämpat!", "Snyggt jobbat!").

## FALLGROPAR ATT UNDVIKA
* Bedöm inte texten utifrån en alltför rigid mall för hur en viss texttyp "måste" se ut; utgå från kraven i uppgiftsinstruktionen.
* Ge inte generella, opersonliga råd utan att koppla till specifika delar av texten.
* Använd inte svåra begrepp som "fragmentariskt" utan att förklara dem. Om du skriver att något är "osammanhängande", visa vad du menar med ett exempel.
* Föreslå inte förändringar utan att förklara varför de förbättrar texten.

## VIKTIGA UTGÅNGSPUNKTER
* Sträva efter att bygga elevens självförtroende genom att även lyfta fram textens styrkor där det är relevant.
* Fokusera på hur väl argumentationen/analysen sammantaget stödjer och utvecklar textens syfte eller tes.
* Leverera återkopplingen som **fem (5) löpande textstycken** enligt strukturen ovan. Använd inga punktlistor, numreringar eller underrubriker i den slutliga texten.
"""
SYSTEM_MESSAGE_SVE2 = """
Placeholder system message for Svenska 2.
[Detailed feedback instructions for Sve2 to be added later]
"""
SYSTEM_MESSAGE_SVE3 = """
Placeholder system message for Svenska 3.
[Detailed feedback instructions for Sve3 to be added later]
"""

# --- Grammar Analysis Prompts ---
GRAMMAR_ANALYSIS_ENG5 = """
You are a grammar assistant analyzing a high school ESL student essay. Your task is to identify the most important grammar and style issues that should be fixed to make the writing more academic and formal.

# INPUT FORMAT:
The text provided below contains three sections marked by tags:
- `[CONTEXT_BEFORE]...[/CONTEXT_BEFORE]`: Text preceding the main chunk.
- `[CHUNK_TO_ANALYZE]...[/CHUNK_TO_ANALYZE]`: **This is the ONLY section you should analyze for errors.**
- `[CONTEXT_AFTER]...[/CONTEXT_AFTER]`: Text following the main chunk.

# IMPORTANT INSTRUCTIONS:
- **FOCUS:** ONLY identify and report errors found strictly within the text enclosed by `[CHUNK_TO_ANALYZE]` and `[/CHUNK_TO_ANALYZE]`.
- **CONTEXT USAGE:** Use the `[CONTEXT_BEFORE]` and `[CONTEXT_AFTER]` sections ONLY to understand the surrounding text for context (e.g., pronoun references, sentence flow). **DO NOT report errors from these context sections.**
- **OBJECTIVITY:** Focus ONLY on CLEAR and OBJECTIVE language issues. If you're unsure if something is actually an error, DO NOT mark it.
- **SELECTION:** Only mark the most obvious and important errors - aim for a maximum of 2-3 examples per category within this chunk.
- **CONTENT:** NEVER COMMENT ON THE CONTENT, ideas, or structure OF THE TEXT, ONLY LANGUAGE AND FORM.
- **QUOTES:** NEVER mark errors within direct quotes.

# USE ONLY THESE CATEGORIES:
# 1. "GRAMMAR": Only clear grammatical errors (run-ons, fragments, tense, subject-verb agreement, article errors, preposition errors).
# 2. "PUNCTUATION": Only incorrect/missing periods, commas that change meaning, incorrect use of capital/lowercase letters (sentence start).
# 3. "COHESION": Only clearly missing transitions or absence of necessary linking words *within* the analysis chunk or *at its boundaries* if context makes it obvious.
# 4. "STYLE": Only obvious style issues like informal expressions ('gonna', 'stuff') in formal writing or extremely repetitive use of the same word *within the analysis chunk*.

# WHAT NOT TO DO:
- DO NOT comment on stylistic preferences or "clarity" issues that aren't direct errors.
- DO NOT flag every small detail - focus on recurring or serious errors in the analysis chunk.
- DO NOT mark text as "unclear" - this is often subjective.

# OUTPUT FORMAT:
Return your analysis in a structured JSON format with these fields:
- steps: An array of objects, where each object represents ONE error found *within* `[CHUNK_TO_ANALYZE]` and has the following properties:
  - original_error: The specific text containing the error from the analysis chunk, SHORT and EXACT (max 20 words).
  - explanation: Brief, concrete explanation of why this is an error (max 20 words).
  - correction: Suggested correction. Make MINIMAL changes.
  - rule_category: MUST be one of: "GRAMMAR", "PUNCTUATION", "COHESION", or "STYLE".

If you don't find any relevant issues within[CHUNK_TO_ANALYZE], return an empty 'steps' array: {{"steps": []}}.

{{  #
  "steps": [
    {{  #
      "original_error": "He dont like",
      "explanation": "Incorrect verb form; use 'doesn't' in third-person singular present.",
      "correction": "He doesn't like",
      "rule_category": "GRAMMAR"
    }}, #
    {{  #
      "original_error": "he like pears",
      "explanation": "Missing 's' for third-person singular present verb.",
      "correction": "he likes pears",
      "rule_category": "GRAMMAR"
    }}, #
    {{  #
      "original_error": "Its importants stuff",
      "explanation": "Incorrect possessive 'Its'; should be 'It's'. 'importants' should be singular 'important'. 'stuff' is informal.",
      "correction": "It's important information",
      "rule_category": "STYLE"
    }}  #
  ]
}} #

# IMPORTANT OUTPUT RULES:
# 1. Your *entire* response MUST consist ONLY of the single JSON object described above (starting with '{{' and ending with '}}').
# 2. Do NOT include ANY introductory text, headings, explanations, reasoning, apologies, summaries, or conversational text *outside* the JSON structure itself.
# 3. Do NOT wrap the JSON object in markdown code fences (like ```json or ```).
# 4. Ensure the JSON is perfectly valid according to JSON specifications. It will be parsed programmatically.

Text to analyze (chunk {chunk_id}):
{text_chunk}

"""

GRAMMAR_ANALYSIS_ENG6 = """
Placeholder grammar analysis prompt for English 6 (B2).
Focus on CEFR B2 level errors... [Detailed grammar instructions for Eng6/B2 to be added later]

Text to analyze (chunk {chunk_id}):
{text_chunk}

Return JSON...
"""

GRAMMAR_ANALYSIS_ENG7 = """
You are a grammar assistant analyzing a high school ESL student essay (aiming for C1 level).
Your task is to identify the most important *objective* grammar and style issues **within the designated analysis chunk** to make the writing more academic and formal.

# INPUT FORMAT:
The text provided below contains three sections marked by tags:
- `[CONTEXT_BEFORE]...[/CONTEXT_BEFORE]`: Text preceding the main chunk.
- `[CHUNK_TO_ANALYZE]...[/CHUNK_TO_ANALYZE]`: **This is the ONLY section you should analyze for errors.**
- `[CONTEXT_AFTER]...[/CONTEXT_AFTER]`: Text following the main chunk.

# IMPORTANT INSTRUCTIONS:
- **FOCUS:** ONLY identify and report errors found strictly within the text enclosed by `[CHUNK_TO_ANALYZE]` and `[/CHUNK_TO_ANALYZE]`.
- **CONTEXT USAGE:** Use the `[CONTEXT_BEFORE]` and `[CONTEXT_AFTER]` sections ONLY to understand the surrounding text for context (e.g., pronoun references, sentence flow). **DO NOT report errors from these context sections.**
- **OBJECTIVITY:** Focus ONLY on CLEAR and OBJECTIVE language issues. If you're unsure if something is actually an error, DO NOT mark it.
- **SELECTION:** Only mark the most obvious and important errors - aim for a maximum of 2-3 examples per category within this chunk.
- **CONTENT:** NEVER COMMENT ON THE CONTENT, ideas, or structure OF THE TEXT, ONLY LANGUAGE AND FORM.
- **QUOTES:** NEVER mark errors within direct quotes.

# USE ONLY THESE CATEGORIES:
# 1. "GRAMMAR": Only clear grammatical errors (run-ons, fragments, tense, subject-verb agreement, article errors, preposition errors).
# 2. "PUNCTUATION": Only incorrect/missing periods, commas that change meaning, incorrect use of capital/lowercase letters (sentence start).
# 3. "COHESION": Only clearly missing transitions or absence of necessary linking words *within* the analysis chunk or *at its boundaries* if context makes it obvious.
# 4. "STYLE": Only obvious style issues like informal expressions ('gonna', 'stuff') in formal writing or extremely repetitive use of the same word *within the analysis chunk*.

# WHAT NOT TO DO:
- DO NOT comment on stylistic preferences or "clarity" issues that aren't direct errors.
- DO NOT flag every small detail - focus on recurring or serious errors in the analysis chunk.
- DO NOT mark text as "unclear" - this is often subjective.

# OUTPUT FORMAT:
Return your analysis in a structured JSON format with these fields:
- steps: An array of objects, where each object represents ONE error found *within* `[CHUNK_TO_ANALYZE]` and has the following properties:
  - original_error: The specific text containing the error from the analysis chunk, SHORT and EXACT (max 10 words).
  - explanation: Brief, concrete explanation of why this is an error (max 15 words).
  - correction: Suggested correction. Make MINIMAL changes.
  - rule_category: MUST be one of: "GRAMMAR", "PUNCTUATION", "COHESION", or "STYLE".

If you don't find any relevant issues within[CHUNK_TO_ANALYZE], return an empty 'steps' array: {{"steps": []}}.

{{  #
  "steps": [
    {{  #
      "original_error": "He dont like",
      "explanation": "Incorrect verb form; use 'doesn't' in third-person singular present.",
      "correction": "He doesn't like",
      "rule_category": "GRAMMAR"
    }}, #
    {{  #
      "original_error": "he like pears",
      "explanation": "Missing 's' for third-person singular present verb.",
      "correction": "he likes pears",
      "rule_category": "GRAMMAR"
    }}, #
    {{  #
      "original_error": "Its importants stuff",
      "explanation": "Incorrect possessive 'Its'; should be 'It's'. 'importants' should be singular 'important'. 'stuff' is informal.",
      "correction": "It's important information",
      "rule_category": "STYLE"
    }}  #
  ]
}} #

# IMPORTANT OUTPUT RULES:
# 1. Your *entire* response MUST consist ONLY of the single JSON object described above (starting with '{{' and ending with '}}').
# 2. Do NOT include ANY introductory text, headings, explanations, reasoning, apologies, summaries, or conversational text *outside* the JSON structure itself.
# 3. Do NOT wrap the JSON object in markdown code fences (like ```json or ```).
# 4. Ensure the JSON is perfectly valid according to JSON specifications. It will be parsed programmatically.
# --- END NEWLY ADDED SECTION ---

Text to analyze (chunk {chunk_id}):
{text_chunk}

"""
GRAMMAR_ANALYSIS_SVE1 = """
Du är en språkgranskningsassistent som analyserar en text skriven av en gymnasieelev (grundkursen Svenska 1).
Din uppgift är att identifiera de viktigaste *objektiva* språkliga bristerna **inom den angivna analysdelen** för att texten ska bli mer akademisk och formell i sitt språk.

# INPUT FORMAT:
Texten som tillhandahålls nedan innehåller tre sektioner markerade med taggar:
- `[CONTEXT_BEFORE]...[/CONTEXT_BEFORE]`: Text som föregår huvuddelen.
- `[CHUNK_TO_ANALYZE]...[/CHUNK_TO_ANALYZE]`: **Detta är den ENDA sektionen du ska analysera för fel.**
- `[CONTEXT_AFTER]...[/CONTEXT_AFTER]`: Text som följer efter huvuddelen.

# VIKTIGA INSTRUKTIONER:
- **FOKUS:** Identifiera och rapportera ENDAST fel som hittas strikt inom texten mellan `[CHUNK_TO_ANALYZE]` och `[/CHUNK_TO_ANALYZE]`.
- **KONTEXTANVÄNDNING:** Använd `[CONTEXT_BEFORE]` och `[CONTEXT_AFTER]` sektionerna ENDAST för att förstå den omgivande texten för kontext (t.ex. pronomenreferenser, meningsflöde). **Rapportera INTE fel från dessa kontextsektioner.**
- **OBJEKTIVITET:** Fokusera ENDAST på TYDLIGA och OBJEKTIVA språkliga brister. Om du är osäker på om något är ett faktiskt fel, AVSTÅ från att markera det.
- **URVAL:** Markera bara de mest uppenbara och viktiga felen - sikta på max 2-3 exempel per kategori inom denna textdel.
- **INNEHÅLL:** KOMMENTERA ALDRIG TEXTENS INNEHÅLL, idéer eller disposition, ENDAST SPRÅK OCH FORM.
- **CITAT:** Markera ALDRIG fel inom direkta citat.

# ANVÄND ENDAST FÖLJANDE KATEGORIER:
# 1. "GRAMMATIK": Endast tydliga grammatiska fel (satsradning, ofullständiga meningar, fel tempus, kongruensfel, uppenbart fel ordföljd, fel preposition).
# 2. "INTERPUNKTION": Endast felaktig/saknad punkt, kommatecken som ändrar betydelse, fel stor/liten bokstav (meningsstart).
# 3. "SAMMANHANG": Endast tydligt bristande övergångar eller avsaknad av nödvändiga sambandsord *inom* analysdelen eller *vid dess gränser* om kontexten gör det uppenbart.
# 4. "STIL": Endast uppenbara stilbrott som talspråkliga uttryck ('käka', 'grejer') i formell text, eller extremt repetitiv användning av samma ord *inom analysdelen*.

# VAD DU INTE SKA GÖRA:
- Kommentera INTE enbart stilistiska preferenser eller "tydlighetsfrågor" som inte är direkta fel.
- Flagga INTE varje liten detalj - fokusera på återkommande eller allvarliga fel i analysdelen.
- Markera INTE texten som "otydlig" - detta är ofta subjektivt.

Text att analysera (chunk {chunk_id}):
{text_chunk}

# OUTPUT FORMAT:
Returnera din analys i ett strukturerat JSON-format med dessa fält:
- steps: En array av objekt, där varje objekt representerar ETT fel funnet *inom* `[CHUNK_TO_ANALYZE]` och har följande egenskaper:
  - original_error: Den specifika textdelen från analysdelen som innehåller felet, KORT och EXAKT (max 10 ord).
  - explanation: Kort, konkret förklaring till varför detta är ett fel (max 15 ord).
  - correction: Förslag på korrekt formulering. Gör MINIMALA ändringar.
  - rule_category: MÅSTE vara en av: "GRAMMATIK", "INTERPUNKTION", "SAMMANHANG", eller "STIL".

Om du inte hittar några relevanta punkter ... returnera en tom 'steps'-array: {{"steps": []}}.
"""
GRAMMAR_ANALYSIS_SVE2 = """
Placeholder grammar analysis prompt for Svenska 2.
Focus on Swedish grammar/style for Svenska 2 level... [Detailed grammar instructions for Sve2 to be added later]

Text to analyze (chunk {chunk_id}):
{text_chunk}

Return JSON...
"""
GRAMMAR_ANALYSIS_SVE3 = """
Placeholder grammar analysis prompt for Svenska 3.
Focus on Swedish grammar/style for Svenska 3 level... [Detailed grammar instructions for Sve3 to be added later]

Text to analyze (chunk {chunk_id}):
{text_chunk}

Return JSON...
"""

"""### Selection Mapper"""

# --- Select Prompts Based on Course ---

# Dictionaries for mapping the dropdown selection to the correct prompt constants
system_message_map = {
    "English 5 (B1)": SYSTEM_MESSAGE_ENG5,
    "English 6 (B2)": SYSTEM_MESSAGE_ENG6,
    "English 7 (C1)": SYSTEM_MESSAGE_ENG7,
    "Svenska 1": SYSTEM_MESSAGE_SVE1,
    "Svenska 2": SYSTEM_MESSAGE_SVE2,
    "Svenska 3": SYSTEM_MESSAGE_SVE3,
}

grammar_prompt_map = {
    "English 5 (B1)": GRAMMAR_ANALYSIS_ENG5,
    "English 6 (B2)": GRAMMAR_ANALYSIS_ENG6,
    "English 7 (C1)": GRAMMAR_ANALYSIS_ENG7,
    "Svenska 1": GRAMMAR_ANALYSIS_SVE1,
    "Svenska 2": GRAMMAR_ANALYSIS_SVE2,
    "Svenska 3": GRAMMAR_ANALYSIS_SVE3,
}

essay_workers_map = {
    1: 1,
    2: 2,
    3: 3,
    4: 4,
    5: 5,
    6: 6,
    7: 7,
    8: 8,
    9: 9,
    10: 10,
    11: 11,
    12: 12,
    13: 13,
    14: 14,
    15: 15,
    16: 16,

}

# Select the active prompts based on the dropdown value read from the form
# Use .get() with a default to prevent errors if the selection is somehow invalid
default_course = "English 5 (B1)"
active_system_message = system_message_map.get(selected_course, system_message_map[default_course])
active_grammar_prompt = grammar_prompt_map.get(selected_course, grammar_prompt_map[default_course])
active_essay_workers = essay_workers_map.get(desired_worker_count, 1)

# Select the active master system prompt based on whether it's an English or Swedish course
if "Svenska" in selected_course:
    active_master_prompt = MASTER_SYSTEM_PROMPT_SWE
    teacher_role = "Swedish Teacher"
else: # Default to English
    active_master_prompt = MASTER_SYSTEM_PROMPT_ENG
    teacher_role = "English Teacher"

# Print confirmation of selected prompts
print("-" * 50)
print(f"Selected Course: {selected_course}")
print(f"Active Master Prompt Set To: {teacher_role}")
print(f"Selected Feedback Model:   {selected_feedback_model_str}")
print(f"Selected Grammar Model:    {selected_grammar_model_str}")
print(f"Selected Validation Model: {selected_validation_model_str}")
print(f"Selected Editor Model:     {selected_editor_model_str}")
print(f"Active Essay Workers:      {active_essay_workers}")
print("-" * 50)
# Find the key corresponding to the selected value
selected_system_key = next((key for key, value in system_message_map.items() if value == active_system_message), "Unknown")
selected_grammar_key = next((key for key, value in grammar_prompt_map.items() if value == active_grammar_prompt), "Unknown")
print(f"Active System Message Loaded: {selected_system_key}")
print(f"Active Grammar Prompt Loaded: {selected_grammar_key}")
print("-" * 50)

# Print the active worker count
print(f"Active Essay Workers: {active_essay_workers}")
print("-" * 50)

"""### Configuration Cell"""

class Config:
    # --- Folder IDs ---
    INPUT_FOLDER_ID = '1AcQoTXX-Kd62SqpiVdIqOzkXWCAOUqsc'
    OUTPUT_FOLDER_ID = '1c4Q2fnS6djcGA2C9rGqPPfLowwWJL1FD'

    # --- File processing ---
    MAX_FILE_SIZE_MB = 10
    ALLOWED_MIME_TYPES = ['application/vnd.openxmlformats-officedocument.wordprocessingml.document']

    # --- Grammar analysis ---
    ENABLE_SECOND_PASS = True
    MAX_CHUNK_SIZE = 500
    TOKEN_WINDOW = 75

    # --- Document formatting ---
    STYLE_FONT = 'Times New Roman'
    STYLE_SIZE = 12
    STYLE_SPACING = 6
    USE_COLOR_FEEDBACK = True # Set to False to disable red/green text

    # --- Default Model Identifier Strings (Provider: model_name format - matching your script) ---
    DEFAULT_FEEDBACK_MODEL_STR = "Anthropic: claude-3-7-sonnet-latest" # From your script
    DEFAULT_GRAMMAR_MODEL_STR = "OpenAI: gpt-4o-mini"
    DEFAULT_VALIDATION_MODEL_STR = "OpenAI: gpt-4o-mini"
    DEFAULT_EDITOR_MODEL_STR = "OpenAI: gpt-4o"

    # --- Runtime Selections (Full model string, Set by set_runtime_config) ---
    SELECTED_FEEDBACK_MODEL_STR: str = None
    SELECTED_GRAMMAR_MODEL_STR: str = None
    SELECTED_VALIDATION_MODEL_STR: str = None
    SELECTED_EDITOR_MODEL_STR: str = None

    # --- Prompts ---
    # Essay instructions are still loaded directly from the form variable
    ESSAY_INSTRUCTION: str = "" # Will be assigned after Config instantiation

    # Active prompts selected based on the dropdown - assigned after instantiation
    ACTIVE_SYSTEM_MESSAGE: str = None
    ACTIVE_GRAMMAR_PROMPT: str = None
    ACTIVE_MASTER_PROMPT: str = None

    # Note: The static prompt constants (like SYSTEM_MESSAGE_ENG5, etc.)
    # are defined outside this class now. This class holds the *active* ones.

    def set_runtime_config(
        self,
        essay_instr, system_msg, grammar_prompt, master_prompt,
        selected_feedback_str, selected_grammar_str,
        selected_validation_str, selected_editor_str
    ):

        self.ESSAY_INSTRUCTION = essay_instr
        self.ACTIVE_SYSTEM_MESSAGE = system_msg
        self.ACTIVE_GRAMMAR_PROMPT = grammar_prompt
        self.ACTIVE_MASTER_PROMPT = master_prompt

        self.SELECTED_FEEDBACK_MODEL_STR = selected_feedback_str or self.DEFAULT_FEEDBACK_MODEL_STR
        self.SELECTED_GRAMMAR_MODEL_STR = selected_grammar_str or self.DEFAULT_GRAMMAR_MODEL_STR
        self.SELECTED_VALIDATION_MODEL_STR = selected_validation_str or self.DEFAULT_VALIDATION_MODEL_STR
        self.SELECTED_EDITOR_MODEL_STR = selected_editor_str or self.DEFAULT_EDITOR_MODEL_STR


        logger.info("--- Runtime Model Selection ---")
        logger.info(f"Feedback:   {self.SELECTED_FEEDBACK_MODEL_STR}")
        logger.info(f"Grammar:    {self.SELECTED_GRAMMAR_MODEL_STR}")
        logger.info(f"Validation: {self.SELECTED_VALIDATION_MODEL_STR}")
        logger.info(f"Editor:     {self.SELECTED_EDITOR_MODEL_STR}")
        logger.info("-------------------------------")

"""### Library Installation"""

!pip install python-docx google-auth google-auth-oauthlib google-api-python-client language-tool-python
!pip install openai --upgrade
!pip install anthropic --upgrade
!pip install sentencepiece

"""### Imports and Global constants"""

# Commented out IPython magic to ensure Python compatibility.
import os
import io
import re
import hashlib
import json
import google.auth
from googleapiclient.discovery import build
import logging
import sys
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload, MediaIoBaseUpload
import docx
from docx import Document
from docx.shared import RGBColor, Pt, Inches
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from typing import Any, Dict, Optional, Tuple, List, Set
from contextlib import contextmanager
from google.colab import userdata
import language_tool_python
import gc
from openai import OpenAI
import anthropic
from pydantic import BaseModel
from datetime import date, datetime
import csv
from IPython.display import display, HTML, clear_output
import multiprocessing
import time
import traceback
import difflib
from collections import defaultdict
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    RetryError
)

from openai import RateLimitError as OpenAIRateLimitError
from openai import APIConnectionError as OpenAIAPIConnectionError
from openai import APITimeoutError as OpenAIAPITimeoutError
from openai import APIStatusError as OpenAIAPIStatusError

from anthropic import RateLimitError as AnthropicRateLimitError
from anthropic import APIConnectionError as AnthropicAPIConnectionError
from anthropic import APITimeoutError as AnthropicAPITimeoutError
from anthropic import InternalServerError as AnthropicInternalServerError

# Define exceptions that should trigger a retry attempt
RETRYABLE_EXCEPTIONS = (
    OpenAIRateLimitError,
    OpenAIAPIConnectionError,
    OpenAIAPITimeoutError,
    OpenAIAPIStatusError, # Retry on 5xx server errors
    AnthropicRateLimitError,
    AnthropicAPIConnectionError,
    AnthropicAPITimeoutError,
    AnthropicInternalServerError, # Retry on Anthropic 5xx errors
    # Add other transient network/API errors if identified, e.g.,
    # requests.exceptions.Timeout, requests.exceptions.ConnectionError if using requests directly
    # httpx.TimeoutException, httpx.NetworkError if using httpx
)

# %env PYTHONUNBUFFERED=1

"""### Model Provider Getter Helper Function"""

def get_provider_and_model(selected_model_str: str) -> Tuple[Optional[str], Optional[str]]:
    """
    Parses a string like "Provider: model_name" or "Provider:model/name-with-slashes".

    Returns:
        Tuple[Optional[str], Optional[str]]: (provider, model_name) or (None, None) if parsing fails.
        Provider is normalized to "OpenAI", "Anthropic", or "OpenRouter".
    """
    if not selected_model_str or ':' not in selected_model_str:
        logger.error(f"Invalid model string format: '{selected_model_str}'. Expected 'Provider: model_name'.")
        return None, None

    # Use regex for flexibility (handles potential extra spaces, different provider names)
    # Matches "ProviderName : Model Name/String"
    # Group 1: Provider Name, Group 2: Model Name/String
    match = re.match(r'^\s*([a-zA-Z0-9_]+)\s*:\s*(.+?)\s*$', selected_model_str)

    if not match:
        logger.error(f"Could not parse model string: '{selected_model_str}'. Using fallback regex.")
        # Fallback regex attempt if the first fails (e.g., no space after colon)
        match = re.match(r'^\s*([a-zA-Z0-9_]+):(.+?)\s*$', selected_model_str)
        if not match:
             logger.error(f"Fallback regex also failed for: '{selected_model_str}'.")
             return None, None

    provider_raw = match.group(1).strip()
    model_name = match.group(2).strip()

    # Normalize provider name
    provider_lower = provider_raw.lower()
    if provider_lower == 'openai':
        provider = 'OpenAI'
    elif provider_lower == 'anthropic':
        provider = 'Anthropic' # Match the key used internally
    elif provider_lower == 'openrouter':
        provider = 'OpenRouter'
    else:
        logger.warning(f"Unrecognized provider '{provider_raw}' in string '{selected_model_str}'. Assuming 'OpenRouter'.")
        # Defaulting to OpenRouter if unsure, as it supports various models. Adjust if needed.
        provider = 'OpenRouter'
        # OR return None, None if strict matching is required:
        # logger.error(f"Unrecognized provider: {provider_raw}")
        # return None, None

    if not model_name:
        logger.error(f"Parsed empty model name from string: '{selected_model_str}'.")
        return None, None

    # logger.debug(f"Parsed '{selected_model_str}' -> Provider: {provider}, Model: {model_name}") # Optional debug log
    return provider, model_name

"""## Main Script

### Logger
"""

class ColabLogger:
    """Logging utility for Google Colab that ensures real-time output."""

    def __init__(self, name="ColabLogger", level=logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        self.log_buffer = []

        # Stop messages from propagating to the root logger
        self.logger.propagate = False

        # Clear any *existing* handlers *on this specific logger*
        # Note: This won't remove handlers attached to the root logger
        if self.logger.hasHandlers():
             self.logger.handlers.clear()

        # Add console handler with formatting
        console_handler = logging.StreamHandler(sys.stdout)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

    def _ensure_output(self, message_with_level):
        """Adds message to buffer for status display. No longer prints directly."""
        # Keep buffer logic for potential status display
        # Store the message with level prefix as passed from info/warning/etc.
        self.log_buffer.append(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message_with_level}")
        if len(self.log_buffer) > 100:  # Keep buffer size reasonable
            self.log_buffer = self.log_buffer[-100:]

    def debug(self, message):
        self.logger.debug(message)
        # No _ensure_output for debug by default

    def info(self, message):
        self.logger.info(message)
        # Pass level prefix to buffer logic
        self._ensure_output(f"INFO - {message}")

    def warning(self, message):
        self.logger.warning(message)
        self._ensure_output(f"WARNING - {message}")

    def error(self, message):
        self.logger.error(message)
        self._ensure_output(f"ERROR - {message}")

    def critical(self, message):
        self.logger.critical(message)
        self._ensure_output(f"CRITICAL - {message}")

    def display_status(self, message, progress=None, total=None):
        """Display a status update with optional progress bar."""
       # clear_output(wait=True)

        status_html = f"<h3>{message}</h3>"

        # Add progress bar if provided
        if progress is not None and total is not None and total > 0: # Avoid division by zero
            percent = min(100, int((progress / total) * 100))
            status_html += f"""
            <div style="width:100%; background-color:#ddd; border-radius:5px;">
                <div style="width:{percent}%; background-color:#4CAF50; height:24px; border-radius:5px;
                     text-align:center; line-height:24px; color:white;">
                    {percent}% ({progress}/{total})
                </div>
            </div>
            """

        display(HTML(status_html))
        sys.stdout.flush() # Force flush for HTML display

# Initialize the logger
logger = ColabLogger()

# Test logger with forced output
logger.info("Initializing Essay Feedback System")
logger.info("Logger configured for real-time output in Colab")

def log_before_retry(retry_state):
    """
    Logs a warning message before a retry attempt using the logger
    passed into the decorated function.

    Args:
        retry_state: The tenacity RetryCallState object.
                     Provides access to function args, kwargs, attempt number, etc.
    """
    # Attempt to find the logger instance passed as an argument to the wrapped function.
    # This relies on the logger being passed either positionally or by keyword.
    logger_instance = None
    if 'logger_instance' in retry_state.kwargs:
        logger_instance = retry_state.kwargs['logger_instance']
    else:
        # Find the logger by checking positional arguments if signature is known
        # This is less robust; passing by keyword is preferred.
        # Assuming it's the last positional arg for this example if not in kwargs:
        try:
            # You might need to adjust the index based on generate_feedback's signature
            arg_names = retry_state.fn.__code__.co_varnames[:retry_state.fn.__code__.co_argcount]
            if 'logger_instance' in arg_names:
                 logger_arg_index = arg_names.index('logger_instance')
                 if logger_arg_index < len(retry_state.args):
                      logger_instance = retry_state.args[logger_arg_index]

        except Exception: # Fallback if introspection fails
             pass # logger_instance remains None

    # Log the retry attempt if we found the logger
    if logger_instance and isinstance(logger_instance, logging.Logger):
         logger_instance.warning(
             f"Retrying API call (attempt {retry_state.attempt_number}) "
             f"due to error: {retry_state.outcome.exception()}. "
             f"Waiting {retry_state.next_action.sleep:.2f}s before next attempt."
         )
    else:
         print( # Use print as a last resort if logging isn't available
             f"WARNING: Retrying API call (attempt {retry_state.attempt_number}) "
             f"due to error: {retry_state.outcome.exception()}. "
             f"Waiting {retry_state.next_action.sleep:.2f}s. (Logger instance not found in args)"
         )

"""### Environmental Secrets"""

def get_secret(secret_name: str, required: bool = True) -> Optional[str]:
    """
    Retrieve a secret from Google Colab's userdata or environment variables.

    Args:
        secret_name (str): The name of the secret to retrieve
        required (bool): If True, raises ValueError when secret isn't found
                         If False, returns None when secret isn't found

    Returns:
        Optional[str]: The secret value if found, None if not found and required=False

    Raises:
        ValueError: If the secret is not found and required=True
    """
    # First try Google Colab's userdata
    try:
        # Check if running in Colab context before using userdata
        if 'google.colab' in sys.modules:
            secret = userdata.get(secret_name)
            if secret:
                # logger.debug(f"Retrieved '{secret_name}' from Colab userdata.")
                return secret
        else:
            # logger.debug("Not in Colab, skipping userdata check.")
            pass
    except Exception as e:
        # logger.warning(f"Could not access Colab userdata for '{secret_name}': {e}")
        pass  # Silently continue to try environment variables

    # Then try environment variables
    secret = os.getenv(secret_name)
    if secret:
        # logger.debug(f"Retrieved '{secret_name}' from environment variables.")
        return secret

    # Handle case where secret wasn't found
    if required:
        error_msg = f"Secret '{secret_name}' not found in userdata or environment variables"
        logger.error(error_msg)
        raise ValueError(error_msg)
    # logger.warning(f"Optional secret '{secret_name}' not found.")
    return None

def validate_secrets() -> bool:
    """Validate that all required secrets are available."""
    # Added ANTHROPIC_API_KEY and OPEN_ROUTER_API_KEY to required secrets
    required_secrets = ['OPENAI_API_KEY', 'GOOGLE_CLOUD_PRIVATE_KEY', 'ANTHROPIC_API_KEY', 'OPEN_ROUTER_API_KEY']
    missing = []
    for secret in required_secrets:
        # Use get_secret to check both Colab userdata and environment variables
        if not get_secret(secret, required=False):
            missing.append(secret)

    if missing:
        logger.error(f"Missing required secrets: {', '.join(missing)}")
        logger.error("Please ensure these secrets are set in Colab Secrets (sidebar) or as environment variables.")
        return False
    logger.info("All required secrets are present.")
    return True

def setup_api_keys():
    """
    Retrieve required API keys and optionally set OpenAI key in environment.
    Does *not* set Anthropic or Open Router keys in the environment by default.
    """
    logger.info("Setting up API keys...")
    # Validate secrets before proceeding
    if not validate_secrets():
        logger.critical("Required secrets not found. Cannot proceed.")
        raise SystemExit("Exiting due to missing secrets.")

    # Retrieve keys (validation ensures they exist)
    openai_key = get_secret('OPENAI_API_KEY')
    anthropic_key = get_secret('ANTHROPIC_API_KEY')
    open_router_key = get_secret('OPEN_ROUTER_API_KEY')
    google_key = get_secret('GOOGLE_CLOUD_PRIVATE_KEY')

    # Set OpenAI key in environment for libraries that expect it
    try:
        if openai_key:
            os.environ["OPENAI_API_KEY"] = openai_key
            logger.info("OpenAI API key successfully set in environment variable.")
        else:
            # This case should technically be caught by validate_secrets, but included for robustness
            logger.error("OpenAI API key retrieved as None/empty despite validation passing. Check secret storage.")
            raise ValueError("OpenAI API Key is missing.")

    except Exception as e:
        logger.error(f"Failed to set OpenAI API key in environment: {e}")
        raise SystemExit("Exiting due to OpenAI key setup failure.")

    # or API calls. Just confirm they were retrieved.
    if anthropic_key:
        logger.info("Anthropic API key retrieved.")
    if open_router_key:
        logger.info("Open Router API key retrieved.")
    if google_key:
        logger.info("Google Cloud key retrieved.")

# Call this function early in the script execution, e.g., in the Entry Point
# setup_api_keys() # Moved the call to the entry point cell

print("Secrets validation and setup functions defined.")

"""### Basic I/O"""

class Essay:
    """
    Represents a student essay with its metadata and different processed versions.
    """
    def __init__(self, original_text: str, filename: str = None):
        # Content versions
        self.original_text = original_text
        self.corrected_text = original_text  # Starts as original, updated after grammar check
        self.edited_text = None  # AI-edited version with improvements

        # Metadata
        self.student_name = None
        self.student_email = None
        self.word_count = None
        self.filename = filename

        # Analysis data
        self.grammar_issues = []
        self.has_grammar_processed = False
        self.has_feedback_processed = False

        # Extract metadata from content if available
        if original_text and len(original_text.strip()) > 0:
            self.process_metadata()

    def process_metadata(self):
        """Extract student info and prepare essay content."""
        clean_text, name, email, count = preprocess_essay_content(
            self.original_text, self.filename)

        # Update properties
        self.student_name = name
        self.student_email = email
        self.word_count = count
        self.original_text = clean_text  # Use cleaned text as original
        self.corrected_text = clean_text  # Initialize corrected version

def validate_file(file_metadata: dict, file_name: str, config: Config) -> bool:
    """
    Validate if a file meets processing requirements.

    Args:
        file_metadata: File metadata from Google Drive
        file_name: Name of the file
        config: Configuration object

    Returns:
        bool: True if file is valid, False otherwise
    """
    # Check file type
    mime_type = file_metadata.get('mimeType')
    if mime_type not in config.ALLOWED_MIME_TYPES:
        logger.warning(f"Wrong file type for {file_name}. Skipping.")
        return False

    # Check file size
    file_size = int(file_metadata.get('size', 0))
    max_size = config.MAX_FILE_SIZE_MB * 1024 * 1024
    if file_size > max_size:
        logger.warning(f"File {file_name} exceeds size limit. Skipping.")
        return False

    return True

def download_file(service, file_id):
    """Download a file from Google Drive."""
    try:
        request = service.files().get_media(fileId=file_id)
        file_io = io.BytesIO()
        downloader = MediaIoBaseDownload(file_io, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
            logger.info(f"Download status: {int(status.progress() * 100)}%")
        file_io.seek(0)
        return file_io
    except Exception as e:
        logger.error(f"Error downloading file ID {file_id}: {e}")
        raise

def read_docx_content(file_io) -> str:
    """
    Extract text content from a DOCX file.

    Args:
        file_io: File IO object containing DOCX data

    Returns:
        str: Extracted text content
    """
    try:
        doc = docx.Document(file_io)
        paragraphs = []
        for para in doc.paragraphs:
            if not para.text.strip():
                continue
            text = para.text.strip()
            logger.debug(f"Processing paragraph: '{text[:50]}...'")
            paragraphs.append(text)
        full_text = '\n\n'.join(paragraphs)  # Join with double newlines
        return full_text
    except Exception as e:
        logger.error(f"Error reading DOCX content: {e}")
        raise

def preprocess_essay_content(raw_text: str, filename: str = None) -> Tuple[str, str, Optional[str], Optional[int]]:
    """
    Preprocess essay content to extract student info and clean the text.

    Args:
        raw_text: The raw text from the document
        filename: The original filename (used as fallback for student name)

    Returns:
        Tuple containing:
        - Cleaned essay text
        - Student name
        - Student email (or None)
        - Word count (or None)
    """
    # Normalize newlines and remove triple+ newlines
    normalized_text = re.sub(r'\n{3,}', '\n\n', raw_text)

    # Split into paragraphs
    paragraphs = [p.strip() for p in normalized_text.split('\n\n')]
    non_empty_paragraphs = [p for p in paragraphs if p]

    logger.debug(f"Found {len(non_empty_paragraphs)} non-empty paragraphs")

    student_name = None
    student_email = None
    word_count = None

    # Skip processing if fewer than 5 paragraphs (4 header + at least 1 content)
    if len(non_empty_paragraphs) < 5:
        logger.warning(f"Essay has fewer than 5 paragraphs, may be missing header or content")
        if filename:
            student_name = extract_student_name_from_filename(filename)
        return raw_text, student_name, None, None

    # Check the first 4 non-empty paragraphs for header info
    header_paragraphs = non_empty_paragraphs[:4]

    # Extract information from header paragraphs, stripping asterisks and formatting
    clean_paragraphs = [re.sub(r'\*\*|\*', '', para).strip() for para in header_paragraphs]

    # Try to extract student name from first paragraph
    if clean_paragraphs[0]:
        name_match = re.match(r'^(.*?)(?=\d|$)', clean_paragraphs[0])
        if name_match and name_match.group(1).strip():
            student_name = name_match.group(1).strip()
            logger.debug(f"Extracted student name from header: {student_name}")

    # Look for email in any of the header paragraphs
    for i, para in enumerate(clean_paragraphs):
        email_match = re.search(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)', para)
        if email_match:
            student_email = email_match.group(1)
            logger.debug(f"Found email in paragraph {i+1}: {student_email}")
            break

    # Look for word count in any of the header paragraphs
    for i, para in enumerate(clean_paragraphs):
        word_count_match = re.search(r'(?:Antal ord|Number of words):\s*(\d+)', para, re.IGNORECASE)
        if word_count_match:
            word_count = int(word_count_match.group(1))
            logger.debug(f"Found word count in paragraph {i+1}: {word_count}")
            break

    # Join the remaining paragraphs for essay content
    essay_content = non_empty_paragraphs[4:]
    essay_text = '\n\n'.join(essay_content)

    # Fallback to filename for student name if needed
    if not student_name and filename:
        student_name = extract_student_name_from_filename(filename)
        logger.debug(f"Using filename-based student name: {student_name}")

    # Capitalize name if present
    if student_name:
        student_name = capitalize_name(student_name)

    logger.info(f"Preprocessing results: Name: {student_name}, Email: {student_email}, Word count: {word_count}")

    return essay_text, student_name, student_email, word_count

def extract_student_name_from_filename(filename: str) -> Optional[str]:
    """Extract student name from filename as fallback method."""
    if not filename:
        return None

    filename = os.path.splitext(filename)[0]
    pattern = r'^(\S+)\s+(\S+)'
    match = re.match(pattern, filename)
    if match:
        student_name = f"{match.group(1)} {match.group(2)}"
        return student_name
    return filename  # Just return the filename as is if pattern doesn't match

def capitalize_name(name: str) -> str:
    """Capitalize each word in a name."""
    if not name:
        return ""
    return ' '.join(word.capitalize() for word in name.split())

def upload_file(service, folder_id, doc, file_name):
    """Upload a document to Google Drive."""
    try:
        file_io = io.BytesIO()
        doc.save(file_io)
        file_io.seek(0)
        file_metadata = {
            'name': file_name,
            'parents': [folder_id],
            'mimeType': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        }
        media = MediaIoBaseUpload(
            file_io,
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )
        file = service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id'
        ).execute()
        logger.info(f"File uploaded with ID: {file.get('id')}")
        return file.get('id')
    except Exception as e:
        logger.error(f"Error uploading file {file_name}: {e}")
        raise



"""### Grammar and Spell Correction + Grammar Analysis Logic"""

class GrammarStep(BaseModel):
    """Represents a single grammar issue with its context and correction."""
    original_error: str
    context: str
    explanation: str
    correction: str
    rule_category: str
    gpt_action: Optional[str] = None

class GrammarFeedback(BaseModel):
    """Collection of grammar issues with the final corrected text."""
    steps: List[GrammarStep]
    final_suggestion: str

def get_issue_context(text: str, offset: int, length: int, window: int = 100) -> str:
    """
    Get a larger context window around the issue for better analysis.

    Args:
        text: The full text containing the error
        offset: The starting position of the error
        length: The length of the error text
        window: The number of characters to include before and after the error

    Returns:
        String with the extracted context with the error highlighted
    """
    start = max(0, offset - window)
    end = min(len(text), offset + length + window)

    context = text[start:end]

    if start > 0:
        context = f"...{context}"
    if end < len(text):
        context = f"{context}..."

    # Highlight the error in the context
    highlighted_error = text[offset:offset + length]
    context = context.replace(highlighted_error, f"**{highlighted_error}**")

    return context

def is_trivial_whitespace_error(error_detail: dict) -> bool:
    """Determine if an error is just a trivial whitespace issue."""
    message = error_detail.get('message', '').lower()
    error_text = error_detail.get('error_text', '').strip()
    if (not error_text and "whitespace" in message) or \
       ("extra space" in message) or \
       ("whitespace" in message and not error_text):
        return True
    return False

def format_language_tool_issues(text: str, matches, truncate_window: int = 50) -> List[Dict[str, Any]]:
    """Format LanguageTool issues with full and truncated context."""
    return [
        {
            "message": match.message,
            "error_text": text[match.offset:match.offset + match.errorLength],
            "full_context": get_issue_context(text, match.offset, match.errorLength, window=100),
            "truncated_context": get_issue_context(text, match.offset, match.errorLength, window=truncate_window),
            "suggestions": match.replacements,
            "rule_id": match.ruleId,
            "category": match.category
        }
        for match in matches
    ]

def apply_corrections(text: str, matches) -> str:
    """
    Apply LanguageTool corrections to text in reverse offset order.

    Args:
        text: The original text to correct
        matches: List of LanguageTool matches

    Returns:
        The corrected text
    """
    corrected = text
    # Sort matches by offset in reverse order to maintain offsets
    for match in sorted(matches, key=lambda x: x.offset, reverse=True):
        if not match.replacements:
            continue
        start = match.offset
        end = start + match.errorLength
        corrected = corrected[:start] + match.replacements[0] + corrected[end:]
    return corrected


def find_natural_break(text: str, start_pos: int, max_len: int) -> int:
    """
    Finds the nearest sentence or paragraph break at or before start_pos + max_len.
    Prefers paragraph breaks (\n\n) then sentence breaks (.!?). Helper for chunk_text.
    """
    target_end = min(start_pos + max_len, len(text))
    # Search area ends at target_end, BUT we search from start_pos within the original text
    search_area_text = text[start_pos:target_end]

    # Look for paragraph breaks first (\n\n) within the search text, searching backwards
    para_break_rel = search_area_text.rfind('\n\n')
    if para_break_rel != -1:
       # Return position *after* the double newline in the *original* text
       # Ensure it's a valid break *after* the start_pos
       absolute_break_pos = start_pos + para_break_rel + 2
       if absolute_break_pos > start_pos:
          return absolute_break_pos

    # Look for sentence breaks next (.!?) followed by space, searching backwards
    # We search the reversed search text to find the *last* occurrence easily
    sentence_break_match = re.search(r'\s+[.!?]', search_area_text[::-1])
    if sentence_break_match:
        # Calculate position from the start of the *original* text
        # match.start(0) gives position from the end of the reversed string
        # length of search_area_text - match.start(0) gives position from start of search_area_text
        # Add start_pos to get absolute position. Find the position *after* the punctuation.
        break_pos_rel = (len(search_area_text) - sentence_break_match.start(0)) -1 # Position of punctuation
        absolute_break_pos = start_pos + break_pos_rel + 1 # Position *after* punctuation
        # Ensure it's a valid break *after* the start_pos
        if absolute_break_pos > start_pos:
           # Skip potential leading spaces after the break for the next chunk start
           while absolute_break_pos < len(text) and text[absolute_break_pos].isspace():
                 absolute_break_pos += 1
           return absolute_break_pos


    # If no natural break found within the range, return the target end
    # Ensure we don't split mid-word if possible
    final_target_end = target_end
    if final_target_end < len(text) and text[final_target_end].isalnum() and final_target_end > start_pos and text[final_target_end - 1].isalnum():
       # Try to backtrack to the nearest space if we are splitting mid-word
       last_space = text.rfind(' ', start_pos, final_target_end)
       if last_space != -1 and last_space > start_pos:
          final_target_end = last_space + 1 # Start next chunk after space

    return final_target_end


def chunk_text(text: str, max_length: int, token_window: int) -> List[Tuple[int, str]]:
    """
    Splits text into chunks, each with marked context windows.
    Reuses MAX_CHUNK_SIZE (for max_length) and TOKEN_WINDOW (for context size).

    Args:
        text: The full text to chunk.
        max_length: The target maximum size for the main analysis chunk (from MAX_CHUNK_SIZE).
        token_window: Number of characters for context before/after (from TOKEN_WINDOW).

    Returns:
        List of tuples: (chunk_index, formatted_chunk_string)
                       where formatted_chunk_string includes markers.
    """
    chunks = []
    current_pos = 0
    chunk_index = 0
    text_len = len(text)

    while current_pos < text_len:
        chunk_index += 1
        # Determine the end of the core chunk using the helper
        core_end = find_natural_break(text, current_pos, max_length) # Use max_length here

        # Extract core chunk
        core_chunk = text[current_pos:core_end].strip()

        # Handle potential edge case where find_natural_break returns current_pos or less
        if core_end <= current_pos:
             # Force advance if stuck, e.g. take max_length or remaining text
           core_end = min(current_pos + max_length, text_len)
           core_chunk = text[current_pos:core_end].strip()
           if not core_chunk and current_pos < text_len:
              # If still empty and not at end, likely non-printable chars, advance anyway
              core_end = current_pos + 1
              current_pos = core_end
              continue # Skip this empty iteration


        if not core_chunk and current_pos >= text_len:
           break # Reached end

        if not core_chunk: # Skip empty core chunks if break logic leads to them
           current_pos = core_end
           continue


        # Extract context before using token_window
        context_before_start = max(0, current_pos - token_window) # Use token_window
        context_before = text[context_before_start:current_pos].strip()

        # Extract context after using token_window
        context_after_end = min(text_len, core_end + token_window) # Use token_window
        context_after = text[core_end:context_after_end].strip()

        # Format the chunk with markers
        formatted_chunk = ""
        if context_before:
           formatted_chunk += f"[CONTEXT_BEFORE]\n{context_before}\n[/CONTEXT_BEFORE]\n\n"

        formatted_chunk += f"[CHUNK_TO_ANALYZE]\n{core_chunk}\n[/CHUNK_TO_ANALYZE]"

        if context_after:
           formatted_chunk += f"\n\n[CONTEXT_AFTER]\n{context_after}\n[/CONTEXT_AFTER]"

        chunks.append((chunk_index, formatted_chunk))

        # Move to the next position
        current_pos = core_end

    return chunks

def analyze_text(text: str, language_tool, openai_client, logger) -> Optional[GrammarFeedback]:
    """
    Perform grammar analysis using LanguageTool with GPT validation.

    Args:
        text: The text to analyze
        language_tool: LanguageTool instance
        openai_client: OpenAI client instance
        logger: Logger instance

    Returns:
        GrammarFeedback or None if analysis fails
    """
    logger.info("Starting grammar analysis")
    logger.info(f"Text length: {len(text)} characters")

    matches = []

    try:
        # --- ADD THIS LOGGING LINE ---
      if hasattr(language_tool, 'custom_words'):
        logger.info(f"LanguageTool custom words loaded before check: {language_tool.custom_words}")
      else:
        logger.warning("LanguageTool instance seems to lack custom_words attribute before check.")
        # --- END OF ADDED LOGGING ---

        logger.info("Running LanguageTool checks...")
        matches = language_tool.check(text)
        logger.info(f"LanguageTool found {len(matches)} potential issues")
    except Exception as e:
        logger.error(f"LanguageTool check failed: {e}")
        # Return fallback, matches remains [] if error occurred before assignment
        return GrammarFeedback(
            steps=[],
            final_suggestion=text
        )

    # Filter out trivial whitespace errors
    filtered_matches = []
    for match in matches:
        error_detail = {
            "message": match.message,
            "error_text": text[match.offset:match.offset + match.errorLength]
        }
        if not is_trivial_whitespace_error(error_detail):
            filtered_matches.append(match)

    logger.info(f"After filtering, {len(filtered_matches)} significant issues remain")

    # Debug the found issues
    for i, match in enumerate(filtered_matches[:3]):  # Log first 3 issues for debugging
        error_text = text[match.offset:match.offset + match.errorLength]
        suggestion = match.replacements[0] if match.replacements else "NO SUGGESTION"
        logger.info(f"Issue {i+1}: '{error_text}' -> '{suggestion}' ({match.message})")

    # If no significant issues found, return original text as final_suggestion
    if not filtered_matches:
        logger.info("No significant grammar issues detected")
        return GrammarFeedback(
            steps=[],
            final_suggestion=text  # same as original
        )

    # Apply corrections directly with LanguageTool
    logger.info("Applying LanguageTool corrections...")
    corrected_text = apply_corrections(text, filtered_matches)

    # Create steps for the grammar feedback AS DICTIONARIES
    steps_data = [] # List to hold step dictionaries
    for match in filtered_matches:
        error_text = text[match.offset:match.offset + match.errorLength]
        context = get_issue_context(text, match.offset, match.errorLength)
        # Ensure correction is always a string, even if empty
        correction = match.replacements[0] if match.replacements else ""
        # Ensure category is a string
        category = str(match.category) if match.category else "UNKNOWN"
        # Ensure message is a string
        explanation = str(match.message) if match.message else "No explanation provided."


        # Create a dictionary with keys matching GrammarStep fields
        step_dict = {
            "original_error": error_text,
            "context": context,
            "explanation": explanation, # Use sanitized explanation
            "correction": correction,   # Use sanitized correction
            "rule_category": category,  # Use sanitized category
            "gpt_action": "validated",
            # DO NOT include complex objects like match or match.__dict__ here
        }
        steps_data.append(step_dict) # Add the dictionary

    logger.info(f"Created {len(steps_data)} grammar step dictionaries")

    # Create and return the grammar feedback object
    # Pydantic will parse the list of dictionaries into GrammarStep objects
    try:
        feedback = GrammarFeedback(
            steps=steps_data, # Pass the list of dictionaries
            final_suggestion=corrected_text
        )
        logger.info("Grammar analysis completed successfully")
        return feedback
    except Exception as pydantic_error:
        # Add specific logging for Pydantic errors if they persist
        logger.error(f"Pydantic validation failed during GrammarFeedback creation: {pydantic_error}")
        logger.error(traceback.format_exc()) # Log the full traceback
        # Return a fallback object or None to prevent downstream errors
        return GrammarFeedback(steps=[], final_suggestion=text) # Fallback with original text


import json # Ensure json is imported

def reiterate_analysis(
    text_chunk: str,          # The formatted chunk with context markers
    config: Config,           # Pass full config for prompts
    provider: str,            # Determined provider
    model_name: str,          # Determined model name
    client: Any,              # Initialized client for the provider
    logger,                   # Logger instance
    chunk_id: int = None      # Chunk ID for logging
    ) -> Optional[GrammarFeedback]: # Returns GrammarFeedback or None
    """
    Perform second pass grammar/style analysis on a text chunk using the specified LLM.
    """
    log_prefix = f"[Chunk {chunk_id}] " if chunk_id is not None else ""
    logger.info(f"{log_prefix}Starting second pass grammar analysis using Provider: {provider}, Model: {model_name}")

    # --- Input Validation ---
    if not client:
        logger.error(f"{log_prefix}LLM client for provider '{provider}' is not available.")
        return None
    if not config.ACTIVE_GRAMMAR_PROMPT:
        logger.error(f"{log_prefix}ACTIVE_GRAMMAR_PROMPT is missing in config.")
        return None
    if not model_name:
         logger.error(f"{log_prefix}No grammar model name provided for provider {provider}.")
         return None

    # --- Prepare Prompt ---
    chunk_id_value = chunk_id if chunk_id is not None else 1
    try:
        # Use the grammar prompt selected based on course/language
        formatted_prompt = config.ACTIVE_GRAMMAR_PROMPT.format(
            chunk_id=chunk_id_value,
            text_chunk=text_chunk # Pass the chunk including context markers
        )
    except KeyError as e:
        logger.error(f"{log_prefix}Error formatting grammar prompt. Missing key: {e}.")
        return None
    except Exception as e:
        logger.error(f"{log_prefix}Unexpected error formatting prompt: {e}")
        return None

    # --- API Call based on Provider ---
    response_text = None
    try:
        system_content_grammar = "You are a grammar assistant performing analysis based on the provided instructions and text chunk."

        if provider == "OpenAI":
            if not isinstance(client, OpenAI): raise TypeError("Invalid client type for OpenAI")
            messages = [{"role": "system", "content": system_content_grammar}, {"role": "user", "content": formatted_prompt}]
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.1,
                response_format={"type": "json_object"}, # Expect JSON
                max_tokens=2000
            )
            if response.choices: response_text = response.choices[0].message.content

        elif provider == "Anthropic":
            if not isinstance(client, anthropic.Anthropic): raise TypeError("Invalid client type for Anthropic")
            # Anthropic doesn't have a guaranteed JSON mode like OpenAI, prompt needs to be very clear.
            # The prompt already asks for JSON, hope for the best.
            response = client.messages.create(
                model=model_name,
                system=system_content_grammar,
                messages=[{"role": "user", "content": formatted_prompt}],
                temperature=0.1,
                max_tokens=2000
            )
            if response.content and isinstance(response.content[0], anthropic.types.TextBlock):
                # Attempt to extract JSON from the response text
                raw_output = response.content[0].text.strip()
                # Basic extraction: find first '{' and last '}'
                json_start = raw_output.find('{')
                json_end = raw_output.rfind('}')
                if json_start != -1 and json_end != -1 and json_end > json_start:
                    response_text = raw_output[json_start:json_end+1]
                else:
                    logger.warning(f"{log_prefix}Could not extract JSON from Anthropic response. Raw: {raw_output[:200]}...")
                    response_text = None # Failed to get JSON

        elif provider == "OpenRouter":
            if not isinstance(client, OpenAI): raise TypeError("Invalid client type for OpenRouter")
            messages = [{"role": "system", "content": system_content_grammar}, {"role": "user", "content": formatted_prompt}]

            # Check if the model is likely a Google model - avoid forcing response_format if so.
            # Rely on prompting for JSON for Google models via OpenRouter.
            is_google_model = "google/" in model_name.lower() or "gemini" in model_name.lower() or "gemma" in model_name.lower()

            completion_kwargs = {
                "model": model_name,
                "messages": messages,
                "temperature": 0.1,
                "max_tokens": 2000
            }

            if not is_google_model:
                # Apply JSON mode for non-Google models
                logger.info(f"{log_prefix}Applying response_format=json_object for OpenRouter model: {model_name}")
                completion_kwargs["response_format"] = {"type": "json_object"}
            else:
                # Rely on prompting for Google models via OpenRouter
                logger.info(f"{log_prefix}Skipping response_format for likely Google model via OpenRouter: {model_name}. Relying on prompt.")
            # --- MODIFICATION END ---

            response = client.chat.completions.create(**completion_kwargs) # Use updated kwargs
            if response.choices: response_text = response.choices[0].message.content

        else:
            logger.error(f"{log_prefix}Unsupported grammar provider: {provider}")
            return None

    except Exception as e_api:
        # ... (keep existing error handling) ...
        logger.error(f"{log_prefix}API Error during grammar analysis with {provider} ({model_name}): {e_api}")
        logger.error(traceback.format_exc())
        return None

    # --- Process Response ---
    if not response_text:
        logger.warning(f"{log_prefix}Grammar analysis with {provider} ({model_name}) returned empty content.")
        # Return empty feedback obj, not None, to indicate processing finished without finding issues/errors
        return GrammarFeedback(steps=[], final_suggestion="Analysis returned empty content")

    # --- NEW v3: Clean the response text (Fence Removal + Basic Check) ---
    cleaned_json_text = response_text.strip()
    logger.debug(f"{log_prefix}Raw response: '{cleaned_json_text}'") # Log raw response

    # 1. Remove known code fences (more robustly)
    if cleaned_json_text.startswith("```json"):
        cleaned_json_text = cleaned_json_text[len("```json"):].strip()
    elif cleaned_json_text.startswith("```"):
        cleaned_json_text = cleaned_json_text[len("```"):].strip()

    if cleaned_json_text.endswith("```"):
        cleaned_json_text = cleaned_json_text[:-len("```")].strip()

    # 2. Check if it looks like JSON after cleaning fences
    if not (cleaned_json_text.startswith('{') and cleaned_json_text.endswith('}')) and \
      not (cleaned_json_text.startswith('[') and cleaned_json_text.endswith(']')):
        # If it doesn't look like JSON after removing fences, something else might be wrong
        logger.warning(f"{log_prefix}Response after cleaning fences doesn't look like JSON object/array: '{cleaned_json_text[:200]}...'")
        # Still attempt parsing below, but log warning. Could add stricter error handling here if needed.

    logger.info(f"{log_prefix}Attempting JSON parse on cleaned text (after fence removal)...")


    try:
        logger.info(f"{log_prefix}Attempting JSON parse on cleaned/extracted text...")
        gpt_feedback = json.loads(cleaned_json_text)

        # --- Create GrammarStep dictionaries ---
        steps_data = []
        if "steps" in gpt_feedback and gpt_feedback["steps"]:
             # ... (Loop through steps_data_from_json - same as before) ...
             for step_data_from_json in gpt_feedback["steps"]:
                 required_keys = ["original_error", "explanation", "correction", "rule_category"]
                 if not all(k in step_data_from_json for k in required_keys):
                     logger.warning(f"{log_prefix}Skipping step due to missing keys: {step_data_from_json}")
                     continue
                 context_text = step_data_from_json.get("context", f"...{step_data_from_json['original_error']}...")
                 chunk_id_value = chunk_id if chunk_id is not None else 1
                 full_context = f"Chunk {chunk_id_value}: {context_text}"
                 step_dict = {
                     "original_error": step_data_from_json["original_error"], "context": full_context,
                     "explanation": step_data_from_json["explanation"], "correction": step_data_from_json["correction"],
                     "rule_category": step_data_from_json["rule_category"], "gpt_action": "new_issue"
                 }
                 steps_data.append(step_dict)
        else:
             logger.info(f"{log_prefix}No 'steps' found in parsed JSON.")
             return GrammarFeedback(steps=[], final_suggestion="No issues identified by LLM")

        if not steps_data:
            logger.info(f"{log_prefix}No valid issues found after processing JSON steps.")
            return GrammarFeedback(steps=[], final_suggestion="No valid issues found")

        feedback_obj = GrammarFeedback(
             steps=steps_data,
             final_suggestion="Second pass analysis results"
        )
        logger.info(f"{log_prefix}Second pass completed with {len(feedback_obj.steps)} suggestions using {provider} ({model_name}).")
        return feedback_obj

    except json.JSONDecodeError as e_json:
        logger.error(f"{log_prefix}JSON parsing error in second pass ({provider}, {model_name}): {e_json}")
        logger.error(f"Cleaned text causing error: {cleaned_json_text}") # Log cleaned text
        return None
    except Exception as e_proc:
        logger.error(f"{log_prefix}Error processing grammar response ({provider}, {model_name}): {e_proc}")
        logger.error(traceback.format_exc())
        return None


def validate_grammar_issues_batch(
    grammar_issues: List[GrammarStep], # Input list of issues
    config: Config,            # Pass config (unused directly, but good practice)
    provider: str,             # Determined provider
    model_name: str,           # Determined model name
    client: Any,               # Initialized client for the provider
    logger,                    # Logger instance
    batch_size: int = 5        # Batch size
    ) -> List[GrammarStep]:    # Return validated list
    """
    Validerar grammatikproblem i batchar med den specificerade LLM:en. Svensk version.
    """
    if not grammar_issues: return []
    logger.info(f"Startar SV validering av {len(grammar_issues)} problem med Provider: {provider}, Model: {model_name}")

    # --- Input Validation ---
    if not client or not model_name: # Combined validation check
        logger.error(f"SV Validering avbruten: Klient eller Modellnamn saknas för provider '{provider}'.")
        return grammar_issues # Returnera originallistan

    validated_issues = []
    # System prompt på svenska
    system_content_validation = "Du är en språkexpert som validerar potentiella grammatik- och stilfel. Var strikt med vad som räknas som faktiska, objektiva språkfel baserat på svensk standardgrammatik. Ignorera stilpreferenser."

    # Processa i batchar
    for i in range(0, len(grammar_issues), batch_size):
        batch = grammar_issues[i:i+batch_size]
        batch_validated = False
        log_prefix_val = f"SV Validation Batch {i}: " # Prefix for logs in this batch

        # Konstruera valideringsprompt för batchen (på svenska)
        issues_text = "\n\n".join([
            f"PROBLEM {j+1}:\n" +
            f"Ursprunglig text: \"{issue.original_error}\"\n" +
            f"Föreslagen korrigering: \"{issue.correction}\"\n" +
            f"Förklaring: \"{issue.explanation}\"\n" +
            f"Kategori: {issue.rule_category}"
            for j, issue in enumerate(batch)
        ])

        # Använd den svenska prompten
        validation_prompt = f"""
Granska följande potentiella språkproblem och avgör vilka som verkligen är språkfel (och inte bara stilpreferenser):

{issues_text}

För VARJE problem, svara:
1. Är det verkligen ett objektivt språkfel (inte bara en stilpreferens eller petitess)? Svara JA eller NEJ.
2. Om JA, ge en tydlig förklaring (max 20 ord). Om NEJ, kan förklaringen vara tom.

Formatera ditt svar ENDAST som ett JSON-objekt med en enda nyckel "validations",
vilken är en array av objekt, där varje objekt innehåller "issue_number" (int), "is_valid" (boolean),
och "explanation" (string, använd originalet om bra, förbättra vid behov, tom om ej giltig). Exempel:
{{
  "validations": [
    {{ "issue_number": 1, "is_valid": true, "explanation": "Felaktig subjektskongruens." }},
    {{ "issue_number": 2, "is_valid": false, "explanation": "" }}
  ]
}}
"""
        response_text = None
        # --- API-anrop baserat på Provider ---
        try:
            # (API call logic for OpenAI, Anthropic, OpenRouter remains the same as in the English version)
            if provider == "OpenAI":
                 if not isinstance(client, OpenAI): raise TypeError("Ogiltig klienttyp för OpenAI")
                 messages = [{"role": "system", "content": system_content_validation}, {"role": "user", "content": validation_prompt}]
                 response = client.chat.completions.create(model=model_name, messages=messages, temperature=0.1, response_format={"type": "json_object"}, max_tokens=1000)
                 if response.choices: response_text = response.choices[0].message.content
            elif provider == "Anthropic":
                 if not isinstance(client, anthropic.Anthropic): raise TypeError("Ogiltig klienttyp för Anthropic")
                 response = client.messages.create(model=model_name, system=system_content_validation, messages=[{"role": "user", "content": validation_prompt}], temperature=0.1, max_tokens=1000)
                 if response.content and isinstance(response.content[0], anthropic.types.TextBlock): # ... extract basic {} block ...
                    raw_output = response.content[0].text.strip(); json_start = raw_output.find('{'); json_end = raw_output.rfind('}')
                    if json_start != -1 and json_end != -1: response_text = raw_output[json_start:json_end+1]
                    else: logger.warning(f"{log_prefix_val}Kunde ej extrahera JSON block från Anthropic.")
                 else: logger.warning(f"{log_prefix_val}Anthropic svar ej textblock.")
            elif provider == "OpenRouter":
                 if not isinstance(client, OpenAI): raise TypeError("Ogiltig klienttyp för OpenRouter")
                 messages = [{"role": "system", "content": system_content_validation}, {"role": "user", "content": validation_prompt}]

                 # Check if the model is likely a Google model - avoid forcing response_format if so.
                 is_google_model = "google/" in model_name.lower() or "gemini" in model_name.lower() or "gemma" in model_name.lower()

                 completion_kwargs = {
                     "model": model_name,
                     "messages": messages,
                     "temperature": 0.1,
                     "max_tokens": 1000
                 }

                 if not is_google_model:
                     # Apply JSON mode for non-Google models
                     logger.info(f"{log_prefix_val}Applying response_format=json_object for OpenRouter model: {model_name}")
                     completion_kwargs["response_format"] = {"type": "json_object"}
                 else:
                     # Rely on prompting for Google models via OpenRouter
                     logger.info(f"{log_prefix_val}Skipping response_format for likely Google model via OpenRouter: {model_name}. Relying on prompt.")

                 response = client.chat.completions.create(**completion_kwargs) # Use updated kwargs
                 if response.choices: response_text = response.choices[0].message.content
            else:
                logger.error(f"SV Validering: Okänd provider: {provider}")
                validated_issues.extend(batch); continue


            # --- Process Response ---
            if response_text:

                cleaned_json_text = response_text.strip()
                logger.debug(f"{log_prefix_val}Raw response: '{cleaned_json_text}'")

                if cleaned_json_text.startswith("```json"):
                    cleaned_json_text = cleaned_json_text[len("```json"):].strip()
                elif cleaned_json_text.startswith("```"):
                    cleaned_json_text = cleaned_json_text[len("```"):].strip()

                if cleaned_json_text.endswith("```"):
                    cleaned_json_text = cleaned_json_text[:-len("```")].strip()

                if not (cleaned_json_text.startswith('{') and cleaned_json_text.endswith('}')) and \
                   not (cleaned_json_text.startswith('[') and cleaned_json_text.endswith(']')):
                    logger.warning(f"{log_prefix_val}Response after cleaning fences doesn't look like JSON: '{cleaned_json_text[:200]}...'")

                logger.info(f"{log_prefix_val}Attempting JSON parse on cleaned text...")

                try:
                    # Use cleaned_json_text for parsing
                    validation_results = json.loads(cleaned_json_text)
                    # Process results map and loop through batch (same logic as before)
                    results_map = {val.get("issue_number"): val for val in validation_results.get("validations", [])}
                    for j, issue in enumerate(batch):
                        issue_num = j + 1
                        val_result = results_map.get(issue_num)
                        if val_result and val_result.get("is_valid", False):
                             validated_issue = GrammarStep( # ... create validated_issue ... )
                                 original_error=issue.original_error, context=issue.context,
                                 explanation=val_result.get("explanation", "").strip() or issue.explanation,
                                 correction=issue.correction, rule_category=issue.rule_category,
                                 gpt_action=issue.gpt_action
                             )
                             validated_issues.append(validated_issue)
                             batch_validated = True
                        else:
                             logger.info(f"{log_prefix_val}Invaliderat problem #{issue_num}: '{issue.original_error}'")
                             if val_result: batch_validated = True

                    if not batch_validated and len(batch)>0:
                         logger.warning(f"{log_prefix_val}JSON tolkad men inga problem processade.")
                         validated_issues.extend(batch) # Fallback

                except json.JSONDecodeError as e_json:
                    logger.error(f"{log_prefix_val}JSON-tolkningsfel ({provider}, {model_name}): {e_json}")
                    logger.error(f"Rensad text som orsakade felet: {cleaned_json_text}") # Log cleaned text
                    validated_issues.extend(batch) # Fallback
                except Exception as e_proc:
                    logger.error(f"{log_prefix_val}Fel vid bearbetning ({provider}, {model_name}): {e_proc}", exc_info=True)
                    validated_issues.extend(batch) # Fallback
            else:
                logger.warning(f"{log_prefix_val}Fick tomt svar från {provider} ({model_name}).")
                validated_issues.extend(batch) # Fallback

        except Exception as e_api:
            logger.error(f"{log_prefix_val}API-fel ({provider}, {model_name}): {e_api}", exc_info=True)
            validated_issues.extend(batch) # Fallback

    logger.info(f"SV Batch-validering klar med {provider} ({model_name}): {len(validated_issues)} problem återstår av {len(grammar_issues)}.")
    return validated_issues


def validate_grammar_issues_batch_en(
    grammar_issues: List[GrammarStep], # Input list of issues
    config: Config,            # Pass config (unused directly, but good practice)
    provider: str,             # Determined provider
    model_name: str,           # Determined model name
    client: Any,               # Initialized client for the provider
    logger,                    # Logger instance
    batch_size: int = 5        # Batch size
    ) -> List[GrammarStep]:    # Return validated list
    """
    Validates grammar issues in batches using the specified LLM. English version.
    """
    if not grammar_issues: return []
    logger.info(f"Starting EN validation of {len(grammar_issues)} issues using Provider: {provider}, Model: {model_name}")

    # --- Input Validation ---
    if not client:
        logger.error(f"EN Validation skipped: LLM client for provider '{provider}' is not available.")
        return grammar_issues # Return original list if validation client fails
    if not model_name:
        logger.error(f"EN Validation skipped: No validation model name provided for provider {provider}.")
        return grammar_issues

    validated_issues = []
    system_content_validation = "You are a language expert validating potential grammar and style issues. Be strict about what counts as actual, objective language errors based on standard English grammar. Ignore stylistic preferences."

    for i in range(0, len(grammar_issues), batch_size):
        batch = grammar_issues[i:i+batch_size]
        batch_validated = False
        log_prefix_val = f"EN Validation Batch {i}: " # Prefix for logs

        # Construct validation prompt for the batch
        issues_text = "\n\n".join([
            f"ISSUE {j+1}:\n" +
            f"Original text: \"{issue.original_error}\"\n" +
            f"Suggested correction: \"{issue.correction}\"\n" +
            f"Explanation: \"{issue.explanation}\"\n" +
            f"Category: {issue.rule_category}"
            for j, issue in enumerate(batch)
        ])

        validation_prompt = f"""
Review these potential language issues and determine which are correctely assessed issues and which are 1) false positives or 2) a matter of pure personal preference:

# VALIDATION RULES:
Spelling is never a valid issue
missing or extra white space is never a valid issue

{issues_text}

# For EACH issue, provide:
1. Is it truly an issue? Answer YES or NO.
2. If YES but explanation is wrong, provide a clear, concise explanation (max 30 words). If NO, explanation can be empty.

Format your response ONLY as a JSON object with a single key "validations",
which is an array of objects, each containing "issue_number" (int), "is_valid" (boolean),
and "explanation" (string, use original if good, improve if needed, empty if not valid). Example:
{{
  "validations": [
    {{ "issue_number": 1, "is_valid": true, "explanation": "Incorrect subject-verb agreement." }},
    {{ "issue_number": 2, "is_valid": false, "explanation": "" }}
  ]
}}

# IMPORTANT OUTPUT RULES:
# 1. Your *entire* response MUST consist ONLY of the single JSON object described above containing the "validations" array (starting with '{{' and ending with '}}').
# 2. Do NOT include ANY text, reasoning, explanations, summaries, or conversation *before* or *after* the JSON object.
# 3. Do NOT use markdown code fences (like ```json or ```).
# 4. Ensure the JSON is perfectly valid. The output is parsed programmatically.

"""
        response_text = None
        # --- API Call based on Provider ---
        try:
            if provider == "OpenAI":
                if not isinstance(client, OpenAI): raise TypeError("Invalid client type for OpenAI")
                messages = [{"role": "system", "content": system_content_validation}, {"role": "user", "content": validation_prompt}]
                response = client.chat.completions.create(
                    model=model_name, messages=messages, temperature=0.1,
                    response_format={"type": "json_object"}, max_tokens=1000 # Adjust tokens
                )
                if response.choices: response_text = response.choices[0].message.content

            elif provider == "Anthropic":
                 if not isinstance(client, anthropic.Anthropic): raise TypeError("Invalid client type for Anthropic")
                 response = client.messages.create(
                    model=model_name, system=system_content_validation,
                    messages=[{"role": "user", "content": validation_prompt}],
                    temperature=0.1, max_tokens=1000
                )
                 if response.content and isinstance(response.content[0], anthropic.types.TextBlock): # ... extract basic {} block ...
                    raw_output = response.content[0].text.strip(); json_start = raw_output.find('{'); json_end = raw_output.rfind('}')
                    json_start = raw_output.find('{')
                    json_end = raw_output.rfind('}')
                    if json_start != -1 and json_end != -1: response_text = raw_output[json_start:json_end+1]
                    else: logger.warning(f"{log_prefix_val}Could not extract JSON block from Anthropic.")
                 else: logger.warning(f"{log_prefix_val}Anthropic response not text block.")

            elif provider == "OpenRouter":
                 if not isinstance(client, OpenAI): raise TypeError("Invalid client type for OpenRouter")
                 messages = [{"role": "system", "content": system_content_validation}, {"role": "user", "content": validation_prompt}]

                 # Check if the model is likely a Google model - avoid forcing response_format if so.
                 is_google_model = "google/" in model_name.lower() or "gemini" in model_name.lower() or "gemma" in model_name.lower()

                 completion_kwargs = {
                     "model": model_name,
                     "messages": messages,
                     "temperature": 0.1,
                     "max_tokens": 1000
                 }

                 if not is_google_model:
                     # Apply JSON mode for non-Google models
                     logger.info(f"{log_prefix_val}Applying response_format=json_object for OpenRouter model: {model_name}")
                     completion_kwargs["response_format"] = {"type": "json_object"}
                 else:
                     # Rely on prompting for Google models via OpenRouter
                     logger.info(f"{log_prefix_val}Skipping response_format for likely Google model via OpenRouter: {model_name}. Relying on prompt.")

                 response = client.chat.completions.create(**completion_kwargs) # Use updated kwargs
                 if response.choices: response_text = response.choices[0].message.content
            else:
                logger.error(f"EN Validation: Unsupported validation provider: {provider}")
                validated_issues.extend(batch)
                continue

            if response_text:

                cleaned_json_text = response_text.strip()
                logger.debug(f"{log_prefix_val}Raw response: '{cleaned_json_text}'")

                if cleaned_json_text.startswith("```json"):
                    cleaned_json_text = cleaned_json_text[len("```json"):].strip()
                elif cleaned_json_text.startswith("```"):
                    cleaned_json_text = cleaned_json_text[len("```"):].strip()

                if cleaned_json_text.endswith("```"):
                    cleaned_json_text = cleaned_json_text[:-len("```")].strip()

                if not (cleaned_json_text.startswith('{') and cleaned_json_text.endswith('}')) and \
                   not (cleaned_json_text.startswith('[') and cleaned_json_text.endswith(']')):
                    logger.warning(f"{log_prefix_val}Response after cleaning fences doesn't look like JSON: '{cleaned_json_text[:200]}...'")

                logger.info(f"{log_prefix_val}Attempting JSON parse on cleaned text...")

                try:
                    # Use cleaned_json_text for parsing
                    validation_results = json.loads(cleaned_json_text)
                     # Process results map and loop through batch (same logic as before)
                    results_map = {val.get("issue_number"): val for val in validation_results.get("validations", [])}
                    for j, issue in enumerate(batch):
                        issue_num = j + 1
                        val_result = results_map.get(issue_num)
                        if val_result and val_result.get("is_valid", False):
                             validated_issue = GrammarStep( # ... create validated_issue ... )
                                 original_error=issue.original_error, context=issue.context,
                                 explanation=val_result.get("explanation", "").strip() or issue.explanation,
                                 correction=issue.correction, rule_category=issue.rule_category,
                                 gpt_action=issue.gpt_action
                             )
                             validated_issues.append(validated_issue)
                             batch_validated = True
                        else:
                             logger.info(f"{log_prefix_val}Invalidated issue #{issue_num}: '{issue.original_error}'")
                             if val_result: batch_validated = True

                    if not batch_validated and len(batch)>0:
                         logger.warning(f"{log_prefix_val}Cleaned JSON parsed but no items processed.")
                         validated_issues.extend(batch) # Fallback

                except json.JSONDecodeError as e_json:
                    logger.error(f"{log_prefix_val}JSON parsing error ({provider}, {model_name}): {e_json}")
                    logger.error(f"Cleaned text causing error: {cleaned_json_text}")
                    validated_issues.extend(batch) # Fallback
                except Exception as e_proc:
                    logger.error(f"{log_prefix_val}Error processing results ({provider}, {model_name}): {e_proc}", exc_info=True)
                    validated_issues.extend(batch) # Fallback
            else:
                logger.warning(f"{log_prefix_val}Received empty response from {provider} ({model_name}).")
                validated_issues.extend(batch) # Fallback

        except Exception as e_api:
            logger.error(f"{log_prefix_val}API Error ({provider}, {model_name}): {e_api}", exc_info=True)
            validated_issues.extend(batch) # Fallback

    logger.info(f"EN Batch validation complete using {provider} ({model_name}): {len(validated_issues)} issues remain from {len(grammar_issues)}.")
    return validated_issues

def deduplicate_issues_by_original_error(
    issues: List[GrammarStep],
    logger,
    similarity_threshold: float = 0.80
    ) -> List[GrammarStep]:
    """
    Removes duplicate or near-duplicate issues based on original_error similarity.
    Enhanced: Checks exact match (case-insensitive, ignoring surrounding whitespace) first.
    """
    if not issues:
        return []

    logger.info(f"Starting deduplication based on original_error for {len(issues)} issues...")
    unique_issues = []
    # Store the NORMALIZED original_error text of items added to unique_issues
    added_normalized_errors = set() # Use a set for efficient exact checking

    for current_issue in issues:
        is_duplicate = False
        # Normalize the current issue's error text (strip whitespace, convert to lowercase)
        normalized_current = current_issue.original_error.strip().lower()

        # --- Check for EXACT match first ---
        if normalized_current in added_normalized_errors:
            logger.debug(f"Found EXACT duplicate (case-insensitive, ignoring whitespace). Skipping: '{current_issue.original_error}'")
            is_duplicate = True
        else:
            # --- If no exact match, check for SIMILAR matches (optional, based on threshold) ---
            # This part prevents adding items that are very similar (but not identical)
            # to items *already added*.
            for added_issue in unique_issues:
                # Compare original strings using difflib for similarity ratio
                matcher = difflib.SequenceMatcher(None, current_issue.original_error.strip(), added_issue.original_error.strip())
                ratio = matcher.ratio()

                if ratio >= similarity_threshold:
                    # Log near-duplicates if ratio meets threshold but wasn't an exact match
                    logger.debug(f"Found NEAR-duplicate original_error (Ratio: {ratio:.2f}). Skipping: '{current_issue.original_error}' (similar to '{added_issue.original_error}')")
                    is_duplicate = True
                    break # Found a similar item, no need to check further against others

        # Add the issue if it's not an exact or near duplicate of anything added so far
        if not is_duplicate:
            unique_issues.append(current_issue)
            added_normalized_errors.add(normalized_current) # Add the normalized version to the set for future exact checks

    logger.info(f"Reduced to {len(unique_issues)} issues after enhanced original_error deduplication.")
    return unique_issues

def select_final_issues_by_explanation(
    unique_issues: List[GrammarStep],
    logger,
    max_total: int = 10,
    explanation_similarity_threshold: float = 0.60 # Adjust as needed (looser threshold might group more)
    ) -> List[GrammarStep]:
    """
    Selects a final set of issues for display from a de-duplicated list,
    prioritizing variety based on explanation similarity.

    Args:
        unique_issues: List of GrammarStep objects, already deduplicated based on original_error.
        logger: Logger instance.
        max_total: Maximum number of issues to return.
        explanation_similarity_threshold: Threshold for grouping explanations.

    Returns:
        A selected list of GrammarStep objects up to max_total.
    """
    if not unique_issues:
        return []

    if len(unique_issues) <= max_total:
        logger.info(f"Returning all {len(unique_issues)} unique issues as it's less than or equal to max_total ({max_total}).")
        return unique_issues

    logger.info(f"Selecting final {max_total} issues from {len(unique_issues)} unique candidates using explanation similarity...")

    # --- 1. Cluster issues by explanation similarity ---
    explanation_clusters = [] # List of lists, each inner list is a cluster of GrammarStep objects
    cluster_representative_explanations = [] # Track the first explanation added for each cluster

    for issue in unique_issues:
        found_cluster_idx = -1
        highest_similarity = -1.0

        # Find the best matching existing cluster
        for i, representative_explanation in enumerate(cluster_representative_explanations):
            matcher = difflib.SequenceMatcher(None, issue.explanation, representative_explanation)
            ratio = matcher.ratio()
            if ratio >= explanation_similarity_threshold and ratio > highest_similarity:
                 highest_similarity = ratio
                 found_cluster_idx = i

        if found_cluster_idx != -1:
             # Add to the best matching existing cluster
             explanation_clusters[found_cluster_idx].append(issue)
             logger.debug(f"Added issue '{issue.original_error}' to explanation cluster {found_cluster_idx} (similarity: {highest_similarity:.2f})")
        else:
            # Start a new cluster
            explanation_clusters.append([issue])
            cluster_representative_explanations.append(issue.explanation)
            logger.debug(f"Started new explanation cluster {len(explanation_clusters)-1} with issue '{issue.original_error}'")

    logger.info(f"Grouped {len(unique_issues)} issues into {len(explanation_clusters)} clusters based on explanation similarity.")

    # --- 2. Select issues from clusters ---
    final_selection = []
    remaining_slots = max_total

    # Sort clusters: prioritize larger clusters (more common error types)
    # Also add category as a secondary sort key? E.g., prioritize GRAMMAR clusters?
    # Simple sort by size for now:
    explanation_clusters.sort(key=len, reverse=True)
    logger.info(f"Cluster sizes (desc): {[len(c) for c in explanation_clusters]}")


    # Keep track of how many we've picked from each cluster to ensure variety
    picked_from_cluster_count = defaultdict(int)
    cluster_indices_order = list(range(len(explanation_clusters))) # Order based on sorted clusters

    # Strategy: Cycle through clusters, taking one from each until max_total is reached
    items_added_in_cycle = True # Flag to detect when no more items can be added
    while remaining_slots > 0 and items_added_in_cycle:
        items_added_in_cycle = False # Reset flag for this cycle
        for i in cluster_indices_order:
            if remaining_slots <= 0:
                break # Stop if we hit the limit

            cluster = explanation_clusters[i]
            pick_index = picked_from_cluster_count[i]

            if pick_index < len(cluster):
                 # Pick the next available issue from this cluster
                 issue_to_add = cluster[pick_index]
                 final_selection.append(issue_to_add)
                 picked_from_cluster_count[i] += 1
                 remaining_slots -= 1
                 items_added_in_cycle = True # We added something in this cycle
                 logger.debug(f"Selected '{issue_to_add.original_error}' from cluster {i} (pick {picked_from_cluster_count[i]}). Remaining slots: {remaining_slots}")


    logger.info(f"Selected a total of {len(final_selection)} issues using explanation clustering and round-robin selection.")
    return final_selection

def _run_grammar_analysis_and_validation(
    essay: Essay,
    language_tool,
    openai_client,
    config: Config,
    logger
    ) -> Tuple[Optional[str], List[GrammarStep]]:
    """
    Runs first pass (LT) and second pass (GPT) analysis and validation.
    """
    corrected_text = essay.original_text
    validated_steps = []

    # --- First Pass (LanguageTool) ---
    try:
        feedback = analyze_text(
            text=essay.original_text,
            language_tool=language_tool,
            openai_client=openai_client,
            logger=logger
        )
        if feedback:
            logger.info(f"First pass complete: {len(feedback.steps)} LT issues. Text corrected.")
            corrected_text = feedback.final_suggestion
        else:
            logger.warning(f"First pass failed/no feedback for {essay.student_name}. Using original text.")
    except Exception as e_first_pass:
        logger.error(f"Error during first pass analysis: {e_first_pass}") # CORRECTED INDENTATION
        logger.warning("Proceeding with original text due to first pass error.") # CORRECTED INDENTATION

    # --- Second Pass (GPT-based Analysis & Validation) ---
    if config.ENABLE_SECOND_PASS and openai_client:
        logger.info(f"Performing second pass analysis for {essay.student_name}...")
        try:
            text_chunks_with_context = chunk_text(
                corrected_text,
                max_length=config.MAX_CHUNK_SIZE,
                token_window=config.TOKEN_WINDOW
            )
            logger.info(f"Split text into {len(text_chunks_with_context)} chunks with context for second pass.")

            second_pass_raw_steps = []
            for chunk_id, formatted_chunk in text_chunks_with_context:
                chunk_feedback = reiterate_analysis(
                    text_chunk=formatted_chunk,
                    openai_client=openai_client,
                    logger=logger,
                    config=config,
                    language_tool=language_tool, # Pass LT instance even if reiterate_analysis doesn't use it
                    chunk_id=chunk_id
                )

                if chunk_feedback and chunk_feedback.steps:
                    filtered_chunk_steps = [
                        step for step in chunk_feedback.steps
                        if "spell" not in step.rule_category.lower() and
                           "typo" not in step.rule_category.lower()
                    ]
                    # Add chunk ID prefix to context
                    for step in filtered_chunk_steps:
                           step.context = f"[Chunk {chunk_id}] {step.context}"
                    second_pass_raw_steps.extend(filtered_chunk_steps)


            logger.info(f"Second pass identified {len(second_pass_raw_steps)} raw grammar/style/cohesion issues.")

            if second_pass_raw_steps:
                logger.info("Validating second pass grammar issues...")
                # Determine validator based on language (assuming config holds this info indirectly)
                if config.ACTIVE_MASTER_PROMPT == MASTER_SYSTEM_PROMPT_SWE: # Check against actual constant
                    validator_func = validate_grammar_issues_batch
                    logger.info("Using Swedish grammar validator.")
                else:
                    validator_func = validate_grammar_issues_batch_en
                    logger.info("Using English grammar validator.")

                validated_steps = validator_func(
                    second_pass_raw_steps,
                    openai_client,
                    logger,
                    config
                )
                logger.info(f"Validation complete: {len(validated_steps)}/{len(second_pass_raw_steps)} issues validated.")
            else:
                logger.info("No second pass issues found to validate.")

        except Exception as e_second_pass:
            logger.error(f"Error during second pass analysis/validation: {e_second_pass}") # CORRECTED INDENTATION
            logger.warning("Proceeding without second pass issues due to error.") # CORRECTED INDENTATION
            validated_steps = [] # Ensure empty list on error # CORRECTED INDENTATION

    else:
        logger.info("Second pass analysis skipped (disabled in config or OpenAI client unavailable).")

    return corrected_text, validated_steps


# NEW Helper Function 2: Filtering and Selection
def _filter_and_select_final_issues(
    validated_steps: List[GrammarStep],
    config: Config, # Pass config if max_total or thresholds are needed from there
    logger
    ) -> List[GrammarStep]:
    """
    Applies de-duplication and sophisticated selection to validated issues.
    Includes extensive logging for debugging duplicate/count issues.
    """
    logger.info("Applying final filtering and selection to validated second-pass issues...")

    logger.info(f"--> [DEBUG] DETAILED Input to filtering/selection stage ({len(validated_steps)} items):")
    if not validated_steps:
        logger.info("    (List is empty)")
    else:
        # Log details for EACH item received before any processing
        for i, step in enumerate(validated_steps):
            # Log key fields to identify potential identical/near-identical inputs
            logger.info(f"    PreFilter[{i:02d}]: Orig='{step.original_error}' | Expl='{step.explanation}' | Cat='{step.rule_category}'")
            # Optionally log context or correction if needed for debugging:
            # logger.info(f"        Context: '{step.context[:100]}...' | Correction: '{step.correction}'")
    logger.info("--- End Detailed Input ---")

    final_issues_for_doc = [] # Default to empty list

    try:
        # 1. Deduplicate based on original_error
        # Assuming deduplicate_issues_by_original_error uses enhanced logic (exact match first, lowercasing)
        unique_error_instances = deduplicate_issues_by_original_error(
            issues=validated_steps,
            logger=logger,
            similarity_threshold=0.85 # Consider making this configurable via config
        )

        # --- 2. LOGGING: Input to Selection (Post-Deduplication) ---
        logger.info(f"--> [DEBUG] Input to selection (after deduplication) has {len(unique_error_instances)} items.")
        for i, step in enumerate(unique_error_instances[:15]):
             logger.info(f"    Unique[{i:02d}]: Orig='{step.original_error}' | Expl='{step.explanation}' | Cat='{step.rule_category}'")
        if len(unique_error_instances) > 30:
             logger.info(f"    (logged details for first 30 of {len(unique_error_instances)} unique items)")



        # 2. Select final representative issues based on explanation, category, etc.
        final_issues_for_doc = select_final_issues_by_explanation(
            unique_issues=unique_error_instances,
            logger=logger,
            max_total=8, # Hardcoded limit from original problem description
            explanation_similarity_threshold=0.80 # Consider making this configurable via config
        )

        # --- 3. LOGGING: Final Selection Output ---
        logger.info(f"--> [DEBUG] Final selected issues count for document: {len(final_issues_for_doc)}")
        for i, step in enumerate(final_issues_for_doc):
            logger.info(f"    Final[{i:02d}]: Orig='{step.original_error}' | Expl='{step.explanation}' | Cat='{step.rule_category}'")
        # --- END LOGGING ---

    except Exception as e_filtering:
        logger.error(f"Error during filtering/selection: {e_filtering}")
        logger.warning("Returning empty list for grammar issues due to filtering error.")
        final_issues_for_doc = [] # Assign empty list on filtering error
        # Log the state just before the error if possible, or just log the error.

    # Return the final curated list
    return final_issues_for_doc


# Refactored Main Function
def process_grammar_analysis(essay: Essay, language_tool, openai_client, config: Config) -> bool:
    """
    Orchestrates grammar analysis, validation, filtering, and selection for an essay.

    Args:
        essay: The Essay object to analyze and update.
        language_tool: LanguageTool instance.
        openai_client: OpenAI client instance.
        config: Configuration object.

    Returns:
        bool: True if the overall process completed (even if no issues found), False on critical error.
    """
    try:
        logger.info(f"Beginning grammar processing orchestration for {essay.student_name}")

        # 1. Run Analysis and Validation
        corrected_text, validated_steps = _run_grammar_analysis_and_validation(
            essay, language_tool, openai_client, config, logger
        )

        # Update essay's corrected text (important for feedback generation later)
        essay.corrected_text = corrected_text if corrected_text is not None else essay.original_text

        # 2. Filter and Select Final Issues
        final_issues_for_doc = _filter_and_select_final_issues(
            validated_steps, config, logger
        )

        # 3. Final Assignment to Essay Object
        essay.grammar_issues = final_issues_for_doc
        essay.has_grammar_processed = True # Mark grammar phase as completed

        logger.info(f"Grammar analysis phase complete for {essay.student_name}. Stored {len(final_issues_for_doc)} issues for document.")
        return True # Indicate overall success of this phase

    except Exception as e_main:
        # Catch any unexpected errors during orchestration
        logger.error(f"Critical error in process_grammar_analysis orchestration for {essay.student_name}: {e_main}")
        import traceback
        logger.error(traceback.format_exc())
        # Ensure essay object reflects failure state
        essay.grammar_issues = []
        essay.corrected_text = essay.original_text # Revert to original on failure?
        essay.has_grammar_processed = False # Mark as not successfully processed
        return False # Indicate failure of the analysis phase

"""### Feedback Generation

### Service Initialization
"""

@retry(
    stop=stop_after_attempt(4), # 1 initial call + 3 retries = 4 total attempts
    wait=wait_exponential(multiplier=1, min=2, max=30), # Wait 2s, 4s, 8s,... up to 30s max
    retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),
    before_sleep=log_before_retry, # Use the custom logging callback
    reraise=True # Re-raise the last exception if all retries fail
)
def generate_feedback(
    essay: Essay,
    config: Config,
    provider: str,
    model_name: str,
    client: Any,
    logger_instance: logging.Logger # <<< ADDED: Explicit logger instance parameter
    ) -> Optional[str]:
    """
    Generate essay feedback using the specified provider, model, and client.
    Includes tenacity retry logic for transient API errors defined in
    RETRYABLE_EXCEPTIONS.

    Args:
        essay: The Essay object containing text and metadata.
        config: The configuration object with prompts and settings.
        provider: The name of the LLM provider ("OpenAI", "Anthropic", "OpenRouter").
        model_name: The specific model identifier for the provider.
        client: The initialized API client for the specified provider.
        logger_instance: The logger object to use for logging within this function.

    Returns:
        The generated feedback text as a string, or None if generation fails
        after retries or encounters a non-retryable error.

    Raises:
        RetryError: If all retry attempts fail for a RETRYABLE_EXCEPTION.
        Other exceptions: If a non-retryable error occurs during the API call.
    """
    # Use the passed-in logger for all logging within this function
    log = logger_instance
    student_name_fallback = essay.student_name or "the student"
    # Log the first attempt (or subsequent attempts by tenacity decorator)
    # The log_before_retry callback handles logging *before sleeping* for retries.
    # We log the initial attempt info here.
    log.info(f"Attempting feedback generation for '{student_name_fallback}' using Provider: {provider}, Model: {model_name}")

    # --- Input Validation (using 'log') ---
    if not client:
        log.error(f"LLM client for provider '{provider}' is not available or not initialized.")
        return None
    if not essay.corrected_text:
        log.warning("No corrected text available for feedback generation.")
        return None
    if not model_name:
        log.error(f"No feedback model name provided for provider {provider}.")
        return None
    if not config.ACTIVE_MASTER_PROMPT or not config.ACTIVE_SYSTEM_MESSAGE or not config.ESSAY_INSTRUCTION:
        log.error("Essential prompt components (Master, System, Instructions) missing in config.")
        return None

    # --- Prepare Prompts (using 'log') ---
    system_content = config.ACTIVE_MASTER_PROMPT # Master prompt defines the overall role

    try:
        formatted_system_message = config.ACTIVE_SYSTEM_MESSAGE.format(student_name=student_name_fallback)
    except KeyError as e:
        log.error(f"Error formatting ACTIVE_SYSTEM_MESSAGE for student '{student_name_fallback}'. Missing key: {e}. Using unformatted message.")
        formatted_system_message = config.ACTIVE_SYSTEM_MESSAGE
    except Exception as e_format:
        log.error(f"Unexpected error formatting ACTIVE_SYSTEM_MESSAGE: {e_format}. Using unformatted message.")
        formatted_system_message = config.ACTIVE_SYSTEM_MESSAGE

    user_content = (
        formatted_system_message + "\n\n" +
        "ESSAY INSTRUCTIONS PROVIDED TO STUDENT:\n" +
        config.ESSAY_INSTRUCTION + "\n\n" +
        "STUDENT ESSAY TEXT:\n" +
        essay.corrected_text
    )

    # --- API Call based on Provider ---
    # The @retry decorator handles the loop and exceptions in RETRYABLE_EXCEPTIONS.
    # This internal try/except catches non-retryable errors or other code issues.
    try:
        feedback = None
        if provider == "OpenAI":
            if not isinstance(client, OpenAI): raise TypeError(f"Invalid client type for OpenAI: {type(client)}")
            messages = [{"role": "system", "content": system_content}, {"role": "user", "content": user_content}]
            response = client.chat.completions.create(
                model=model_name, messages=messages, temperature=0.3, max_tokens=1500
            )
            if response.choices: feedback = response.choices[0].message.content.strip()

        elif provider == "Anthropic":
            if not isinstance(client, anthropic.Anthropic): raise TypeError(f"Invalid client type for Anthropic: {type(client)}")
            response = client.messages.create(
                model=model_name, system=system_content,
                messages=[{"role": "user", "content": user_content}],
                temperature=0.7, max_tokens=1500
            )
            if response.content and isinstance(response.content[0], anthropic.types.TextBlock):
                feedback = response.content[0].text.strip()
            else:
                log.error(f"Anthropic response content not text block. Type: {type(response.content[0]) if response.content else 'None'}")

        elif provider == "OpenRouter":
            if not isinstance(client, OpenAI): raise TypeError(f"Invalid client type for OpenRouter (expected OpenAI type): {type(client)}")
            messages = [{"role": "system", "content": system_content}, {"role": "user", "content": user_content}]
            response = client.chat.completions.create(
                model=model_name, messages=messages, temperature=0.7, max_tokens=1500
            )
            if response.choices: feedback = response.choices[0].message.content.strip()

        else:
            log.error(f"Unsupported feedback provider specified: {provider}")
            return None # Explicitly return None for unsupported provider

        # --- Process Result ---
        if feedback:
            log.info(f"Successfully generated feedback using {provider} ({model_name}) for {essay.student_name}")
            essay.has_feedback_processed = True
            return feedback
        else:
            log.warning(f"Feedback generation using {provider} ({model_name}) resulted in empty content.")
            # This might happen even without an exception, considered a failure here.
            return None

    except RETRYABLE_EXCEPTIONS as e:
         # This block should technically only be reached if reraise=False (which it isn't)
         # or if the retry condition fails unexpectedly.
         # With reraise=True, the exception is raised *out* of the function by tenacity.
         # Log it defensively here, but the worker should catch the re-raised exception.
         log.error(f"Retryable Exception occurred within generate_feedback: {e}", exc_info=True)
         raise # Re-raise it as per decorator config

    except Exception as e:
        # Catches non-retryable API errors or other code errors within this function
        log.error(f"Non-retryable API error or other exception during feedback generation with {provider} ({model_name}) for {essay.student_name}: {e}")
        log.error(traceback.format_exc())
        # Return None to indicate failure for non-retryable issues
        return None

# Define translations for the editor prompt components
EDITOR_PROMPTS = {
    'en-US': {
        'grammar_intro': "Grammar and style issues detected:\n", # This isn't used in the prompt construction, but kept for potential future use
        'grammar_summary_more': "... plus {count} more issues\n", # Also not currently used in the prompt
        # --- MODIFIED LABEL VALUE ---
        'original_label': "<CORRECTED ESSAY TEXT (LanguageTool Pass):>", # Changed from "<ORIGINAL ESSAY:>"
        # --- END MODIFICATION ---
        'teacher_feedback_label': "<TEACHER FEEDBACK:>", # Remains the same
        'task_label': "<INSTRUCTIONS/YOUR TASK:>", # Remains the same
        'task_instruction_1': "Fixes grammar, spelling, and punctuation issues",
        'task_instruction_2': "Preserves the student's original style, voice, and ideas",
        'task_instruction_3': "Improves the structure based on teacher feedback",
        'task_instruction_4': "Only incorporates advice present in the attached teacher FEEDBACK",
        'output_instruction': "Return ONLY the complete edited text within a set of <redigerad_text>tags, with no explanations or comments.",
        'system_message_role': "You are an expert text editor who uses teacher feedback to make improvements to student essays while preserving their voice, tone and ideas."
    },
    'sv-SE': {
        'grammar_intro': "Språk- och stilproblem identifierade (fokusera på att åtgärda dessa):\n", # Not used in prompt
        'grammar_summary_more': "... samt ytterligare {count} identifierade problem.\n", # Not used in prompt
        'system_message_role': "Du är en noggrann textredaktör som hjälper elever genom att implementera specifik lärarfeedback i deras texter. Ditt mål är att förbättra texten, åtgärda alla kvarvarande språkfel.",
        # --- MODIFIED LABEL VALUE ---
        'original_label': "<GRANSKAD TEXT (LanguageTool):>", # Changed from "<ursprunglig_text>"
        # --- END MODIFICATION ---
        'teacher_feedback_label': "<lararfeedback>", # Remains the same
        'task_label': "<instruktioner>\nDIN UPPGIFT:", # Remains the same
        'task_instruction_1': "- Åtgärdar problem med grammatik, stavning och interpunktion samtidigt som du följer övriga instruktioner.",
        'task_instruction_2': "- **VIKTIGT: Samtidigt som du bevarar elevens ursprungliga röst och idéinnehåll ska du:.**",
        'task_instruction_3':   "- förbättra textens textbindning, struktur och styckeövergångar **helt enligt LÄRARFEEDBACKENS önskemål**.",
        'task_instruction_4':   "- Tillämpa övriga konkreta tips och råd som tas upp i LÄRARFEEDBACKEN.",
        'output_instruction': "Returnera ENDAST den fullständiga, redigerade texten innanför <redigerad_text>-taggar, utan några förklaringar eller metakommentarer.\n</instruktioner>"
    }
}


def generate_edited_version(
    essay: Essay,
    feedback: str,            # Teacher feedback text
    config: Config,           # Pass config for prompts etc.
    provider: str,            # Determined provider
    model_name: str,          # Determined model name
    client: Any,              # Initialized client for the provider
    language_code: str        # 'sv-SE' or 'en-US' for prompt selection
    ) -> Optional[str]:
    """
    Generate an edited version of the essay using the specified LLM.
    """
    logger.info(f"Generating edited version for '{essay.student_name}' using Provider: {provider}, Model: {model_name}")

    # --- Input Validation ---
    if not client:
        logger.error(f"LLM client for provider '{provider}' is not available for editing.")
        return None
    if not essay.corrected_text or not feedback:
        logger.warning("Missing corrected text or feedback for AI editing.")
        return None
    if not model_name:
        logger.error(f"No editor model name provided for provider {provider}.")
        return None

    # --- Select Prompts based on language ---
    prompts = EDITOR_PROMPTS.get(language_code, EDITOR_PROMPTS['en-US']) # Default to English
    required_keys = [ # Check essential keys
         'system_message_role', 'original_label', 'teacher_feedback_label',
         'task_label', 'task_instruction_1', 'task_instruction_2',
         'task_instruction_3', 'task_instruction_4', 'output_instruction',
    ]
    if not all(key in prompts for key in required_keys):
        logger.error(f"Essential keys missing from EDITOR_PROMPTS for lang '{language_code}'.")
        return None

    # --- Prepare Prompt ---
    # Use the <CORRECTED ESSAY TEXT...> label from the prompts dict
    original_tag_open = prompts['original_label']
    # Derive closing tag correctly, assuming format like "<TAG STUFF:>" -> "</TAG STUFF:>"
    original_tag_close = f"</{original_tag_open[1:]}"

    # Use the <lararfeedback> or <TEACHER FEEDBACK:> label
    feedback_tag_open = prompts['teacher_feedback_label']
    feedback_tag_close = f"</{feedback_tag_open[1:]}"


    prompt_user = f"""{original_tag_open}
{essay.corrected_text}
{original_tag_close}

{feedback_tag_open}
{feedback}
{feedback_tag_close}

{prompts['task_label']}
{prompts['task_instruction_1']}
{prompts['task_instruction_2']}
{prompts['task_instruction_3']}
{prompts['task_instruction_4']}
{prompts['output_instruction']}
"""
    prompt_system = prompts['system_message_role']

    # --- API Call based on Provider ---
    raw_edited_essay = None
    try:
        if provider == "OpenAI":
             if not isinstance(client, OpenAI): raise TypeError("Invalid client type for OpenAI")
             messages = [{"role": "system", "content": prompt_system}, {"role": "user", "content": prompt_user}]
             response = client.chat.completions.create(
                model=model_name, messages=messages, temperature=0.3, max_tokens=2500 # Adjust tokens
             )
             if response.choices: raw_edited_essay = response.choices[0].message.content.strip()

        elif provider == "Anthropic":
             if not isinstance(client, anthropic.Anthropic): raise TypeError("Invalid client type for Anthropic")
             response = client.messages.create(
                model=model_name, system=prompt_system,
                messages=[{"role": "user", "content": prompt_user}],
                temperature=0.3, max_tokens=2500 # Use max_tokens
             )
             if response.content and isinstance(response.content[0], anthropic.types.TextBlock):
                 raw_edited_essay = response.content[0].text.strip()
             else: logger.warning("Anthropic editor response not text block.")

        elif provider == "OpenRouter":
             if not isinstance(client, OpenAI): raise TypeError("Invalid client type for OpenRouter")
             messages = [{"role": "system", "content": prompt_system}, {"role": "user", "content": prompt_user}]
             response = client.chat.completions.create(
                model=model_name, messages=messages, temperature=0.3, max_tokens=2500
             )
             if response.choices: raw_edited_essay = response.choices[0].message.content.strip()

        else:
            logger.error(f"Unsupported editor provider: {provider}")
            return None

        # --- Process Response ---
        if not raw_edited_essay:
             logger.warning(f"Editor ({provider}, {model_name}) returned empty content.")
             return None # Return None if API response was empty

        # Extract text within <redigerad_text> tags (case-insensitive)
        # Make sure the tag matches the one requested in the prompt's output_instruction
        match = re.search(r"<redigerad_text>(.*?)</redigerad_text>", raw_edited_essay, re.DOTALL | re.IGNORECASE)
        if match:
            edited_essay = match.group(1).strip()
            logger.info(f"Successfully extracted edited text within tags using {provider} ({model_name}).")
        else:
            logger.warning(f"Could not find <redigerad_text> tags in the response from {provider} ({model_name}). Using the full response.")
            edited_essay = raw_edited_essay # Fallback to full response

        # --- Simple Validation (Optional - keep existing) ---
        if len(edited_essay) < len(essay.corrected_text) * 0.7: logger.warning("Edited essay significantly shorter.")
        elif len(edited_essay) > len(essay.corrected_text) * 1.5: logger.warning("Edited essay significantly longer.")

        return edited_essay

    except Exception as e:
        logger.error(f"API Error generating edited version with {provider} ({model_name}): {e}")
        logger.error(traceback.format_exc())
        return None

@contextmanager
def language_tool_context(language_code: str):
    """
    Context manager for LanguageTool that selects language dynamically.

    Args:
        language_code (str): The language code to use (e.g., 'sv-SE' or 'en-US').
    """
    tool = None
    logger.info(f"Attempting to initialize LanguageTool for language: {language_code}")

    # --- Define custom words for different languages ---
    custom_words_en = {
        # Literature-specific terms (English) - From your original script
        'Omelas', 'LeGuin', 'Ursula', 'Tessie', 'Hutchinson',
        'Shirley', 'Jackson', 'Dunbar', 'Summers', 'Graves',
        'Delacroix', 'Bentham', 'Johnnie','Roanhorse',
        # Literary analysis terms (English) - From your original script
        'utilitarian', 'utilitarianism', 'dystopian', 'dystopia',
        'foreshadowing', 'symbolism', 'allegory', 'allegorical',
        'metaphorical', 'motif', 'motifs', 'diction'
    }

    custom_words_sv = {
        # General Swedish Literature/Context examples
        'Strindberg', 'Lagerlöf', 'Tranströmer', 'Nobelpriset',

        # Specific terms for "Låt den rätte komma in" <<< ADDED WORDS HERE
        'Oskar',          # Huvudkaraktär
        'Eli',            # Huvudkaraktär
        'Håkan',          # Viktig bikaraktär
        'Lacke',          # Bikärkaraktär
        'Virginia',       # Bikärkaraktär
        'Jonny',          # Bikärkaraktär (Oskars mobbare)
        'Tommy',          # Bikärkaraktär (Oskars mobbare)
        'Blackeberg',     # Plats
        'Vällingby',      # Plats
        'Ajvide',         # Del av författarens namn
        'Lindqvist',      # Del av författarens namn
        'fokaliseras',    # Term
        'fokaliserad',    # Term
        'fokalisera'      # Term
        # Lägg till fler namn/termer här om de ofta förekommer och flaggas
    }

    try:
        # --- Select the appropriate custom words list ---
        if language_code == 'sv-SE':
            active_custom_words = custom_words_sv
            logger.info("Using Swedish custom words list.")
        elif language_code == 'en-US':
            active_custom_words = custom_words_en
            logger.info("Using English custom words list.")
        else:
            logger.warning(f"Unsupported language code '{language_code}'. Using empty custom words list.")
            active_custom_words = set()

        # --- Initialize LanguageTool with the specified language ---
        logger.info(f"Initializing local LanguageTool with language code: {language_code}")
        tool = language_tool_python.LanguageTool(language_code)

        # --- Assign custom words ---
        # Ensure tool object exists before assigning custom_words
        if tool:
             tool.custom_words = active_custom_words
             if active_custom_words:
                  logger.info(f"Applied {len(active_custom_words)} custom words for {language_code}.")
             else:
                  logger.info(f"No custom words applied for {language_code}.")
        else:
             # This case should ideally not happen if initialization succeeds,
             # but added for robustness.
             logger.error("LanguageTool object is None, cannot assign custom words.")


        logger.info(f"LanguageTool for {language_code} initialized successfully.")

        yield tool # Provide the initialized tool instance

    except Exception as e:
        logger.error(f"LanguageTool initialization failed for {language_code}: {e}")
        yield None # Indicate failure

    finally:
        # --- Cleanup: Close the tool ---
        if tool is not None:
            try:
                tool.close()
                logger.info(f"LanguageTool for {language_code} closed successfully.")
            except Exception as e:
                logger.error(f"Error closing LanguageTool for {language_code}: {e}")

# Ensure necessary imports like Optional, OpenAI, get_secret, logger are available

class ServiceInitializer:
    """Initializes external services like OpenAI, Anthropic, OpenRouter, and Google APIs."""

    @staticmethod
    def initialize_openai(api_key: str, logger_instance=None) -> Optional[OpenAI]: # Accept api_key
        """Initialize OpenAI client using the provided API key."""
        log = logger_instance or logger
        log.info("Initializing OpenAI client")
        try:
            # Use the passed api_key, remove get_secret call
            if not api_key or not api_key.startswith("sk-"):
                log.error("Provided OpenAI API key is invalid or missing.")
                return None

            client = OpenAI(api_key=api_key)
            client.models.list() # Validate
            log.info("OpenAI client initialized successfully")
            return client
        except Exception as e:
            log.error(f"Error initializing OpenAI client: {e}", exc_info=True)
            return None

    @staticmethod
    def initialize_anthropic(api_key: str, logger_instance=None) -> Optional[anthropic.Anthropic]: # Accept api_key
        """Initialize Anthropic client using the provided API key."""
        log = logger_instance or logger
        log.info("Initializing Anthropic client")
        try:
             # Use the passed api_key, remove get_secret call
            if not api_key:
                log.error("Provided Anthropic API key is missing.")
                return None

            client = anthropic.Anthropic(api_key=api_key)
            if client:
                 log.info("Anthropic client initialized successfully (basic check).")
                 return client
            else:
                 log.error("Anthropic client object creation failed.")
                 return None
        except Exception as e:
            log.error(f"Failed to initialize Anthropic client: {e}", exc_info=True)
            return None

    @staticmethod
    def initialize_openrouter_client(api_key: str, logger_instance=None) -> Optional[OpenAI]: # Accept api_key
        """
        Initializes an OpenAI client configured for OpenRouter using the provided API key.
        """
        log = logger_instance or logger
        log.info("Initializing OpenRouter client (via OpenAI library)...")
        try:
            # Use the passed api_key, remove get_secret call
            if not api_key:
                log.error("Provided OpenRouter API key is missing.")
                return None

            openrouter_client = OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=api_key,
                # Optional default_headers
            )
            # Optional validation
            # openrouter_client.models.list()
            log.info("OpenRouter client initialized successfully.")
            return openrouter_client
        except Exception as e:
            log.error(f"Failed to initialize OpenRouter client: {e}", exc_info=True)
            return None


    @staticmethod
    def initialize_google_services() -> Tuple[Optional[object], Optional[object]]:
        """Initialize Google Drive and Docs services."""
        logger.info("Initializing Google Drive and Docs API services")
        try:
            credentials_json = get_secret('GOOGLE_CLOUD_PRIVATE_KEY')
            if not credentials_json: # Add check if secret is missing
                 logger.error("Failed to get Google Cloud credentials secret.")
                 return None, None
            credentials_info = json.loads(credentials_json)
            credentials = service_account.Credentials.from_service_account_info(
                credentials_info,
                scopes=['https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/documents']
            )
            service_drive = build('drive', 'v3', credentials=credentials)
            service_docs = build('docs', 'v1', credentials=credentials)
            logger.info("Google Services Initialized.") # Add confirmation
            return service_drive, service_docs
        except json.JSONDecodeError as e:
             logger.error(f"Error decoding Google credentials JSON: {e}")
             return None, None
        except Exception as e:
            logger.error(f"Error initializing Google services: {e}", exc_info=True) # Add exc_info
            return None, None

class ServiceManager:
    """Manages all external service connections."""

    def __init__(self, config):
        self.config = config
        self.openai_client = None
        self.anthropic_client = None
        self.service_drive = None
        self.service_docs = None
        self.language_tool = None

    def initialize_services(self) -> bool:
        """Initialize all required services for the main process."""
        services_ok = True

        # Initialize OpenAI (conditional on config or always for validation?)
        # Decide if you *always* want to try initializing OpenAI even if Anthropic is selected,
        # maybe for validation or if other parts still use it.
        # Here, we initialize based on need + validation.
        self.openai_client = ServiceInitializer.initialize_openai()
        if not self.openai_client:
            logger.warning("Failed to initialize OpenAI client in main process (may not be needed if Anthropic selected).")
            # Set services_ok to False only if OpenAI is the selected provider or needed elsewhere
            if self.config.FEEDBACK_PROVIDER == "OpenAI":
                 logger.error("OpenAI client initialization failed, but it's the selected provider.")
                 services_ok = False

        # Initialize Anthropic (conditional or always for validation?)
        self.anthropic_client = ServiceInitializer.initialize_anthropic()
        if not self.anthropic_client:
            logger.warning("Failed to initialize Anthropic client in main process (may not be needed if OpenAI selected).")
            # Set services_ok to False only if Anthropic is the selected provider
            if self.config.FEEDBACK_PROVIDER == "Anthropic Claude":
                 logger.error("Anthropic client initialization failed, but it's the selected provider.")
                 services_ok = False

        # Initialize Google services (usually always needed)
        self.service_drive, self.service_docs = ServiceInitializer.initialize_google_services()
        if not self.service_drive or not self.service_docs:
            logger.error("Failed to initialize Google services in main process.")
            services_ok = False # Google services are likely essential

        return services_ok

    def set_language_tool(self, tool):
        """Set the language tool instance."""
        self.language_tool = tool

"""### Document Creation and Upload"""

class DocumentCreator:
    """Creates and formats feedback documents with language support."""

    # Dictionary holding translations for document elements
    LABELS = {
        'en-US': {
            'email': "Email:",
            'word_count': "Word count:",
            'original_essay': "Original Essay",
            'grammar_analysis': "Grammar and Style Analysis",
            'grammar_summary': "{count} potential grammar and style improvements were identified by the AI assistant. Here's a representative selection of {displayed} key issues to focus on:",
            'grammar_summary_all': "{count} potential grammar and style improvements were identified by the AI assistant:",
            'explanation': "Explanation:",
            'teacher_feedback': "Teacher Feedback",
            'corrected_version': "Corrected Version",
            'corrected_intro': "Below is your essay with some of the suggested improvements applied:"
        },
        'sv-SE': {
            'email': "E-post:",
            'word_count': "Antal ord:",
            'original_essay': "Ursprunglig text",
            'grammar_analysis': "Språk- och stilanalys",
            'grammar_summary': "{count} potentiella förbättringsområden inom språk och stil identifierades. Här är ett representativt urval av {displayed} viktiga punkter att fokusera på:",
            'grammar_summary_all': "{count} potentiella förbättringsområden inom språk och stil identifierades:",
            'explanation': "Förklaring:",
            'teacher_feedback': "Lärarfeedback",
            'corrected_version': "Korrigerad version",
            'corrected_intro': "Nedan följer din text med några av de föreslagna förbättringarna införda:"
        }
    }

    def __init__(self, config: Config, logger, language_code: str): # Added language_code
        self.config = config
        self.logger = logger
        self.lang = language_code # Store language code
        # Select the correct set of labels, default to English if code is unknown
        self.labels = self.LABELS.get(self.lang, self.LABELS['en-US'])
        self.logger.info(f"DocumentCreator initialized for language: {self.lang}")

        # Style settings remain the same
        self.style_font = config.STYLE_FONT
        self.style_size = config.STYLE_SIZE
        self.style_spacing = config.STYLE_SPACING


    def _setup_document_style(self, doc: Document) -> None:
        """Set up basic document styling."""
        style = doc.styles['Normal']
        style.paragraph_format.space_after = Pt(self.style_spacing)
        style.font.name = self.style_font
        style.font.size = Pt(self.style_size)

    def _parse_markdown(self, text: str, paragraph) -> None:
        """Parse simple markdown-style formatting in text."""
        # This function is language-agnostic
        parts = re.split(r'(\*\*.*?\*\*|\*.*?\*)', text)
        for part in parts:
            if part.startswith('**') and part.endswith('**'):
                run = paragraph.add_run(part[2:-2])
                run.bold = True
            elif part.startswith('*') and part.endswith('*'):
                run = paragraph.add_run(part[1:-1])
                run.italic = True
            else:
                if part.strip():
                    paragraph.add_run(part)

    def _add_grammar_feedback_section(self, doc: Document, grammar_issues: List[GrammarStep]) -> None:
        """
        Adds the grammar feedback section to the document, including the content
        for each issue and debugging logs. Assumes the input grammar_issues list
        IS the final, selected list.
        """
        # Check if the list is empty
        if not grammar_issues:
            self.logger.info("No grammar issues provided (or remained after filtering) to display.")
            return

        # --- Add Section Heading ---
        doc.add_heading(self.labels['grammar_analysis'], level=1)

        # --- Add Summary Text ---
        summary_text = self.labels['grammar_summary_all'].format(count=len(grammar_issues))
        doc.add_paragraph(summary_text)

        # --- Start Debug Logging ---
        self.logger.info(f"--- Debugging _add_grammar_feedback_section ---")
        self.logger.info(f"Received {len(grammar_issues)} items.")

        # --- Group Issues by Category ---
        categories = {}
        for step in grammar_issues:
            # Use upper() for consistency if categories are like "GRAMMAR", "STYLE"
            category = step.rule_category.upper()
            if category not in categories:
                categories[category] = []
            categories[category].append(step)

        category_keys = sorted(categories.keys())
        self.logger.info(f"Processing categories: {category_keys}")

        # --- Loop Through Categories ---
        for category_idx, category in enumerate(category_keys):
            steps_in_category = categories[category]
            self.logger.info(f"  Processing Category: '{category}' ({len(steps_in_category)} items)")

            # Add category heading paragraph
            category_para = doc.add_paragraph()
            run = category_para.add_run(category)
            run.bold = True

            # --- Loop Through Steps in Current Category ---
            step_counter_inner = 0
            for step in steps_in_category:
                step_counter_inner += 1
                self.logger.info(f"    Adding Step {step_counter_inner}/{len(steps_in_category)} for '{category}': Orig='{step.original_error}'")

                # Extract data for clarity
                original = step.original_error
                correction = step.correction
                explanation = step.explanation

                # Create a new paragraph for this specific issue
                p = doc.add_paragraph()
                p.add_run("- ") # Add the bullet point

                # --- Add the Actual Content Runs ---
                # Add the original text
                original_run = p.add_run(f"'{original}'")
                if self.config.USE_COLOR_FEEDBACK:
                    original_run.font.color.rgb = RGBColor(255, 0, 0) # Red

                # Add the arrow separator
                p.add_run(" → ")

                # Add the correction text
                correction_run = p.add_run(f"'{correction}'")
                if self.config.USE_COLOR_FEEDBACK:
                    correction_run.font.color.rgb = RGBColor(0, 128, 0) # Green

                # Add the explanation if it exists
                if explanation:
                    # Adding newline and indentation for explanation
                    # Ensure self.labels is correctly populated
                    explanation_label = self.labels.get('explanation', 'Explanation:') # Fallback label
                    p.add_run(f"\n      {explanation_label} {explanation}")
                # --- End Content Runs ---

                # Apply paragraph indentation
                # Ensure Inches is imported: from docx.shared import Inches
                try:
                    p.paragraph_format.left_indent = Inches(0.25)
                    p.paragraph_format.first_line_indent = Inches(-0.25)
                except NameError:
                    self.logger.error("Inches not defined - make sure it's imported from docx.shared")
                except Exception as e_indent:
                    self.logger.error(f"Error setting indentation: {e_indent}")

            # --- End Inner Loop ---
            self.logger.info(f"  Finished adding {step_counter_inner} steps for category '{category}'.")

            # --- Add Separator Between Categories ---
            # Add separator AFTER processing ALL steps in the current category,
            # but not after the last category.
            if category_idx < len(category_keys) - 1:
                self.logger.info(f"  Adding separator after category '{category}'.")
                separator = doc.add_paragraph("────────────────────────────────────────")
                # Reset indentation for the separator line if needed (optional)
                separator.paragraph_format.left_indent = None
                separator.paragraph_format.first_line_indent = None

        # --- End Outer Loop ---
        self.logger.info(f"--- Finished _add_grammar_feedback_section ---")

    def create_doc(self, essay: Essay, feedback: str) -> Optional[Document]:
        """
        Create feedback document with all sections and translated labels.
        Includes logging for debugging grammar issues received.
        """
        try:
            if not essay.student_name:
                 # Consider adding language-specific error messages if needed
                 raise ValueError("Student name is required")

            doc = Document()
            self._setup_document_style(doc)

            # Title (Student Name) - Language independent
            title = doc.add_heading(essay.student_name, level=0)
            title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER

            # Student info with translated labels
            if essay.student_email or essay.word_count:
                info_para = doc.add_paragraph()
                if essay.student_email:
                    info_para.add_run(f"{self.labels['email']} {essay.student_email}") # Use label
                    if essay.word_count:
                        info_para.add_run(f" • ")
                if essay.word_count:
                    info_para.add_run(f"{self.labels['word_count']} {essay.word_count}") # Use label
                info_para.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER

            # Original Essay with translated heading
            doc.add_heading(self.labels['original_essay'], level=1) # Use label
            if essay.original_text:
                for para in re.split(r'\n\n|\n', essay.original_text.strip()):
                    if para.strip():
                        doc.add_paragraph(para.strip())
            else:
                 # Consider translating this fallback text
                 doc.add_paragraph("No essay content available")

            self.logger.info(f"--> [DEBUG] DocumentCreator received {len(essay.grammar_issues)} grammar issues to add.")
            for i, step in enumerate(essay.grammar_issues):
                self.logger.info(f"  DocInput[{i:02d}]: Orig='{step.original_error}' | Expl='{step.explanation}' | Cat='{step.rule_category}'")


            # Grammar feedback section (uses translated labels internally)
            # Check if list exists and is not empty before adding section
            if essay.grammar_issues: # Check if the list is truthy (exists and not empty)
                self.logger.info(f"Attempting to add grammar feedback section...") # Add log before calling
                self._add_grammar_feedback_section(doc, essay.grammar_issues)
                # Keep the existing log inside _add_grammar_feedback_section or log success here
                self.logger.info(f"Successfully added grammar feedback section with {len(essay.grammar_issues)} issues.")
            else:
                self.logger.info("Skipping grammar feedback section as essay.grammar_issues is empty or None.")


            # AI Feedback Section with translated heading
            doc.add_heading(self.labels['teacher_feedback'], level=1) # Use label
            if feedback:
                for para in re.split(r'\n\n|\n', feedback.strip()):
                    if para.strip():
                        p = doc.add_paragraph()
                        self._parse_markdown(para.strip(), p) # Markdown parsing is language-agnostic
            else:
                # Consider translating this fallback text
                doc.add_paragraph("No literary analysis feedback available")

            # Corrected Version with translated heading and intro text
            if (essay.edited_text and essay.edited_text.strip() != essay.original_text.strip()):
                doc.add_heading(self.labels['corrected_version'], level=1) # Use label
                # Use translated intro text
                doc.add_paragraph(self.labels['corrected_intro'])
                for para in re.split(r'\n\n|\n', essay.edited_text.strip()):
                    if para.strip():
                        doc.add_paragraph(para.strip())
                self.logger.info("Added corrected version section to document")
            else:
                self.logger.info("Skipping corrected version section - no differences or no edited version")

            return doc

        except Exception as e:
            self.logger.error(f"Error creating feedback document: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

    # create_and_upload_feedback method remains largely the same
    # as filename generation is mostly language-agnostic
    def create_and_upload_feedback(
        self,
        essay: Essay,
        feedback: str,
        service_drive,
        output_folder_id: str,
        file_name: Optional[str] = None
    ) -> Optional[str]:
        """
        Create and upload feedback document to Google Drive.
        """
        self.logger.info(f"Creating feedback document for {essay.student_name} ({self.lang})")

        doc = self.create_doc(essay, feedback)

        if not doc:
            self.logger.error("Failed to create feedback document")
            return None

        try:
            # Filename generation (mostly language independent)
            if not file_name:
                timestamp = datetime.now().strftime('%d%m%y-%H%M')
                student_name_formatted = capitalize_name(essay.student_name).replace(' ', '_')
                email_part = f"-{essay.student_email}" if essay.student_email else ""
                # Optionally add language to filename? e.g., f"-{self.lang}-"
                file_name = f"{student_name_formatted}{email_part}-{timestamp}.docx"

            self.logger.info(f"Uploading feedback document as: {file_name}")
            file_id = upload_file(service_drive, output_folder_id, doc, file_name) # Assumes upload_file is defined

            if file_id:
                self.logger.info(f"Successfully uploaded feedback as: {file_name}")
                return file_id
            else:
                self.logger.error(f"Upload failed for {file_name}")
                return None

        except Exception as e:
            self.logger.error(f"Error during file upload: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

"""### Single Essay Processing Pipeline"""

class EssayTracker:
    """Tracks processed essays and generates summary data."""

    def __init__(self):
        self.processed_essays = {}  # Dictionary to store processed essay details
        self.total_essays = 0       # Total count of essays in input folder
        self.start_time = datetime.now()

    def set_total_count(self, count: int) -> None:
        """Set the total number of essays in the input folder."""
        self.total_essays = count
        logger.info(f"Total essays to process: {count}")

    def add_processed_essay(self, student_name: str, student_email: str,
                           feedback_file_id: str, feedback_file_name: str) -> None:
        """Add a processed essay to the tracker."""
        self.processed_essays[student_name] = {
            'email': student_email,
            'file_id': feedback_file_id,
            'file_name': feedback_file_name,
            'processed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # Log progress
        progress = len(self.processed_essays)
        logger.info(f"Progress: {progress}/{self.total_essays} essays processed")

    def generate_csv_report(self, service_drive, output_folder_id: str, failed_files: List[str] = None) -> Optional[str]:
        """
        Generate a CSV report with processed essay information.

        Args:
            service_drive: Google Drive service
            output_folder_id: Target folder ID
            failed_files: Optional list of failed files for reporting

        Returns:
            Optional[str]: File ID of the generated CSV or None if generation failed
        """
        if not self.processed_essays and not failed_files:
            logger.warning("No essays processed, cannot generate report")
            return None

        try:
            # Create CSV content
            csv_rows = [['Student Name', 'Email', 'Feedback File', 'File ID', 'Processed At', 'Status']]

            # Add successfully processed essays
            for student_name, info in self.processed_essays.items():
                csv_rows.append([
                    student_name,
                    info.get('email', ''),
                    info.get('file_name', ''),
                    info.get('file_id', ''),
                    info.get('processed_at', ''),
                    'Success'
                ])

            # Add failed files if provided
            if failed_files:
                for file_name in failed_files:
                    csv_rows.append([
                        extract_student_name_from_filename(file_name) or 'Unknown',
                        '',  # No email
                        file_name,
                        '',  # No file ID
                        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'Failed'
                    ])

            # Convert to CSV string
            csv_content = io.StringIO()
            csv_writer = csv.writer(csv_content)
            csv_writer.writerows(csv_rows)

            # Create file metadata
            timestamp = datetime.now().strftime('%Y%m%d-%H%M')
            file_name = f"Essay_Feedback_Summary_{timestamp}.csv"
            file_metadata = {
                'name': file_name,
                'parents': [output_folder_id],
                'mimeType': 'text/csv'
            }

            # Upload CSV file
            media = MediaIoBaseUpload(
                io.BytesIO(csv_content.getvalue().encode('utf-8')),
                mimetype='text/csv'
            )

            file = service_drive.files().create(
                body=file_metadata,
                media_body=media,
                fields='id'
            ).execute()

            file_id = file.get('id')
            logger.info(f"Generated CSV report: {file_name} (ID: {file_id})")

            # Log summary statistics
            duration = datetime.now() - self.start_time
            success_count = len(self.processed_essays)
            failure_count = len(failed_files) if failed_files else 0
            logger.info(f"Processing complete: {success_count} succeeded, {failure_count} failed")
            logger.info(f"Total duration: {duration}")

            return file_id

        except Exception as e:
            logger.error(f"Error generating CSV report: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

def get_files_from_drive(service_drive, folder_id: str) -> List[dict]:
    """
    Get list of files from Google Drive folder, excluding subfolders.
    Requests id, name, mimeType, and size. Handles pagination.
    """
    files = []
    page_token = None
    try:
        while True:
            response = service_drive.files().list(
                # Query excludes folders and trashed items
                q=f"'{folder_id}' in parents and mimeType != 'application/vnd.google-apps.folder' and trashed=false",
                spaces='drive',
                fields='nextPageToken, files(id, name, mimeType, size)',
                pageToken=page_token
            ).execute()
            # Add the files found on this page to the list
            files.extend(response.get('files', []))
            # Get the token for the next page, if it exists
            page_token = response.get('nextPageToken', None)
            # Exit loop if there are no more pages
            if page_token is None:
                break
        logger.info(f"Found {len(files)} non-folder items in input folder '{folder_id}'.")
        return files
    except Exception as e:
        logger.error(f"Error listing files from folder {folder_id}: {e}")
        logger.error(traceback.format_exc()) # Add traceback for more details
        return [] # Return empty list on error


def process_single_file_worker(
    file_info: dict,
    config_dict: dict, # SELECTED_..._MODEL_STR from main process Config
    google_key_json_str: str,
    openai_api_key: str,
    anthropic_api_key: str,
    openrouter_api_key: str,
    # language_code: str # language_code can be derived from master prompt now
    ) -> Tuple[bool, Optional[str], Optional[dict]]:
    """
    Worker function to process a single file in a separate process.
    Initializes its own services dynamically based on selected models.
    """
    # --- Logger Setup ---
    file_name = file_info.get('name', 'Unknown File')
    # Configure logger uniquely for this worker process
    worker_logger = logging.getLogger(f"Worker_{os.getpid()}_{file_name[:15]}") # Include PID, shorten filename
    worker_logger.setLevel(logging.INFO) # Or DEBUG for more detail
    worker_logger.propagate = False
    if not worker_logger.hasHandlers():
        handler = logging.StreamHandler(sys.stdout)
        formatter = logging.Formatter(f'%(asctime)s - %(process)d - %(levelname)s - {file_name[:15]} - %(message)s')
        handler.setFormatter(formatter)
        worker_logger.addHandler(handler)
    # --- End Logger Setup ---

    worker_logger.info(f"Starting process...")

    essay: Optional[Essay] = None # Initialize essay variable

    try:
        # --- Recreate Config State ---
        config = Config() # Create default config
        # Populate with passed values from the main process's config
        for key, value in config_dict.items():
            if hasattr(config, key):
                setattr(config, key, value)
            else:
                worker_logger.warning(f"Attribute '{key}' from config_dict not found in Config class.")

        # ---vvv DIAGNOSTIC PRINT: Check config strings received by worker vvv---
        worker_logger.info("-" * 20 + f" Worker Config Check " + "-" * 20)
        worker_logger.info(f"FB Model Str:   {config.SELECTED_FEEDBACK_MODEL_STR}")
        worker_logger.info(f"GR Model Str:   {config.SELECTED_GRAMMAR_MODEL_STR}")
        worker_logger.info(f"VL Model Str:   {config.SELECTED_VALIDATION_MODEL_STR}")
        worker_logger.info(f"ED Model Str:   {config.SELECTED_EDITOR_MODEL_STR}")
        worker_logger.info(f"Enable 2nd Pass:{config.ENABLE_SECOND_PASS}")
        worker_logger.info(f"Master Prompt Set: {'SWE' if config.ACTIVE_MASTER_PROMPT == MASTER_SYSTEM_PROMPT_SWE else 'ENG'}")
        worker_logger.info("-" * 50)
        # ---^^^ END DIAGNOSTIC PRINT ^^^---

        # --- Determine Language Code and Validate Config ---
        if not config.ACTIVE_MASTER_PROMPT:
             worker_logger.critical("ACTIVE_MASTER_PROMPT missing in config. Aborting.")
             return False, file_name, None
        lang_code_worker = 'sv-SE' if config.ACTIVE_MASTER_PROMPT == MASTER_SYSTEM_PROMPT_SWE else 'en-US'
        worker_logger.info(f"Determined language code: {lang_code_worker}")

        # --- Determine Required Providers and Models ---
        providers_needed: Set[str] = set()
        model_details: Dict[str, Tuple[str, str]] = {} # Stores {'task': (provider, model_name)}
        valid_config = True

        tasks_model_strings = {
            'feedback': config.SELECTED_FEEDBACK_MODEL_STR,
            'grammar': config.SELECTED_GRAMMAR_MODEL_STR,
            'validation': config.SELECTED_VALIDATION_MODEL_STR,
            'editor': config.SELECTED_EDITOR_MODEL_STR,
        }

        for task, model_str in tasks_model_strings.items():
            if not model_str: # Check if a selection string is missing
                 worker_logger.error(f"Model string for task '{task}' is missing in config.")
                 valid_config = False
                 continue
            provider, model_name = get_provider_and_model(model_str)
            if provider and model_name:
                # Only add provider if the task is actually enabled (e.g., grammar/validation if ENABLE_SECOND_PASS)
                if task == 'feedback' or task == 'editor' or (task in ['grammar', 'validation'] and config.ENABLE_SECOND_PASS):
                     providers_needed.add(provider)
                model_details[task] = (provider, model_name)
            else:
                worker_logger.error(f"Invalid model configuration for task '{task}': {model_str}")
                valid_config = False

        if not valid_config:
            worker_logger.critical("Invalid model configuration detected. Aborting worker.")
            return False, file_name, None
        worker_logger.info(f"Providers needed based on selections & enabled tasks: {providers_needed}")

        # --- Initialize Services Dynamically ---
        service_drive: Optional[Any] = None
        service_docs: Optional[Any] = None
        clients: Dict[str, Any] = {} # Dictionary to hold initialized clients {provider_name: client_object}

        # Initialize Google (Always needed)
        try:
            if not google_key_json_str: raise ValueError("Google credentials JSON missing.")
            credentials_info = json.loads(google_key_json_str)
            credentials = service_account.Credentials.from_service_account_info(
                credentials_info,
                scopes=['https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/documents']
            )
            service_drive = build('drive', 'v3', credentials=credentials)
            service_docs = build('docs', 'v1', credentials=credentials)
            if not service_drive or not service_docs: raise Exception("Building Google services failed.")
            worker_logger.info("Google services initialized.")
        except Exception as e:
            worker_logger.error(f"Failed to initialize Google Services: {e}", exc_info=True)
            return False, file_name, None # Fail worker if Google fails

        # Initialize required LLM clients *using passed keys*
        init_success = True
        if "OpenAI" in providers_needed:
            # Pass the key received by the worker
            clients["OpenAI"] = ServiceInitializer.initialize_openai(
                api_key=openai_api_key, logger_instance=worker_logger
            )
            if not clients["OpenAI"]: init_success = False # Error logged inside method

        if "Anthropic" in providers_needed:
            # Pass the key received by the worker
            clients["Anthropic"] = ServiceInitializer.initialize_anthropic(
                api_key=anthropic_api_key, logger_instance=worker_logger
            )
            if not clients["Anthropic"]: init_success = False # Error logged inside method

        if "OpenRouter" in providers_needed:
            # Pass the key received by the worker
            clients["OpenRouter"] = ServiceInitializer.initialize_openrouter_client(
                api_key=openrouter_api_key, logger_instance=worker_logger
            )
            if not clients["OpenRouter"]: init_success = False # Error logged inside method

        if not init_success:
            worker_logger.critical("One or more required LLM clients failed to initialize. Aborting worker.")
            return False, file_name, None

        # --- LanguageTool Initialization ---
        # Use context manager to ensure cleanup
        with language_tool_context(lang_code_worker) as tool:
            if not tool:
                worker_logger.error(f"LanguageTool context failed for {lang_code_worker}. Aborting worker.")
                return False, file_name, None
            worker_logger.info(f"LanguageTool for {lang_code_worker} ready.")

            # --- CORE PROCESSING LOGIC ---
            worker_logger.info("Starting core file processing.")

            # 1. Download, Read, Preprocess
            worker_logger.info("Downloading and reading file...")
            file_metadata = service_drive.files().get(fileId=file_info['id'], fields='mimeType, size').execute()
            if not validate_file(file_metadata, file_name, config):
                worker_logger.warning("File validation failed.")
                # No result details needed for validation failure before processing
                return False, file_name, {"status": "Validation Failed", "reason": "Type or Size"}

            file_io = download_file(service_drive, file_info['id'])
            raw_text = read_docx_content(file_io)
            if not raw_text or not raw_text.strip():
                worker_logger.warning("Empty or invalid content.")
                return False, file_name, {"status": "Processing Failed", "reason": "Empty Content"}

            essay = Essay(raw_text, file_name) # Includes metadata extraction
            if not essay.student_name: # Fallback name logic
                 name_from_file = extract_student_name_from_filename(file_name)
                 essay.student_name = capitalize_name(name_from_file or "Unknown_Student")
            worker_logger.info(f"Processing essay for: {essay.student_name}")

            # 2. Grammar Analysis (if enabled)
            all_grammar_steps: List[GrammarStep] = []
            if config.ENABLE_SECOND_PASS:
                worker_logger.info("Starting grammar analysis phase...")
                try:
                    # Get provider/model details for grammar and validation steps
                    gr_provider, gr_model = model_details['grammar']
                    vl_provider, vl_model = model_details['validation']
                    gr_client = clients.get(gr_provider)
                    vl_client = clients.get(vl_provider)

                    if gr_client and vl_client:
                        # Chunk the text (using LT-corrected text if first pass was run, else original)
                        # For simplicity now, chunking original/corrected happens inside the loop
                        text_to_chunk = essay.original_text # Start with original for chunking
                        text_chunks_with_context = chunk_text(
                             text_to_chunk, # Should ideally use LT corrected if available first
                             max_length=config.MAX_CHUNK_SIZE,
                             token_window=config.TOKEN_WINDOW
                         )
                        worker_logger.info(f"Split text into {len(text_chunks_with_context)} chunks for grammar analysis.")

                        raw_chunk_issues: List[GrammarStep] = []
                        for chunk_id, formatted_chunk in text_chunks_with_context:
                             chunk_feedback: Optional[GrammarFeedback] = reiterate_analysis(
                                 text_chunk=formatted_chunk, config=config, logger=worker_logger,
                                 provider=gr_provider, model_name=gr_model, client=gr_client, # Pass correct args
                                 chunk_id=chunk_id
                             )
                             if chunk_feedback and chunk_feedback.steps:
                                 # Filter out spelling/typos if needed (or handle in prompt)
                                 filtered = [s for s in chunk_feedback.steps if "spell" not in s.rule_category.lower() and "typo" not in s.rule_category.lower()]
                                 # Add chunk context to explanation or context field if desired
                                 for step in filtered: step.context = f"[Chunk {chunk_id}] {step.context}" # Example prefixing
                                 raw_chunk_issues.extend(filtered)

                        worker_logger.info(f"Collected {len(raw_chunk_issues)} raw issues from grammar analysis.")


                        worker_logger.info("Deduplicating raw issues based on original_error...")
                        # Ensure deduplicate_issues_by_original_error is accessible in this scope
                        deduplicated_raw_issues = deduplicate_issues_by_original_error(
                            issues=raw_chunk_issues,
                            logger=worker_logger,
                            similarity_threshold=0.85 # Or use config value
                        )
                        worker_logger.info(f"Reduced to {len(deduplicated_raw_issues)} issues after raw deduplication.")


                        # Validate the *deduplicated* list
                        validated_steps: List[GrammarStep] = []
                        # --- Check deduplicated list ---
                        if deduplicated_raw_issues:

                            worker_logger.info(f"Validating {len(deduplicated_raw_issues)} unique raw issues...") # Log count being validated
                            validation_func = validate_grammar_issues_batch if lang_code_worker == 'sv-SE' else validate_grammar_issues_batch_en
                            validated_steps = validation_func(
                                # --- Pass deduplicated list ---
                                grammar_issues=deduplicated_raw_issues,

                                config=config, logger=worker_logger,
                                provider=vl_provider, model_name=vl_model, client=vl_client
                            )
                        else:
                            worker_logger.info("No raw grammar issues found to validate (after deduplication).")

                        # --- Select final issues using only explanation similarity ---
                        worker_logger.info("Selecting final issues based on explanation variety...")
                        # Ensure select_final_issues_by_explanation is accessible
                        final_issues_for_doc = select_final_issues_by_explanation(
                            unique_issues=validated_steps, # Pass the validated list
                            logger=worker_logger,
                            max_total=10, # Example limit
                            explanation_similarity_threshold=0.50 # Example threshold
                        )
                        worker_logger.info(f"Selected a total of {len(final_issues_for_doc)} issues...")

                        essay.grammar_issues = final_issues_for_doc


                    else:
                        worker_logger.warning(f"Grammar analysis skipped: Clients not available (Grammar: {gr_provider} -> {bool(gr_client)}, Validation: {vl_provider} -> {bool(vl_client)}).")
                        essay.corrected_text = essay.original_text # Use original if skipped
                        essay.grammar_issues = []
                        essay.has_grammar_processed = True # Mark as processed (skipped)

                except Exception as e_grammar:
                     worker_logger.error(f"Grammar analysis phase failed critically: {e_grammar}", exc_info=True)
                     essay.corrected_text = essay.original_text # Fallback
                     essay.grammar_issues = []
                     essay.has_grammar_processed = False # Mark as failed
            else:
                worker_logger.info("Grammar second pass disabled by config.")
                essay.corrected_text = essay.original_text # Use original text
                essay.grammar_issues = []
                essay.has_grammar_processed = True # Mark as processed (skipped)


            # 3. Generate AI Feedback
            worker_logger.info("Generating teacher feedback...")
            fb_provider, fb_model = model_details['feedback']
            fb_client = clients.get(fb_provider)
            ai_feedback = None # Initialize feedback variable

            if fb_client: # Check if client was initialized successfully
                try:
                    # Call generate_feedback, passing the worker's logger instance
                    ai_feedback = generate_feedback(
                        essay=essay,
                        config=config,
                        provider=fb_provider,
                        model_name=fb_model,
                        client=fb_client,
                        logger_instance=worker_logger # <<< MODIFIED: Pass worker logger
                    )
                except RetryError as e_retry_final:
                    # Catch the error specifically raised by tenacity after all retries fail
                    worker_logger.error(
                        f"Feedback generation failed for '{essay.student_name}' "
                        f"after {e_retry_final.last_attempt.attempt_number} attempts. "
                        f"Final error: {e_retry_final.cause}" # Access the original exception
                    )
                    ai_feedback = f"[{fb_provider} feedback generation failed after multiple retries.]" # Use fallback text
                except Exception as e_feedback_other:
                    # Catch any other unexpected error during the call
                    worker_logger.error(
                        f"Unexpected error during feedback generation call for '{essay.student_name}': {e_feedback_other}",
                        exc_info=True # Log traceback for unexpected errors
                    )
                    ai_feedback = f"[{fb_provider} feedback generation encountered an unexpected error.]" # Use fallback text
            else:
                 # Handle case where the required client wasn't initialized
                 worker_logger.error(f"Cannot generate feedback: Client for provider '{fb_provider}' not available.")
                 ai_feedback = f"[{fb_provider} feedback client unavailable.]" # Use fallback text

            # Check if feedback generation ultimately failed or produced fallback text
            if not ai_feedback or "failed" in ai_feedback or "error" in ai_feedback or "unavailable" in ai_feedback:
                worker_logger.error(f"Feedback generation failed for '{essay.student_name}'. Using fallback text for document.")
                # === Decision Point: Should a feedback failure stop the whole file processing? ===
                # Option 1: Continue with fallback text (current implementation)
                # Option 2: Mark the whole file as failed
                # return False, file_name, {"status": "Processing Failed", "reason": "Feedback Generation Failed"}
                # For now, we continue with the fallback text recorded in ai_feedback.

            # 4. Generate Edited Version
            worker_logger.info("Generating edited version...")
            ed_provider, ed_model = model_details['editor']
            ed_client = clients.get(ed_provider)
            if ed_client:
                 edited_version = generate_edited_version(
                     essay=essay, feedback=ai_feedback, config=config,
                     provider=ed_provider, model_name=ed_model, client=ed_client,
                     language_code=lang_code_worker
                 )
                 if edited_version:
                     essay.edited_text = edited_version
                     worker_logger.info(f"Successfully created AI-edited version using {ed_provider}.")
                 else:
                     worker_logger.warning(f"Failed to generate edited version using {ed_provider}. Using grammar-corrected/original text.")
                     # If grammar analysis ran, corrected_text might have LT changes, otherwise it's original.
                     essay.edited_text = essay.corrected_text
            else:
                 worker_logger.warning(f"Skipping edited version: Client for provider '{ed_provider}' not available.")
                 essay.edited_text = essay.corrected_text # Use grammar-corrected/original as fallback


            # 5. Create and Upload Document
            worker_logger.info("Creating and uploading feedback document...")
            doc_creator = DocumentCreator(config, logger=worker_logger, language_code=lang_code_worker)
            timestamp = datetime.now().strftime('%d%m%y-%H%M')
            student_name_formatted = capitalize_name(essay.student_name or "Unknown").replace(' ', '_')
            email_part = f"-{essay.student_email}" if essay.student_email else ""
            feedback_file_name = f"{student_name_formatted}{email_part}-{timestamp}.docx"

            file_id = doc_creator.create_and_upload_feedback(
                essay=essay, feedback=ai_feedback, service_drive=service_drive,
                output_folder_id=config.OUTPUT_FOLDER_ID, file_name=feedback_file_name
            )

            # --- Processing Outcome ---
            if file_id:
                worker_logger.info(f"Successfully processed and uploaded file ID: {file_id}")
                result_details = {
                     'student_name': essay.student_name,
                     'student_email': essay.student_email or "",
                     'feedback_file_id': file_id,
                     'feedback_file_name': feedback_file_name,
                     'status': 'Success' # Add status
                }
                return True, file_name, result_details
            else:
                 worker_logger.error("Failed to create/upload feedback document.")
                 # Pass back failure reason if possible
                 return False, file_name, {"status": "Processing Failed", "reason": "Upload Failed"}

    # --- Outer Error Handling ---
    except Exception as e:
        worker_logger.critical(f"WORKER_FATAL_ERROR processing {file_name}: {e}", exc_info=True)
        # Try to return specific failure info if essay object exists
        reason = f"Unexpected Worker Error: {e}"
        status_details = {"status": "Processing Failed", "reason": reason}
        if essay and essay.student_name:
             status_details['student_name'] = essay.student_name # Add name if available
        return False, file_name, status_details

# --- Updated EssayTracker Class ---
class EssayTracker:
    """Tracks processed essays and generates summary data."""

    # Accepts logger_instance in __init__
    def __init__(self, logger_instance):
        self.processed_essays = {}
        self.total_essays = 0
        from datetime import datetime # Keep imports local to methods if preferred
        self.start_time = datetime.now()
        self.logger = logger_instance # Store the passed logger

    def set_total_count(self, count: int) -> None:
        """Set the total number of essays in the input folder."""
        self.total_essays = count
        # Use the instance's logger
        self.logger.info(f"Total essays to process: {self.total_essays}")

    def add_processed_essay(self, student_name: str, student_email: str,
                           feedback_file_id: str, feedback_file_name: str) -> None:
        """Add a processed essay to the tracker."""
        from datetime import datetime
        self.processed_essays[student_name] = {
            'email': student_email,
            'file_id': feedback_file_id,
            'file_name': feedback_file_name,
            'processed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # Log progress using the instance's logger
        progress = len(self.processed_essays)
        total = self.total_essays # Read instance variable
        self.logger.info(f"Progress: {progress}/{total} essays processed")

    # Use self.logger for logging within generate_csv_report as well
    def generate_csv_report(self, service_drive, output_folder_id: str, failed_files: list = None) -> str or None:
         from datetime import datetime
         import io
         import csv
         from googleapiclient.http import MediaIoBaseUpload
         from typing import Optional, List
         # Assuming extract_student_name_from_filename is globally available

         if not self.processed_essays and not failed_files:
            self.logger.warning("No essays processed, cannot generate report") # Use self.logger
            return None

         try:
            # Create CSV content
            csv_rows = [['Student Name', 'Email', 'Feedback File', 'File ID', 'Processed At', 'Status']]

            # Add successfully processed essays
            for student_name, info in self.processed_essays.items():
                csv_rows.append([
                    student_name,
                    info.get('email', ''),
                    info.get('file_name', ''),
                    info.get('file_id', ''),
                    info.get('processed_at', ''),
                    'Success'
                ])

            # Add failed files if provided
            if failed_files:
                for file_name in failed_files:
                    csv_rows.append([
                        extract_student_name_from_filename(file_name) or 'Unknown',
                        '',  # No email
                        file_name,
                        '',  # No file ID
                        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'Failed'
                    ])

            # Convert to CSV string
            csv_content = io.StringIO()
            csv_writer = csv.writer(csv_content)
            csv_writer.writerows(csv_rows)

            # Create file metadata
            timestamp = datetime.now().strftime('%Y%m%d-%H%M')
            file_name = f"Essay_Feedback_Summary_{timestamp}.csv"
            file_metadata = {
                'name': file_name,
                'parents': [output_folder_id],
                'mimeType': 'text/csv'
            }

            # Upload CSV file
            media = MediaIoBaseUpload(
                io.BytesIO(csv_content.getvalue().encode('utf-8')),
                mimetype='text/csv'
            )

            file = service_drive.files().create(
                body=file_metadata,
                media_body=media,
                fields='id'
            ).execute()

            file_id = file.get('id')
            self.logger.info(f"Generated CSV report: {file_name} (ID: {file_id})")

            # Log summary statistics
            duration = datetime.now() - self.start_time
            success_count = len(self.processed_essays)
            failure_count = len(failed_files) if failed_files else 0
            self.logger.info(f"Processing complete: {success_count} succeeded, {failure_count} failed")
            self.logger.info(f"Total duration: {duration}")

            return file_id

         except Exception as e:
                self.logger.error(f"Error generating CSV report: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
                return None

"""### Main Processing"""

def process_single_file_worker_wrapper(args):
     return process_single_file_worker(*args)

def process_files(config: Config, logger):
    """
    Orchestrates the parallel processing of essay files using multiprocessing.
    Retrieves secrets once, passes secrets and config dict to workers.
    """
    logger.info("Starting parallel file processing...")
    logger.display_status("Initializing...")

    # --- Retrieve Secrets ONCE ---
    google_key_json_str = None
    openai_api_key = None
    anthropic_api_key = None
    open_router_api_key = None
    try:
        logger.info("Retrieving secrets...")
        google_key_json_str = get_secret('GOOGLE_CLOUD_PRIVATE_KEY')
        openai_api_key = get_secret('OPENAI_API_KEY')
        anthropic_api_key = get_secret('ANTHROPIC_API_KEY')
        open_router_api_key = get_secret('OPEN_ROUTER_API_KEY')
        # Use validate_secrets which now checks for all three
        if not validate_secrets():
             # validate_secrets already logs the missing ones
             raise ValueError("One or more required secrets could not be retrieved.")
        logger.info("Secrets retrieved successfully.")
    except Exception as e:
        logger.error(f"Failed to retrieve secrets: {e}")
        logger.error(traceback.format_exc())
        logger.display_status(f"Error: Failed to retrieve secrets. Check logs.")
        return

    # --- Initialize Google Drive service for listing files ---
    # (Keep existing Google Drive init for main process)
    service_drive_main = None
    try:
        logger.info("Initializing Google Drive service for file listing...")
        if not google_key_json_str: raise ValueError("Google credentials missing.")
        credentials_info = json.loads(google_key_json_str)
        credentials = service_account.Credentials.from_service_account_info(
            credentials_info, scopes=['https://www.googleapis.com/auth/drive']
        )
        service_drive_main = build('drive', 'v3', credentials=credentials)
        if not service_drive_main: raise Exception("Building Google Drive service failed.")
        logger.info("Google Drive service for listing initialized.")
    except Exception as e:
        logger.error(f"Failed to initialize Google Drive for listing: {e}")
        logger.error(traceback.format_exc())
        logger.display_status(f"Error: Failed to initialize Google Drive. Check logs.")
        return


    # --- Get and Filter Files ---
    # (Keep existing file listing and filtering logic)
    logger.display_status("Listing files from input folder...")
    files_raw = get_files_from_drive(service_drive_main, config.INPUT_FOLDER_ID)
    if not files_raw:
        logger.warning(f"No files found in input folder ID: {config.INPUT_FOLDER_ID}")
        logger.display_status("No files found in input folder. Processing complete.")
        return

    files_to_process = []
    skipped_files = []
    for f in files_raw:
        if f.get('mimeType') in config.ALLOWED_MIME_TYPES:
             files_to_process.append(f)
        else:
             logger.warning(f"Skipping '{f.get('name')}': Unsupported MIME type '{f.get('mimeType')}'.")
             skipped_files.append(f.get('name', 'Unknown Skipped File'))

    if not files_to_process:
        logger.warning("No processable files found after filtering.")
        logger.display_status("No processable files found. Processing complete.")
        # Generate report if needed (keep existing logic)
        essay_tracker = EssayTracker(logger)
        essay_tracker.set_total_count(len(files_raw))
        if service_drive_main:
             essay_tracker.generate_csv_report(service_drive_main, config.OUTPUT_FOLDER_ID, failed_files=skipped_files)
        return

    total_files = len(files_to_process)
    logger.info(f"Found {total_files} files to process.")


    # --- Essay Tracker Setup ---
    essay_tracker = EssayTracker(logger)
    essay_tracker.set_total_count(total_files)
    failed_files_list = list(skipped_files)

    # --- Pre-cache LanguageTool (Optional but recommended) ---
    # (Keep existing pre-caching logic based on selected_course)
    logger.info("Pre-caching LanguageTool...")
    logger.display_status("Preparing LanguageTool...")
    lang_code_main = None  # DEFINE IT HERE, OUTSIDE try
    try:
        if "Svenska" in selected_course:
            lang_code_main = 'sv-SE'
        else:
            lang_code_main = 'en-US'
        logger.info(f"Pre-caching LanguageTool for language: {lang_code_main}")
        with language_tool_context(lang_code_main) as pre_cache_tool:
            if pre_cache_tool:
                logger.info(f"LanguageTool for {lang_code_main} pre-cached/ready.")
            else:
                logger.warning(f"Failed to pre-cache LanguageTool for {lang_code_main}.")
    except NameError:
        logger.error("Failed to determine language for pre-caching: 'selected_course' not found.")
        # Assign a default value in the NameError handler to ensure it's ALWAYS defined
        lang_code_main = 'en-US'  # OR 'sv-SE', depending on your default choice
    except Exception as e:
        logger.error(f"Error during LanguageTool pre-caching: {e}")
        # Assign a default value in the general Exception handler as well
        lang_code_main = 'en-US'  # OR 'sv-SE', depending on your default choice

    # --- Prepare Arguments for Workers ---
    logger.info("Preparing arguments for worker processes...")
    try:
        # Create config_dict with SELECTED MODEL STRINGS
        attributes_to_pass = [
            # General Config
            'INPUT_FOLDER_ID', 'OUTPUT_FOLDER_ID', 'MAX_FILE_SIZE_MB',
            'ALLOWED_MIME_TYPES', 'ENABLE_SECOND_PASS', 'MAX_CHUNK_SIZE',
            'TOKEN_WINDOW', 'STYLE_FONT', 'STYLE_SIZE', 'STYLE_SPACING',
            'USE_COLOR_FEEDBACK',
            # Selected Model Strings
            'SELECTED_FEEDBACK_MODEL_STR',
            'SELECTED_GRAMMAR_MODEL_STR',
            'SELECTED_VALIDATION_MODEL_STR',
            'SELECTED_EDITOR_MODEL_STR',
            # Prompts
            'ESSAY_INSTRUCTION', 'ACTIVE_SYSTEM_MESSAGE',
            'ACTIVE_GRAMMAR_PROMPT', 'ACTIVE_MASTER_PROMPT'
            # Add Default model strings if needed for fallbacks in worker
            # 'DEFAULT_FEEDBACK_MODEL_STR', etc.
        ]
        config_dict = {attr: getattr(config, attr) for attr in attributes_to_pass if hasattr(config, attr)}

        # Basic validation
        if not all(k in config_dict for k in [
            'SELECTED_FEEDBACK_MODEL_STR', 'SELECTED_GRAMMAR_MODEL_STR',
            'SELECTED_VALIDATION_MODEL_STR', 'SELECTED_EDITOR_MODEL_STR',
            'ACTIVE_MASTER_PROMPT' ]):
             raise ValueError("Critical selections missing from config_dict for workers.")

        worker_args = [
            (
                file_info,
                config_dict, # Pass dictionary with model strings
                google_key_json_str,
                openai_api_key,
                anthropic_api_key,
                open_router_api_key
            )
            for file_info in files_to_process
        ]
        logger.info(f"Prepared arguments for {len(worker_args)} worker tasks.")

    except Exception as e:
        logger.error(f"Error preparing worker arguments: {e}")
        logger.error(traceback.format_exc())
        logger.display_status(f"Error preparing for workers. Check logs.")
        return

    # --- Multiprocessing Pool ---
    # (Keep existing pool size calculation logic)
    max_processes = os.cpu_count() or 1
    desired_workers = active_essay_workers # Use the value from the slider/mapper
    num_processes = min(desired_workers, max_processes, total_files)
    logger.info(f"System CPU cores: {max_processes}, Desired workers: {desired_workers}, Files: {total_files}")
    logger.info(f"Initializing multiprocessing pool with {num_processes} worker(s)...")
    logger.display_status(f"Starting {num_processes} workers to process {total_files} files...")


    results = []
    failed_temp = [] # Temp list for failures during iteration
    try:
        with multiprocessing.Pool(processes=num_processes) as pool:
            processed_count = 0
            # Use imap_unordered - yields results as they complete
            result_iterator = pool.imap_unordered(process_single_file_worker_wrapper, worker_args) # Need a wrapper if using starmap args

            for result_tuple in result_iterator:
                processed_count += 1
                results.append(result_tuple) # Collect results
                # Update display after each result is processed by the main thread
                # Extract success status/name for better display message
                success_status, file_name_from_worker, _ = result_tuple if isinstance(result_tuple, tuple) and len(result_tuple) == 3 else (False, "Unknown", None)
                status_msg = f"Processing... ({processed_count}/{total_files} complete)"
                if not success_status:
                    status_msg += f" (Error processing {file_name_from_worker})"
                    failed_temp.append(file_name_from_worker) # Track failure
                logger.display_status(status_msg, progress=processed_count, total=total_files)

            logger.info("Multiprocessing pool finished (imap_unordered).")
            logger.display_status(f"Workers finished. Processing results...", progress=total_files, total=total_files) # Final update


    except KeyboardInterrupt:
         logger.error("Processing terminated by user.")
         logger.display_status("Processing terminated by user.")
         failed_files_list.extend([f['name'] for f in files_to_process]) # Mark all as failed
    except Exception as e:
        logger.error(f"Fatal error during multiprocessing pool execution: {e}")
        logger.error(traceback.format_exc())
        logger.display_status(f"Error during parallel processing. Check logs.")
        failed_files_list.extend([f['name'] for f in files_to_process if f['name'] not in [res[1] for res in results if res and len(res)>1]])


    # --- Process Results ---
    # (Keep existing results processing logic)
    logger.info("Processing results from workers...")
    successful_count = 0
    failed_set = set(failed_files_list) # Start with initially skipped files

    for result_tuple in results:
        try:
            if not isinstance(result_tuple, tuple) or len(result_tuple) != 3:
                 logger.error(f"Received unexpected result format from worker: {result_tuple}")
                 failed_set.add("Unknown File (Malformed Result)")
                 continue

            success_status, file_name_from_worker, result_details = result_tuple
            log_file_name = file_name_from_worker or "Unknown File (from worker)"

            if success_status and result_details:
                essay_tracker.add_processed_essay(
                    student_name=result_details['student_name'],
                    student_email=result_details['student_email'],
                    feedback_file_id=result_details['feedback_file_id'],
                    feedback_file_name=result_details['feedback_file_name']
                )
                successful_count += 1
                # Update status bar after processing each result
                logger.display_status(f"Processing results... ({successful_count}/{total_files} complete)", progress=successful_count, total=total_files)

            else:
                logger.error(f"Processing failed for file: {log_file_name} (reported by worker)")
                failed_set.add(log_file_name)
                # Update status bar even for failures to show progress
                logger.display_status(f"Processing results... ({successful_count}/{total_files} complete, errors encountered)", progress=successful_count, total=total_files)

        except Exception as e:
            logger.error(f"Error processing result tuple '{result_tuple}': {e}")
            logger.error(traceback.format_exc())
            if isinstance(result_tuple, tuple) and len(result_tuple) > 1 and isinstance(result_tuple[1], str):
                 failed_set.add(result_tuple[1])
            else:
                 failed_set.add("Unknown File (Result Processing Error)")

    failed_files_list = list(failed_set)
    total_processed_or_failed = successful_count + len(failed_files_list)
    logger.info(f"Finished processing results: {successful_count} succeeded, {len(failed_files_list)} failed/skipped.")
    logger.display_status(f"Processing results complete. Generating report...", progress=total_processed_or_failed, total=total_files)


    # --- Generate Summary Report ---
    if service_drive_main:
        logger.info("Generating summary CSV report...")
        try:
            report_id = essay_tracker.generate_csv_report(
                service_drive=service_drive_main,
                output_folder_id=config.OUTPUT_FOLDER_ID,
                failed_files=failed_files_list
            )
            if report_id:
                logger.info(f"CSV report generated successfully: ID {report_id}")
                logger.display_status(f"Processing complete. Report generated (ID: {report_id}).", progress=total_processed_or_failed, total=total_files)
            else:
                logger.error("Failed to generate CSV report.")
                logger.display_status(f"Processing complete, but failed to generate report. Check logs.", progress=total_processed_or_failed, total=total_files)
        except Exception as e:
            logger.error(f"Exception during CSV report generation: {e}")
            logger.error(traceback.format_exc())
            logger.display_status(f"Processing complete, but error during report generation. Check logs.", progress=total_processed_or_failed, total=total_files)
    else:
        logger.error("Cannot generate CSV report: Google Drive service was not available.")
        logger.display_status("Processing complete. Cannot generate report (Drive service error).", progress=total_processed_or_failed, total=total_files)

"""### Entry Point"""

if __name__ == "__main__" or 'google.colab' in sys.modules:
    logger.info("Starting AI-Assisted Document Grammar and Feedback Pipeline")
    config = Config() # Instantiate config

    # Assign prompts and selected MODEL STRINGS to the config instance
    try:
        # --- CHECK 1: Validate variable existence using globals() ---
        form_vars_needed = [
            'essay_instructions', 'active_system_message', 'active_grammar_prompt',
            'active_master_prompt', 'selected_feedback_model_str',
            'selected_grammar_model_str', 'selected_validation_model_str',
            'selected_editor_model_str'
        ]
        # Use globals() for checking top-level notebook variables
        missing_vars = [v for v in form_vars_needed if v not in globals()]
        if missing_vars:
            # Raise specific error if variables are missing after running cells
            raise NameError(f"Config variables missing from global scope: {', '.join(missing_vars)}. Ensure preceding cells (Forms, Mapper) ran successfully without errors.")

        # --- CHECK 2: Assign to Config Object ---
        # This part should now work if the globals() check passes
        config.set_runtime_config(
            essay_instr=essay_instructions,
            system_msg=active_system_message,
            grammar_prompt=active_grammar_prompt,
            master_prompt=active_master_prompt,
            selected_feedback_str=selected_feedback_model_str,
            selected_grammar_str=selected_grammar_model_str,
            selected_validation_str=selected_validation_model_str,
            selected_editor_str=selected_editor_model_str
        )
        logger.info(f"Configuration, prompts, and model selections loaded into config object.")

    except NameError as e:
        logger.error(f"Configuration Error: {e}")
        # Log details about scope if possible (optional advanced debugging)
        # logger.debug(f"Globals keys: {list(globals().keys())}")
        # logger.debug(f"Locals keys: {list(locals().keys())}")
        raise SystemExit("Exiting due to configuration NameError.")
    except Exception as e:
        logger.error(f"Unexpected error setting runtime config: {e}")
        logger.error(traceback.format_exc())
        raise SystemExit("Exiting due to unexpected configuration error.")

    # --- CHECK 3: Secret Validation ---
    if not validate_secrets():
        logger.error("Required secrets missing. Exiting.")
        # No SystemExit needed here, validate_secrets handles logging
    else:
        # --- CHECK 4: API Key Setup Call ---
        try:
            # Corrected function call: Use setup_api_keys() which IS defined
            # setup_openai_key() was previously called but wasn't defined.
            setup_api_keys()
            logger.info("API Keys retrieved and OpenAI key set in environment (if applicable).")

            # --- CHECK 5: Run Main Processing ---
            # Run the main processing function
            process_files(config, logger)
            logger.info("Pipeline execution finished.")

        except SystemExit as e: # Catch SystemExit from setup_api_keys if secrets fail post-validation
             logger.critical(f"SystemExit during API key setup: {e}")
             # No need to raise again, already exiting.
        except Exception as e:
            logger.error(f"Fatal error in main execution block: {e}")
            logger.error(traceback.format_exc())
            try:
                logger.display_status(f"Pipeline failed with error: {e}. Check logs.")
            except: # Failsafe if logger itself has issues
                print(f"Pipeline failed with error: {e}")
        finally:
            gc.collect()
            logger.info("Pipeline cleanup completed.")

"""## Grammar Filter Test Suite"""

import os
cores = os.cpu_count()
print(f"Number of CPU cores available in this session: {cores}")