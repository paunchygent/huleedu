# Research-scoped transformer fine-tuning runtime for Hemma GPU execution.
#
# Intent:
# - Keep G3 transformer fine-tuning isolated from the offload API container.
# - Use ROCm torch from the base image as the authoritative GPU runtime.
# - Install only default + `ml-research` Python deps needed for training CLI.
# - Exclude pip-installed torch/cuda packages to avoid overriding ROCm torch.
ARG BASE_IMAGE=rocm/pytorch:rocm7.2_ubuntu24.04_py3.12_pytorch_release_2.9.1
FROM ${BASE_IMAGE} AS runtime

ARG BASE_IMAGE

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PDM_USE_VENV=false \
    PDM_IGNORE_ACTIVE_VENV=1 \
    PROJECT_ROOT=/app \
    PYTHONPATH=/app \
    HF_HOME=/cache/huggingface \
    TRANSFORMERS_CACHE=/cache/huggingface

LABEL org.huleedu.transformer_train.base_image="${BASE_IMAGE}" \
    org.huleedu.transformer_train.rocm_line="7.2" \
    org.huleedu.transformer_train.pytorch_line="2.9" \
    org.huleedu.transformer_train.python_line="3.12"

WORKDIR /app

RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    build-essential \
    python3-dev \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir pdm

COPY pyproject.toml pdm.lock ./

RUN pdm export -G ml-research -f requirements --without-hashes -o /tmp/requirements.txt

RUN python - <<'PY'
from pathlib import Path
import re

source_path = Path("/tmp/requirements.txt")
target_path = Path("/tmp/requirements.filtered.txt")

blocked_exact = {"torch", "torchvision", "triton"}
blocked_prefixes = ("nvidia-", "cuda-")
name_pattern = re.compile(r"^([A-Za-z0-9_.-]+)")

filtered: list[str] = []
for raw_line in source_path.read_text().splitlines():
    line = raw_line.strip()
    if not line or line.startswith("#"):
        continue
    requirement = line.split(";", 1)[0].strip()
    match = name_pattern.match(requirement)
    if not match:
        continue
    package_name = match.group(1).lower().replace("_", "-")
    if package_name in blocked_exact:
        continue
    if any(package_name.startswith(prefix) for prefix in blocked_prefixes):
        continue
    filtered.append(line)

target_path.write_text("\n".join(filtered) + "\n")
PY

RUN python -m pip install --no-cache-dir --no-deps -r /tmp/requirements.filtered.txt

RUN python - <<'PY'
import torch
print("torch", torch.__version__)
print("hip", getattr(torch.version, "hip", None))
print("cuda_is_available", torch.cuda.is_available())
PY

CMD ["sleep", "infinity"]
